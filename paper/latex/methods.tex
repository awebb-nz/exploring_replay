We treated the (navigational) decision-making problem in our variant of the Tolman maze as a partially observable Markov Decision Process (POMDP). The subject was designed in the spirit of the DYNA architecture, such that online decisions were made according to the behavioural model-free policy, and offline planning was used to additionally train the model-free controller. The subject was endowed with a probabilistic belief about the existence of barriers in certain locations in the maze; every decision (real or imagined) therefore transitioned the subject to a new belief state which comprised the subject's physical state, as well as its updated prior belief. For planning (replay), the subject considered how its belief state will evolve up to a fixed horizon. The value of each imagined belief state was approximated with the subject's model-free $Q$-values at the corresponding physical location. Moreover, we discretised the possible belief states into the subject's initial uncertainty, and whether the barrier was certainly present or certainly absent. The priority of each replay update was determined by the expected long-run improvement to the subject's current belief state engendered by each potential replay update. The replay updates were executed until the expected improvement was estimated to be below a fixed threshold. For sequence replay updates, the maximal length of each potential sequence was limited to the distance from the start state to the uncertain barrier.