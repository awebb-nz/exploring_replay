@article{abelExpressivityMarkovReward,
  title = {On the {{Expressivity}} of {{Markov Reward}}},
  author = {Abel, David and Dabney, Will and Harutyunyan, Anna},
  pages = {14},
  abstract = {Reward is the driving force for reinforcement-learning agents. This paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform. We frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Our main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists. We conclude with an empirical study that corroborates and illustrates our theoretical findings.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IADKHUMK/Abel et al. - On the Expressivity of Markov Reward.pdf}
}

@unpublished{abelTheoryAbstractionReinforcement2022,
  title = {A {{Theory}} of {{Abstraction}} in {{Reinforcement Learning}}},
  author = {Abel, David},
  date = {2022-03-01},
  eprint = {2203.00397},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2203.00397},
  urldate = {2022-04-08},
  abstract = {Reinforcement learning defines the problem facing agents that learn to make good decisions through action and observation alone. To be effective problem solvers, such agents must efficiently explore vast worlds, assign credit from delayed feedback, and generalize to new experiences, all while making use of limited data, computational resources, and perceptual bandwidth. Abstraction is essential to all of these endeavors. Through abstraction, agents can form concise models of their environment that support the many practices required of a rational, adaptive decision maker. In this dissertation, I present a theory of abstraction in reinforcement learning. I first offer three desiderata for functions that carry out the process of abstraction: they should 1) preserve representation of near-optimal behavior, 2) be learned and constructed efficiently, and 3) lower planning or learning time. I then present a suite of new algorithms and analysis that clarify how agents can learn to abstract according to these desiderata. Collectively, these results provide a partial path toward the discovery and use of abstraction that minimizes the complexity of effective reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/MG5YUMI4/Abel - 2022 - A Theory of Abstraction in Reinforcement Learning.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/29XSJEYN/2203.html}
}

@article{addicottPrimerForagingExplore2017,
  title = {A {{Primer}} on {{Foraging}} and the {{Explore}}/{{Exploit Trade-Off}} for {{Psychiatry Research}}},
  author = {Addicott, M. A. and Pearson, J. M. and Sweitzer, M. M. and Barack, D. L. and Platt, M. L.},
  date = {2017-09},
  journaltitle = {Neuropsychopharmacology},
  shortjournal = {Neuropsychopharmacol},
  volume = {42},
  number = {10},
  pages = {1931--1939},
  publisher = {{Nature Publishing Group}},
  issn = {1740-634X},
  doi = {10.1038/npp.2017.108},
  url = {https://www.nature.com/articles/npp2017108},
  urldate = {2022-08-12},
  abstract = {Foraging is a fundamental behavior, and many types of animals appear to have solved foraging problems using a shared set of mechanisms. Perhaps the most common foraging problem is the choice between exploiting a familiar option for a known reward and exploring unfamiliar options for unknown rewards—the so-called explore/exploit trade-off. This trade-off has been studied extensively in behavioral ecology and computational neuroscience, but is relatively new to the field of psychiatry. Explore/exploit paradigms can offer psychiatry research a new approach to studying motivation, outcome valuation, and effort-related processes, which are disrupted in many mental and emotional disorders. In addition, the explore/exploit trade-off encompasses elements of risk-taking and impulsivity—common behaviors in psychiatric disorders—and provides a novel framework for understanding these behaviors within an ecological context. Here we explain relevant concepts and some common paradigms used to measure explore/exploit decisions in the laboratory, review clinically relevant research on the neurobiology and neuroanatomy of explore/exploit decision making, and discuss how computational psychiatry can benefit from foraging theory.},
  issue = {10},
  langid = {english},
  keywords = {Cognitive neuroscience,Computational neuroscience,Decision,Medical research,Motivation,Psychiatric disorders,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/35UXMAET/Addicott et al. - 2017 - A Primer on Foraging and the ExploreExploit Trade.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/QCQKLKFH/npp2017108.html}
}

@article{agrawalAnalysisThompsonSampling,
  title = {Analysis of {{Thompson Sampling}} for the {{Multi-armed Bandit Problem}}},
  author = {Agrawal, Shipra and Goyal, Navin},
  pages = {26},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/94X7P8GQ/Agrawal and Goyal - Analysis of Thompson Sampling for the Multi-armed .pdf}
}

@unpublished{agrawalPosteriorSamplingReinforcement2020,
  title = {Posterior Sampling for Reinforcement Learning: Worst-Case Regret Bounds},
  shorttitle = {Posterior Sampling for Reinforcement Learning},
  author = {Agrawal, Shipra and Jia, Randy},
  date = {2020-03-30},
  eprint = {1705.07041},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1705.07041},
  urldate = {2021-12-07},
  abstract = {We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of \$\textbackslash tilde\{O\}(DS\textbackslash sqrt\{AT\})\$ for any communicating MDP with \$S\$ states, \$A\$ actions and diameter \$D\$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon \$T\$. This result closely matches the known lower bound of \$\textbackslash Omega(\textbackslash sqrt\{DSAT\})\$. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NZVCT5IG/Agrawal and Jia - 2020 - Posterior sampling for reinforcement learning wor.pdf}
}

@report{agrawalTemporalDynamicsOpportunity2020,
  type = {preprint},
  title = {The {{Temporal Dynamics}} of {{Opportunity Costs}}: {{A Normative Account}} of {{Cognitive Fatigue}} and {{Boredom}}},
  shorttitle = {The {{Temporal Dynamics}} of {{Opportunity Costs}}},
  author = {Agrawal, Mayank and Mattar, Marcelo G. and Cohen, Jonathan D. and Daw, Nathaniel D.},
  date = {2020-09-09},
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.08.287276},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.09.08.287276},
  urldate = {2021-12-08},
  abstract = {Abstract           Cognitive fatigue and boredom are two phenomenological states that reflect overt task disengagement. In this paper, we present a rational analysis of the temporal structure of controlled behavior, which provides a formal account of these phenomena. We suggest that in controlling behavior, the brain faces competing behavioral and computational imperatives, and must balance them by tracking their opportunity costs over time. We use this analysis to flesh out previous suggestions that feelings associated with subjective effort, like cognitive fatigue and boredom, are the phenomenological counterparts of these opportunity cost measures, instead of reflecting the depletion of resources as has often been assumed. Specifically, we propose that both fatigue and boredom reflect the competing value of particular options that require foregoing immediate reward but can improve future performance: Fatigue reflects the value of offline computation (internal to the organism) to improve future decisions, while boredom signals the value of exploration (external in the world). We demonstrate that these accounts provide a mechanistically explicit and parsimonious account for a wide array of findings related to cognitive control, integrating and reimagining them under a single, formally rigorous framework.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/M36CICAS/Agrawal et al. - 2020 - The Temporal Dynamics of Opportunity Costs A Norm.pdf}
}

@article{alvernheLocalRemappingPlace2011,
  title = {Local Remapping of Place Cell Firing in the {{Tolman}} Detour Task},
  author = {Alvernhe, Alice and Save, Etienne and Poucet, Bruno},
  date = {2011},
  journaltitle = {European Journal of Neuroscience},
  volume = {33},
  number = {9},
  pages = {1696--1705},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.2011.07653.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-9568.2011.07653.x},
  urldate = {2022-01-26},
  abstract = {The existence of place cells, whose discharge is strongly related to a rat’s location in its environment, has led to the proposal that they form part of an integrated neural system dedicated to spatial navigation. It has been suggested that this system could represent space as a cognitive map, which is flexibly used by animals to plan new shortcuts or efficient detours. To further understand the relationships between hippocampal place cell firing and cognitive maps, we examined the discharge of place cells as rats were exposed to a Tolman-type detour problem. In specific sessions, a transparent barrier was placed onto the maze so as to block the shortest central path between the two rewarded end locations of a familiar three-way maze. We found that rats rapidly and consistently chose the shortest alternative detour. Furthermore, both CA1 and CA3 place cells that had a field in the vicinity of the barrier displayed local remapping. In contrast, neither CA1 nor CA3 cells that had a field away from the barrier were affected. This finding, at odds with our previous report of altered CA3 discharge for distant fields in a shortcut task, suggests that the availability of a novel path and the blocking of a familiar path are not equivalent and could lead to different responses of the CA3 place cell population. Together, the two studies point to a specific role of CA3 in the representation of spatial connectivity and sequences.},
  langid = {english},
  keywords = {hippocampus,network,rat,spatial processing,unit recordings},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-9568.2011.07653.x},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DAEWJFIF/Alvernhe et al. - 2011 - Local remapping of place cell firing in the Tolman.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/E2M5M4H8/j.1460-9568.2011.07653.html}
}

@article{alvernheLocalRemappingPlace2011a,
  title = {Local Remapping of Place Cell Firing in the {{Tolman}} Detour Task},
  author = {Alvernhe, Alice and Save, Etienne and Poucet, Bruno},
  date = {2011},
  journaltitle = {European Journal of Neuroscience},
  volume = {33},
  number = {9},
  pages = {1696--1705},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.2011.07653.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-9568.2011.07653.x},
  urldate = {2022-05-04},
  abstract = {The existence of place cells, whose discharge is strongly related to a rat’s location in its environment, has led to the proposal that they form part of an integrated neural system dedicated to spatial navigation. It has been suggested that this system could represent space as a cognitive map, which is flexibly used by animals to plan new shortcuts or efficient detours. To further understand the relationships between hippocampal place cell firing and cognitive maps, we examined the discharge of place cells as rats were exposed to a Tolman-type detour problem. In specific sessions, a transparent barrier was placed onto the maze so as to block the shortest central path between the two rewarded end locations of a familiar three-way maze. We found that rats rapidly and consistently chose the shortest alternative detour. Furthermore, both CA1 and CA3 place cells that had a field in the vicinity of the barrier displayed local remapping. In contrast, neither CA1 nor CA3 cells that had a field away from the barrier were affected. This finding, at odds with our previous report of altered CA3 discharge for distant fields in a shortcut task, suggests that the availability of a novel path and the blocking of a familiar path are not equivalent and could lead to different responses of the CA3 place cell population. Together, the two studies point to a specific role of CA3 in the representation of spatial connectivity and sequences.},
  langid = {english},
  keywords = {hippocampus,network,rat,spatial processing,unit recordings},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-9568.2011.07653.x},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NNQ79IND/Alvernhe et al. - 2011 - Local remapping of place cell firing in the Tolman.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/U474GH53/j.1460-9568.2011.07653.html}
}

@article{ambroseReverseReplayHippocampal2016,
  title = {Reverse {{Replay}} of {{Hippocampal Place Cells Is Uniquely Modulated}} by {{Changing Reward}}},
  author = {Ambrose, R. Ellen and Pfeiffer, Brad E. and Foster, David J.},
  date = {2016-09},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {91},
  number = {5},
  pages = {1124--1136},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.07.047},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627316304639},
  urldate = {2021-12-08},
  abstract = {Hippocampal replays are episodes of sequential place cell activity during sharp-wave ripple oscillations (SWRs). Conflicting hypotheses implicate awake replay in learning from reward and in memory retrieval for decision making. Further, awake replays can be forward, in the same order as experienced, or reverse, in the opposite order. However, while the presence or absence of reward has been reported to modulate SWR rate, the effect of reward changes on replay, and on replay direction in particular, has not been examined. Here we report divergence in the response of forward and reverse replays to changing reward. While both classes of replays were observed at reward locations, only reverse replays increased their rate at increased reward or decreased their rate at decreased reward, while forward replays were unchanged. These data demonstrate a unique relationship between reverse replay and reward processing and point to a functional distinction between different directions of replay.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/GDSH6KQK/Ambrose et al. - 2016 - Reverse Replay of Hippocampal Place Cells Is Uniqu.pdf}
}

@article{ambroseReverseReplayHippocampal2016a,
  title = {Reverse {{Replay}} of {{Hippocampal Place Cells Is Uniquely Modulated}} by {{Changing Reward}}},
  author = {Ambrose, R. Ellen and Pfeiffer, Brad E. and Foster, David J.},
  date = {2016-09},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {91},
  number = {5},
  pages = {1124--1136},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.07.047},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627316304639},
  urldate = {2021-12-07},
  abstract = {Hippocampal replays are episodes of sequential place cell activity during sharp-wave ripple oscillations (SWRs). Conflicting hypotheses implicate awake replay in learning from reward and in memory retrieval for decision making. Further, awake replays can be forward, in the same order as experienced, or reverse, in the opposite order. However, while the presence or absence of reward has been reported to modulate SWR rate, the effect of reward changes on replay, and on replay direction in particular, has not been examined. Here we report divergence in the response of forward and reverse replays to changing reward. While both classes of replays were observed at reward locations, only reverse replays increased their rate at increased reward or decreased their rate at decreased reward, while forward replays were unchanged. These data demonstrate a unique relationship between reverse replay and reward processing and point to a functional distinction between different directions of replay.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KLTEFRDT/Ambrose et al. - 2016 - Reverse Replay of Hippocampal Place Cells Is Uniqu.pdf}
}

@article{amoGradualTemporalShift2022,
  title = {A Gradual Temporal Shift of Dopamine Responses Mirrors the Progression of Temporal Difference Error in Machine Learning},
  author = {Amo, Ryunosuke and Matias, Sara and Yamanaka, Akihiro and Tanaka, Kenji F. and Uchida, Naoshige and Watabe-Uchida, Mitsuko},
  date = {2022-07-07},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  pages = {1--11},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01109-2},
  url = {https://www.nature.com/articles/s41593-022-01109-2},
  urldate = {2022-07-12},
  abstract = {A large body of evidence has indicated that the phasic responses of midbrain dopamine neurons show a remarkable similarity to a type of teaching signal (temporal difference (TD) error) used in machine learning. However, previous studies failed to observe a key prediction of this algorithm: that when an agent associates a cue and a reward that are separated in time, the timing of dopamine signals should gradually move backward in time from the time of the reward to the time of the cue over multiple trials. Here we demonstrate that such a gradual shift occurs both at the level of dopaminergic cellular activity and dopamine release in the ventral striatum in mice. Our results establish a long-sought link between dopaminergic activity and the TD learning algorithm, providing fundamental insights into how the brain associates cues and rewards that are separated in time.},
  langid = {english},
  keywords = {Learning algorithms,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/J8FBXSEL/Amo et al. - 2022 - A gradual temporal shift of dopamine responses mir.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/QVAWE2KI/s41593-022-01109-2.html}
}

@inproceedings{andrychowiczHindsightExperienceReplay2017,
  title = {Hindsight {{Experience Replay}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html},
  urldate = {2022-10-31},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KFKWDEEA/Andrychowicz et al. - 2017 - Hindsight Experience Replay.pdf}
}

@article{antonovOptimismPessimismOptimised2022,
  title = {Optimism and Pessimism in Optimised Replay},
  author = {Antonov, Georgy and Gagne, Christopher and Eldar, Eran and Dayan, Peter},
  date = {2022-01-12},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {18},
  number = {1},
  pages = {e1009634},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009634},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009634},
  urldate = {2022-02-03},
  abstract = {The replay of task-relevant trajectories is known to contribute to memory consolidation and improved task performance. A wide variety of experimental data show that the content of replayed sequences is highly specific and can be modulated by reward as well as other prominent task variables. However, the rules governing the choice of sequences to be replayed still remain poorly understood. One recent theoretical suggestion is that the prioritization of replay experiences in decision-making problems is based on their effect on the choice of action. We show that this implies that subjects should replay sub-optimal actions that they dysfunctionally choose rather than optimal ones, when, by being forgetful, they experience large amounts of uncertainty in their internal models of the world. We use this to account for recent experimental data demonstrating exactly pessimal replay, fitting model parameters to the individual subjects’ choices.},
  langid = {english},
  keywords = {Agent-based modeling,Algorithms,Behavior,Decision making,Entropy,Learning,Probability distribution,Rodents},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9VDDYIXH/Antonov et al. - 2022 - Optimism and pessimism in optimised replay.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ISXL482D/article.html}
}

@article{aronovMappingNonspatialDimension2017,
  title = {Mapping of a Non-Spatial Dimension by the Hippocampal–Entorhinal Circuit},
  author = {Aronov, Dmitriy and Nevers, Rhino and Tank, David W.},
  date = {2017-03},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {543},
  number = {7647},
  pages = {719--722},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature21692},
  url = {http://www.nature.com/articles/nature21692},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FEPMPV9U/Aronov et al. - 2017 - Mapping of a non-spatial dimension by the hippocam.pdf}
}

@inproceedings{asadiAlternativeSoftmaxOperator2017,
  title = {An {{Alternative Softmax Operator}} for {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Asadi, Kavosh and Littman, Michael L.},
  date = {2017-07-17},
  pages = {243--252},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/asadi17a.html},
  urldate = {2022-02-09},
  abstract = {A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EQ9A5NXJ/Asadi and Littman - 2017 - An Alternative Softmax Operator for Reinforcement .pdf}
}

@article{asmuthBayesianSamplingApproach2009,
  title = {A {{Bayesian Sampling Approach}} to {{Exploration}} in {{Reinforcement Learning}}},
  author = {Asmuth, John and Li, Lihong and Littman, Michael L and Nouri, Ali and Wingate, David},
  date = {2009},
  pages = {8},
  abstract = {We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/VTF6CXM7/Asmuth et al. - 2009 - A Bayesian Sampling Approach to Exploration in Rei.pdf}
}

@article{asoDopaminergicNeuronsWrite2016,
  title = {Dopaminergic Neurons Write and Update Memories with Cell-Type-Specific Rules},
  author = {Aso, Yoshinori and Rubin, Gerald M},
  editor = {Luo, Liqun},
  date = {2016-07-21},
  journaltitle = {eLife},
  volume = {5},
  pages = {e16135},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.16135},
  url = {https://doi.org/10.7554/eLife.16135},
  urldate = {2022-02-18},
  abstract = {Associative learning is thought to involve parallel and distributed mechanisms of memory formation and storage. In Drosophila, the mushroom body (MB) is the major site of associative odor memory formation. Previously we described the anatomy of the adult MB and defined 20 types of dopaminergic neurons (DANs) that each innervate distinct MB compartments (Aso et al., 2014a, 2014b). Here we compare the properties of memories formed by optogenetic activation of individual DAN cell types. We found extensive differences in training requirements for memory formation, decay dynamics, storage capacity and flexibility to learn new associations. Even a single DAN cell type can either write or reduce an aversive memory, or write an appetitive memory, depending on when it is activated relative to odor delivery. Our results show that different learning rules are executed in seemingly parallel memory systems, providing multiple distinct circuit-based strategies to predict future events from past experiences.},
  keywords = {associative learning,mushroom body,neuromodulator,olfaction,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AEKTU8HD/Aso and Rubin - 2016 - Dopaminergic neurons write and update memories wit.pdf}
}

@article{auerUsingConfidenceBounds2002,
  title = {Using {{Conﬁdence Bounds}} for {{Exploitation-Exploration Trade-oﬀs}}},
  author = {Auer, Peter},
  date = {2002},
  pages = {26},
  abstract = {We show how a standard tool from statistics — namely confidence bounds — can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XW6Z7YE6/Auer - Using Conﬁdence Bounds for Exploitation-Exploratio.pdf}
}

@article{averbeckTheoryChoiceBandit2015,
  title = {Theory of {{Choice}} in {{Bandit}}, {{Information Sampling}} and {{Foraging Tasks}}},
  author = {Averbeck, Bruno B.},
  date = {2015-03-27},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {11},
  number = {3},
  pages = {e1004164},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004164},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004164},
  urldate = {2022-10-13},
  abstract = {Decision making has been studied with a wide array of tasks. Here we examine the theoretical structure of bandit, information sampling and foraging tasks. These tasks move beyond tasks where the choice in the current trial does not affect future expected rewards. We have modeled these tasks using Markov decision processes (MDPs). MDPs provide a general framework for modeling tasks in which decisions affect the information on which future choices will be made. Under the assumption that agents are maximizing expected rewards, MDPs provide normative solutions. We find that all three classes of tasks pose choices among actions which trade-off immediate and future expected rewards. The tasks drive these trade-offs in unique ways, however. For bandit and information sampling tasks, increasing uncertainty or the time horizon shifts value to actions that pay-off in the future. Correspondingly, decreasing uncertainty increases the relative value of actions that pay-off immediately. For foraging tasks the time-horizon plays the dominant role, as choices do not affect future uncertainty in these tasks.},
  langid = {english},
  keywords = {Algorithms,Decision making,Foraging,Learning,Markov models,Markov processes,Polynomials,Probability distribution},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BQXQQRHH/Averbeck - 2015 - Theory of Choice in Bandit, Information Sampling a.pdf}
}

@book{axlerUnderstandingAnalysis2001,
  title = {Understanding {{Analysis}}.},
  author = {Axler, S and Gehring, F. W and Ribet, K. A},
  date = {2001},
  publisher = {{Springer New York}},
  location = {{New York}},
  url = {http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3072695},
  urldate = {2022-03-09},
  isbn = {978-0-387-21506-8},
  langid = {english},
  annotation = {OCLC: 927513426},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5I89HE5M/Axler et al. - 2001 - Understanding Analysis..pdf}
}

@article{babichevReplaysSpatialMemories2019,
  title = {Replays of Spatial Memories Suppress Topological Fluctuations in Cognitive Map},
  author = {Babichev, Andrey and Morozov, Dmitriy and Dabaghian, Yuri},
  date = {2019-01},
  journaltitle = {Network Neuroscience},
  shortjournal = {Network Neuroscience},
  volume = {3},
  number = {3},
  pages = {707--724},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00076},
  url = {https://direct.mit.edu/netn/article/3/3/707-724/2175},
  urldate = {2021-12-08},
  abstract = {The spiking activity of the hippocampal place cells plays a key role in producing and sustaining an internalized representation of the ambient space—a cognitive map. These cells do not only exhibit location-specific spiking during navigation, but also may rapidly replay the navigated routs through endogenous dynamics of the hippocampal network. Physiologically, such reactivations are viewed as manifestations of “memory replays” that help to learn new information and to consolidate previously acquired memories by reinforcing synapses in the parahippocampal networks. Below we propose a computational model of these processes that allows assessing the effect of replays on acquiring a robust topological map of the environment and demonstrate that replays may play a key role in stabilizing the hippocampal representation of space.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CE4WRJ6A/Babichev et al. - 2019 - Replays of spatial memories suppress topological f.pdf}
}

@article{babichevTopologicalSchemasMemory2018,
  title = {Topological {{Schemas}} of {{Memory Spaces}}},
  author = {Babichev, Andrey and Dabaghian, Yuri A.},
  date = {2018-04-24},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {12},
  pages = {27},
  issn = {1662-5188},
  doi = {10.3389/fncom.2018.00027},
  url = {http://journal.frontiersin.org/article/10.3389/fncom.2018.00027/full},
  urldate = {2021-12-08},
  abstract = {Hippocampal cognitive map—a neuronal representation of the spatial environment—is widely discussed in the computational neuroscience literature for decades. However, more recent studies point out that hippocampus plays a major role in producing yet another cognitive framework—the memory space—that incorporates not only spatial, but also non-spatial memories. Unlike the cognitive maps, the memory spaces, broadly understood as “networks of interconnections among the representations of events,” have not yet been studied from a theoretical perspective. Here we propose a mathematical approach that allows modeling memory spaces constructively, as epiphenomena of neuronal spiking activity and thus to interlink several important notions of cognitive neurophysiology. First, we suggest that memory spaces have a topological nature—a hypothesis that allows treating both spatial and non-spatial aspects of hippocampal function on equal footing. We then model the hippocampal memory spaces in different environments and demonstrate that the resulting constructions naturally incorporate the corresponding cognitive maps and provide a wider context for interpreting spatial information. Lastly, we propose a formal description of the memory consolidation process that connects memory spaces to the Morris’ cognitive schemas-heuristic representations of the acquired memories, used to explain the dynamics of learning and memory consolidation in a given environment. The proposed approach allows evaluating these constructs as the most compact representations of the memory space’s structure.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZNC7ZXFX/Babichev and Dabaghian - 2018 - Topological Schemas of Memory Spaces.pdf}
}

@report{baramIntuitivePlanningGlobal2018,
  type = {preprint},
  title = {Intuitive Planning: Global Navigation through Cognitive Maps Based on Grid-like Codes},
  shorttitle = {Intuitive Planning},
  author = {Baram, Alon B. and Muller, Timothy H. and Whittington, James C.R. and Behrens, Timothy E.J.},
  date = {2018-09-19},
  institution = {{Neuroscience}},
  doi = {10.1101/421461},
  url = {http://biorxiv.org/lookup/doi/10.1101/421461},
  urldate = {2021-12-07},
  abstract = {It is proposed that a cognitive map encoding the relationships between objects supports the ability to flexibly navigate the world. Place cells and grid cells provide evidence for such a map in a spatial context. Emerging evidence suggests analogous cells code for non-spatial information. Further, it has been shown that grid cells resemble the eigenvectors of the relationship between place cells and can be learnt from local inputs. Here we show that these locally-learnt eigenvectors contain not only local information but also global knowledge that can provide both distributions over future states as well as a global distance measure encoding approximate distances between every object in the world. By simply changing the weights in the grid cell population, it is possible to switch between computing these different measures. We demonstrate a simple algorithm can use these measures to globally navigate arbitrary topologies without searching more than one step ahead. We refer to this as intuitive planning.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AWCLFU6X/Baram et al. - 2018 - Intuitive planning global navigation through cogn.pdf}
}

@unpublished{bariNeuronalNoiseCritique2021,
  title = {A {{Neuronal Noise Critique}} of {{Integrated Information Theory}}},
  author = {Bari, Refath},
  date = {2021-12-09},
  eprint = {2112.03151},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  url = {http://arxiv.org/abs/2112.03151},
  urldate = {2021-12-13},
  abstract = {Integrated Information Theory (IIT) is an audacious attempt to pin down the abstract, phenomenological experiences of consciousness into a rigorous, mathematical framework. We show that IIT's stance in regards to neuronal noise is inconsistent with experimental data demonstrating that neuronal noise in the brain plays a critical role in learning, visual recognition, and even categorical representation. IIT predicts that entropy due to noise will reduce the information integration of a physical system, which is inconsistent with experimental data demonstrating that decision-related noise is a necessary condition for learning and visual recognition tasks. IIT must therefore be reformulated to accommodate experimental evidence showing both the successes and failures of noise.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CJ8PHSWT/Bari - 2021 - A Neuronal Noise Critique of Integrated Informatio.pdf}
}

@article{barretoSuccessorFeaturesTransfer,
  title = {Successor {{Features}} for {{Transfer}} in {{Reinforcement Learning}}},
  author = {Barreto, Andre and Dabney, Will and Munos, Remi and Hunt, Jonathan J and Schaul, Tom},
  pages = {11},
  abstract = {Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment’s dynamics remain the same. Our approach rests on two key ideas: successor features, a value function representation that decouples the dynamics of the environment from the rewards, and generalized policy improvement, a generalization of dynamic programming’s policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/R9YDGSGX/Barreto et al. - Successor Features for Transfer in Reinforcement L.pdf}
}

@article{barronNeuronalComputationUnderlying2020,
  title = {Neuronal {{Computation Underlying Inferential Reasoning}} in {{Humans}} and {{Mice}}},
  author = {Barron, Helen C. and Reeve, Hayley M. and Koolschijn, Renée S. and Perestenko, Pavel V. and Shpektor, Anna and Nili, Hamed and Rothaermel, Roman and Campo-Urriza, Natalia and O’Reilly, Jill X. and Bannerman, David M. and Behrens, Timothy E.J. and Dupret, David},
  date = {2020-10},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {183},
  number = {1},
  pages = {228-243.e21},
  issn = {00928674},
  doi = {10.1016/j.cell.2020.08.035},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867420310771},
  urldate = {2021-12-08},
  abstract = {Every day we make decisions critical for adaptation and survival. We repeat actions with known consequences. But we also draw on loosely related events to infer and imagine the outcome of entirely novel choices. These inferential decisions are thought to engage a number of brain regions; however, the underlying neuronal computation remains unknown. Here, we use a multi-day cross-species approach in humans and mice to report the functional anatomy and neuronal computation underlying inferential decisions. We show that during successful inference, the mammalian brain uses a hippocampal prospective code to forecast temporally structured learned associations. Moreover, during resting behavior, coactivation of hippocampal cells in sharp-wave/ripples represent inferred relationships that include reward, thereby ‘‘joiningthe-dots’’ between events that have not been observed together but lead to profitable outcomes. Computing mnemonic links in this manner may provide an important mechanism to build a cognitive map that stretches beyond direct experience, thus supporting flexible behavior.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/N6EASXJZ/Barron et al. - 2020 - Neuronal Computation Underlying Inferential Reason.pdf}
}

@article{bartoLearningActUsing,
  title = {Learning to Act Using Real-Time Dynamic Programming},
  author = {Barto, Andrew G and Bradtke, Steven J},
  pages = {58},
  abstract = {Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf’s Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins’ Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/D5BKZQFK/Barto and Bradtke - Learning to act using real-time dynamic programmin.pdf}
}

@unpublished{baudryOptimalThompsonSampling2021,
  title = {Optimal {{Thompson Sampling}} Strategies for Support-Aware {{CVaR}} Bandits},
  author = {Baudry, Dorian and Gautron, Romain and Kaufmann, Emilie and Maillard, Odalric-Ambryn},
  date = {2021-07-27},
  eprint = {2012.05754},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.05754},
  urldate = {2021-12-07},
  abstract = {Risk awareness is an important feature to formulate a variety of real world problems. In this paper we study a multi-arm bandit problem in which the quality of each arm is measured by the Conditional Value at Risk (CVaR) at some level α of the reward distribution. While existing works in this setting mainly focus on Upper Confidence Bound algorithms, we introduce the first Thompson Sampling approaches for CVaR bandits. Building on a recent work by Riou and Honda (2020), we propose α-NPTS for bounded rewards and α-Multinomial-TS for multinomial distributions. We provide a novel lower bound on the CVaR regret which extends the concept of asymptotic optimality to CVaR bandits and prove that α-Multinomial-TS is the first algorithm to achieve this lower bound. Finally, we demonstrate empirically the benefit of Thompson Sampling approaches over their UCB counterparts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/GFV2G4PX/Baudry et al. - 2021 - Optimal Thompson Sampling strategies for support-a.pdf}
}

@article{behrensWhatCognitiveMap2018,
  title = {What {{Is}} a {{Cognitive Map}}? {{Organizing Knowledge}} for {{Flexible Behavior}}},
  shorttitle = {What {{Is}} a {{Cognitive Map}}?},
  author = {Behrens, Timothy E.J. and Muller, Timothy H. and Whittington, James C.R. and Mark, Shirley and Baram, Alon B. and Stachenfeld, Kimberly L. and Kurth-Nelson, Zeb},
  date = {2018-10},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {100},
  number = {2},
  pages = {490--509},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.10.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318308560},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/4MQATEVY/Behrens et al. - 2018 - What Is a Cognitive Map Organizing Knowledge for .pdf}
}

@article{bellemareDistributionalPerspectiveReinforcement,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G and Dabney, Will and Munos, Remi},
  pages = {10},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UYA9DDDP/Bellemare et al. - A Distributional Perspective on Reinforcement Lear.pdf}
}

@article{bellmanTheoryDynamicProgramming1954,
  title = {The Theory of Dynamic Programming},
  author = {Bellman, Richard},
  date = {1954},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  volume = {60},
  number = {6},
  pages = {503--515},
  issn = {0002-9904, 1936-881X},
  doi = {10.1090/S0002-9904-1954-09848-8},
  url = {https://www.ams.org/bull/1954-60-06/S0002-9904-1954-09848-8/},
  urldate = {2022-10-05},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TTUWRCSY/Bellman - 1954 - The theory of dynamic programming.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/GIJCZ3I6/S0002-9904-1954-09848-8.html}
}

@report{bennaArePlaceCells2019,
  type = {preprint},
  title = {Are Place Cells Just Memory Cells? {{Memory}} Compression Leads to Spatial Tuning and History Dependence},
  shorttitle = {Are Place Cells Just Memory Cells?},
  author = {Benna, Marcus K. and Fusi, Stefano},
  date = {2019-04-30},
  institution = {{Neuroscience}},
  doi = {10.1101/624239},
  url = {http://biorxiv.org/lookup/doi/10.1101/624239},
  urldate = {2021-12-07},
  abstract = {The observation of place cells has suggested that the hippocampus plays a special role in encoding spatial information. However, place cell responses are modulated by several non-spatial variables, and reported to be rather unstable. Here we propose a memory model of the hippocampus that provides a novel interpretation of place cells consistent with these observations. We hypothesize that the hippocampus is a memory device that takes advantage of the correlations between sensory experiences to generate compressed representations of the episodes that are stored in memory. A simple neural network model that can efficiently compress information naturally produces place cells that are similar to those observed in experiments. It predicts that the activity of these cells is variable and that the fluctuations of the place fields encode information about the recent history of sensory experiences. Place cells may simply be a consequence of a memory compression process implemented in the hippocampus.                        Significance Statement             Numerous studies on humans revealed the importance of the hippocampus in memory formation. The rodent literature instead focused on the spatial representations that are observed in navigation experiments. Here we propose a simple model of the hippocampus that reconciles the main findings of the human and rodent studies. The model assumes that the hippocampus is a memory system that generates compressed representations of sensory experiences using previously acquired knowledge about the statistics of the world. These experiences can then be memorized more efficiently. The sensory experiences during the exploration of an environment, when compressed by the hippocampus, lead naturally to spatial representations similar to those observed in rodent studies and to the emergence of place cells.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7EZN4ZW3/Benna and Fusi - 2019 - Are place cells just memory cells Memory compress.pdf}
}

@article{bennaComputationalPrinciplesSynaptic2016,
  title = {Computational Principles of Synaptic Memory Consolidation},
  author = {Benna, Marcus K and Fusi, Stefano},
  date = {2016-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {12},
  pages = {1697--1706},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4401},
  url = {http://www.nature.com/articles/nn.4401},
  urldate = {2021-12-13},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/88D9T49C/Benna and Fusi - 2016 - Computational principles of synaptic memory consol.pdf}
}

@article{berkeWhatDoesDopamine2018,
  title = {What Does Dopamine Mean?},
  author = {Berke, Joshua D.},
  date = {2018-06},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {21},
  number = {6},
  pages = {787--793},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0152-y},
  url = {https://www.nature.com/articles/s41593-018-0152-y},
  urldate = {2022-02-15},
  abstract = {Dopamine is a critical modulator of both learning and motivation. This presents a problem: how can target cells know whether increased dopamine is a signal to learn or to move? It is often presumed that motivation involves slow (‘tonic’) dopamine changes, while fast (‘phasic’) dopamine fluctuations convey reward prediction errors for learning. Yet recent studies have shown that dopamine conveys motivational value and promotes movement even on subsecond timescales. Here I describe an alternative account of how dopamine regulates ongoing behavior. Dopamine release related to motivation is rapidly and locally sculpted by receptors on dopamine terminals, independently from dopamine cell firing. Target neurons abruptly switch between learning and performance modes, with striatal cholinergic interneurons providing one candidate switch mechanism. The behavioral impact of dopamine varies by subregion, but in each case dopamine provides a dynamic estimate of whether it is worth expending a limited internal resource, such as energy, attention, or time.},
  issue = {6},
  langid = {english},
  keywords = {Motivation,Operant learning,Psychology,Reward,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8GTW2MZ9/Berke - 2018 - What does dopamine mean.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/3M59DE2J/s41593-018-0152-y.html}
}

@article{bernardiGeometryAbstractionHippocampus2020,
  title = {The {{Geometry}} of {{Abstraction}} in the {{Hippocampus}} and {{Prefrontal Cortex}}},
  author = {Bernardi, Silvia and Benna, Marcus K. and Rigotti, Mattia and Munuera, Jérôme and Fusi, Stefano and Salzman, C. Daniel},
  date = {2020-11},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {183},
  number = {4},
  pages = {954-967.e21},
  issn = {00928674},
  doi = {10.1016/j.cell.2020.09.031},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867420312289},
  urldate = {2021-12-07},
  abstract = {The curse of dimensionality plagues models of reinforcement learning and decision making. The process of abstraction solves this by constructing variables describing features shared by different instances, reducing dimensionality and enabling generalization in novel situations. Here, we characterized neural representations in monkeys performing a task described by different hidden and explicit variables. Abstraction was defined operationally using the generalization performance of neural decoders across task conditions not used for training, which requires a particular geometry of neural representations. Neural ensembles in prefrontal cortex, hippocampus, and simulated neural networks simultaneously represented multiple variables in a geometry reflecting abstraction but that still allowed a linear classifier to decode a large number of other variables (high shattering dimensionality). Furthermore, this geometry changed in relation to task events and performance. These findings elucidate how the brain and artificial systems represent variables in an abstract format while preserving the advantages conferred by high shattering dimensionality.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KZGTV9ED/Bernardi et al. - 2020 - The Geometry of Abstraction in the Hippocampus and.pdf}
}

@article{berners-leeHippocampalReplaysAppear2022,
  title = {Hippocampal Replays Appear after a Single Experience and Incorporate Greater Detail with More Experience},
  author = {Berners-Lee, Alice and Feng, Ting and Silva, Delia and Wu, Xiaojing and Ambrose, Ellen R. and Pfeiffer, Brad E. and Foster, David J.},
  date = {2022-04-04},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {0},
  number = {0},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2022.03.010},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(22)00246-X},
  urldate = {2022-04-05},
  langid = {english},
  keywords = {episodic memory,hippocampus,place cell,replay,semantic memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/E68GPXXR/S0896-6273(22)00246-X.html}
}

@article{berners-leeHippocampalReplaysAppear2022a,
  title = {Hippocampal Replays Appear after a Single Experience and Incorporate Greater Detail with More Experience},
  author = {Berners-Lee, Alice and Feng, Ting and Silva, Delia and Wu, Xiaojing and Ambrose, Ellen R. and Pfeiffer, Brad E. and Foster, David J.},
  date = {2022-04-04},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {0},
  number = {0},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2022.03.010},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(22)00246-X},
  urldate = {2022-04-05},
  langid = {english},
  keywords = {episodic memory,hippocampus,place cell,replay,semantic memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FXP3JTFF/Berners-Lee et al. - 2022 - Hippocampal replays appear after a single experien.pdf}
}

@article{bhattaraiDistinctEffectsReward2020,
  title = {Distinct Effects of Reward and Navigation History on Hippocampal Forward and Reverse Replays},
  author = {Bhattarai, Baburam and Lee, Jong Won and Jung, Min Whan},
  date = {2020-01-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {117},
  number = {1},
  pages = {689--697},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1912533117},
  url = {https://pnas.org/doi/full/10.1073/pnas.1912533117},
  urldate = {2022-10-20},
  abstract = {To better understand the functional roles of hippocampal forward and reverse replays, we trained rats in a spatial sequence memory task and examined how these replays are modulated by reward and navigation history. We found that reward enhances both forward and reverse replays during the awake state, but in different ways. Reward enhances the rate of reverse replays, but it increases the fidelity of forward replays for recently traveled as well as other alternative trajectories heading toward a rewarding location. This suggests roles for forward and reverse replays in reinforcing representations for all potential rewarding trajectories. We also found more faithful reactivation of upcoming than already rewarded trajectories in forward replays. This suggests a role for forward replays in preferentially reinforcing representations for high-value trajectories. We propose that hippocampal forward and reverse replays might contribute to constructing a map of potential navigation trajectories and their associated values (a “value map”) via distinct mechanisms.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ML6R256P/Bhattarai et al. - 2020 - Distinct effects of reward and navigation history .pdf}
}

@article{bittnerBehavioralTimeScale2017,
  title = {Behavioral Time Scale Synaptic Plasticity Underlies {{CA1}} Place Fields},
  author = {Bittner, Katie C. and Milstein, Aaron D. and Grienberger, Christine and Romani, Sandro and Magee, Jeffrey C.},
  date = {2017-09-08},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {357},
  number = {6355},
  pages = {1033--1036},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aan3846},
  url = {https://www.science.org/doi/10.1126/science.aan3846},
  urldate = {2021-12-08},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QRGGWAVA/Bittner et al. - 2017 - Behavioral time scale synaptic plasticity underlie.pdf}
}

@article{bleiVariationalInferenceReview2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  date = {2017-04-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  url = {http://arxiv.org/abs/1601.00670},
  urldate = {2022-05-18},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UKSFMBCM/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/2VUU6ADS/1601.html}
}

@article{boccaraEntorhinalCognitiveMap2019,
  title = {The Entorhinal Cognitive Map Is Attracted to Goals},
  author = {Boccara, Charlotte N. and Nardin, Michele and Stella, Federico and O’Neill, Joseph and Csicsvari, Jozsef},
  date = {2019-03-29},
  journaltitle = {Science},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aav4837},
  url = {https://www.science.org/doi/abs/10.1126/science.aav4837},
  urldate = {2022-01-13},
  abstract = {Goal learning in rats leads to a local distortion of grid cell rate maps, suggesting a complex code beyond simply encoding space.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/85VN2I4X/Boccara et al. - 2019 - The entorhinal cognitive map is attracted to goals.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/UBL3TR33/science.html}
}

@article{botvinickPlanningInference2012,
  title = {Planning as Inference},
  author = {Botvinick, Matthew and Toussaint, Marc},
  date = {2012-10-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {16},
  number = {10},
  pages = {485--488},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2012.08.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661312001957},
  urldate = {2022-04-01},
  abstract = {Recent developments in decision-making research are bringing the topic of planning back to center stage in cognitive science. This renewed interest reopens an old, but still unanswered question: how exactly does planning happen? What are the underlying information processing operations and how are they implemented in the brain? Although a range of interesting possibilities exists, recent work has introduced a potentially transformative new idea, according to which planning is accomplished through probabilistic inference.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JR3F5ATE/Botvinick and Toussaint - 2012 - Planning as inference.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/CLVLG879/S1364661312001957.html}
}

@article{botvinickReinforcementLearningFast2019,
  title = {Reinforcement {{Learning}}, {{Fast}} and {{Slow}}},
  author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
  date = {2019-05},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {5},
  pages = {408--422},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319300610},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/58K3IKEL/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf}
}

@article{boureauOpponencyRevisitedCompetition2011,
  title = {Opponency {{Revisited}}: {{Competition}} and {{Cooperation Between Dopamine}} and {{Serotonin}}},
  shorttitle = {Opponency {{Revisited}}},
  author = {Boureau, Y.-Lan and Dayan, Peter},
  date = {2011-01},
  journaltitle = {Neuropsychopharmacology},
  shortjournal = {Neuropsychopharmacol},
  volume = {36},
  number = {1},
  pages = {74--97},
  publisher = {{Nature Publishing Group}},
  issn = {1740-634X},
  doi = {10.1038/npp.2010.151},
  url = {https://www.nature.com/articles/npp2010151},
  urldate = {2022-05-06},
  abstract = {Affective valence lies on a spectrum ranging from punishment to reward. The coding of such spectra in the brain almost always involves opponency between pairs of systems or structures. There is ample evidence for the role of dopamine in the appetitive half of this spectrum, but little agreement about the existence, nature, or role of putative aversive opponents such as serotonin. In this review, we consider the structure of opponency in terms of previous biases about the nature of the decision problems that animals face, the conflicts that may thus arise between Pavlovian and instrumental responses, and an additional spectrum joining invigoration to inhibition. We use this analysis to shed light on aspects of the role of serotonin and its interactions with dopamine.},
  issue = {1},
  langid = {english},
  keywords = {Neurotransmitters,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BRH92EFB/Boureau and Dayan - 2011 - Opponency Revisited Competition and Cooperation B.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/D6Z449WF/npp2010151.html}
}

@article{breton-provencherSpatiotemporalDynamicsNoradrenaline2022,
  title = {Spatiotemporal Dynamics of Noradrenaline during Learned Behaviour},
  author = {Breton-Provencher, Vincent and Drummond, Gabrielle T. and Feng, Jiesi and Li, Yulong and Sur, Mriganka},
  date = {2022-06-01},
  journaltitle = {Nature},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-04782-2},
  url = {https://www.nature.com/articles/s41586-022-04782-2},
  urldate = {2022-06-07},
  abstract = {Noradrenaline released from the locus coeruleus (LC) is a ubiquitous neuromodulator1–4 that has been linked to multiple functions including arousal5–8, action and sensory gain9–11, and learning12–16. Whether and how activation of noradrenaline-expressing neurons in the LC (LC-NA) facilitates different components of specific behaviours is unknown. Here we show that LC-NA activity displays distinct spatiotemporal dynamics to enable two functions during learned behaviour: facilitating task execution and encoding reinforcement to improve performance accuracy. To examine these functions, we used a behavioural task in mice with graded auditory stimulus detection and task performance. Optogenetic inactivation of the LC demonstrated that LC-NA activity was causal for both task execution and optimization. Targeted recordings of LC-NA neurons using photo-tagging, two-photon micro-endoscopy and two-photon output monitoring showed that transient LC-NA activation preceded behavioural execution and followed reinforcement. These two components of phasic activity were heterogeneously represented in LC-NA cortical outputs, such that the behavioural response signal was higher in the motor cortex and facilitated task execution, whereas the negative reinforcement signal was widely distributed among cortical regions and improved response sensitivity on the subsequent trial. Modular targeting of LC outputs thus enables diverse functions, whereby some noradrenaline signals are segregated among targets, whereas others are broadly distributed.},
  langid = {english},
  keywords = {Decision,Neural circuits,Operant learning,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/GQVXKBIL/Breton-Provencher et al. - 2022 - Spatiotemporal dynamics of noradrenaline during le.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/2YLCCIVG/s41586-022-04782-2.html}
}

@article{breton-provencherSpatiotemporalDynamicsNoradrenaline2022a,
  title = {Spatiotemporal Dynamics of Noradrenaline during Learned Behaviour},
  author = {Breton-Provencher, Vincent and Drummond, Gabrielle T. and Feng, Jiesi and Li, Yulong and Sur, Mriganka},
  date = {2022-06-01},
  journaltitle = {Nature},
  shortjournal = {Nature},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-022-04782-2},
  url = {https://www.nature.com/articles/s41586-022-04782-2},
  urldate = {2022-06-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LD8KUMYF/Breton-Provencher et al. - 2022 - Spatiotemporal dynamics of noradrenaline during le.pdf}
}

@article{breton-provencherSpatiotemporalDynamicsNoradrenaline2022b,
  title = {Spatiotemporal Dynamics of Noradrenaline during Learned Behaviour},
  author = {Breton-Provencher, Vincent and Drummond, Gabrielle T. and Feng, Jiesi and Li, Yulong and Sur, Mriganka},
  date = {2022-06-01},
  journaltitle = {Nature},
  shortjournal = {Nature},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-022-04782-2},
  url = {https://www.nature.com/articles/s41586-022-04782-2},
  urldate = {2022-06-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6MWF5U4D/Breton-Provencher et al. - 2022 - Spatiotemporal dynamics of noradrenaline during le.pdf}
}

@unpublished{brittainPrioritizedSequenceExperience2020,
  title = {Prioritized {{Sequence Experience Replay}}},
  author = {Brittain, Marc and Bertram, Josh and Yang, Xuxi and Wei, Peng},
  date = {2020-02-19},
  eprint = {1905.12726},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.12726},
  urldate = {2021-12-08},
  abstract = {Experience replay is widely used in deep reinforcement learning algorithms and allows agents to remember and learn from experiences from the past. In an effort to learn more efficiently, researchers proposed prioritized experience replay (PER) which samples important transitions more frequently. In this paper, we propose Prioritized Sequence Experience Replay (PSER) a framework for prioritizing sequences of experience in an attempt to both learn more efficiently and to obtain better performance. We compare the performance of PER and PSER sampling techniques in a tabular Q-learning environment and in DQN on the Atari 2600 benchmark. We prove theoretically that PSER is guaranteed to converge faster than PER and empirically show PSER substantially improves upon PER.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AYY9SWGD/Brittain et al. - 2020 - Prioritized Sequence Experience Replay.pdf}
}

@article{brucknerUnderstandingLearningUncertainty2022,
  title = {Understanding {{Learning Through Uncertainty}} and {{Bias}}},
  author = {Bruckner, Rasmus and Heekeren, Hauke R. and Nassar, Matthew R.},
  date = {2022-06-02T09:49:21},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/xjkbg},
  url = {https://psyarxiv.com/xjkbg/},
  urldate = {2022-06-07},
  abstract = {Learning allows humans and animals to make predictions about the environment and increases the likelihood of adaptive behavior. Studying the mechanisms through which humans regulate learning to update their predictions in the face of uncertainty can also shed light on maladaptive behaviors in various clinical and age groups. Drawing on normative learning models, we illustrate how learning should be calibrated to different sources of uncertainty, including perceptual uncertainty, risk, and environmental changes. We show that such models explain many hallmarks of human learning and highlight research on their underlying neural mechanisms. Systematic deviations from normative learning, which have been explored through computational psychiatry, provide a useful lens through which to understand psychiatric illness. In particular, normative models provide important insights into why different clinical and age groups show maladaptive learning behavior, thereby attempting to pinpoint the computational cause of dysfunctional decision making. We conclude by discussing relevant goals for future research.},
  langid = {american},
  keywords = {Behavioral Neuroscience,Cognitive Neuroscience,Cognitive Psychology,Computational Modeling,Computational Neuroscience,Computational Psychiatry,Decision Making,Learning,Neuroscience,Psychiatry,Social and Behavioral Sciences,To read,Uncertainty},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/L75H7CJ5/Bruckner et al. - 2022 - Understanding Learning Through Uncertainty and Bia.pdf}
}

@article{brusSourcesConfidenceValuebased2021,
  title = {Sources of Confidence in Value-Based Choice},
  author = {Brus, Jeroen and Aebersold, Helena and Grueschow, Marcus and Polania, Rafael},
  date = {2021-12-17},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {7337},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-27618-5},
  url = {https://www.nature.com/articles/s41467-021-27618-5},
  urldate = {2021-12-21},
  abstract = {Confidence, the subjective estimate of decision quality, is a cognitive process necessary for learning from mistakes and guiding future actions. The origins of confidence judgments resulting from economic decisions remain unclear. We devise a task and computational framework that allowed us to formally tease apart the impact of various sources of confidence in value-based decisions, such as uncertainty emerging from encoding and decoding operations, as well as the interplay between gaze-shift dynamics and attentional effort. In line with canonical decision theories, trial-to-trial fluctuations in the precision of value encoding impact economic choice consistency. However, this uncertainty has no influence on confidence reports. Instead, confidence is associated with endogenous attentional effort towards choice alternatives and down-stream noise in the comparison process. These findings provide an explanation for confidence (miss)attributions in value-guided behaviour, suggesting mechanistic influences of endogenous attentional states for guiding decisions and metacognitive awareness of choice certainty.},
  issue = {1},
  langid = {english},
  keywords = {Decision,Human behaviour},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Decision;Human behaviour Subject\_term\_id: decision;human-behaviour},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/26HVG7X9/Brus et al. - 2021 - Sources of confidence in value-based choice.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/5NDKHWLN/s41467-021-27618-5.html}
}

@article{burgessModelsPlaceGrid2011,
  title = {Models of Place and Grid Cell Firing and Theta Rhythmicity},
  author = {Burgess, Neil and O’Keefe, John},
  date = {2011-10},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {21},
  number = {5},
  pages = {734--744},
  issn = {09594388},
  doi = {10.1016/j.conb.2011.07.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438811001255},
  urldate = {2021-12-08},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/X8RS7UQH/Burgess and O’Keefe - 2011 - Models of place and grid cell firing and theta rhy.pdf}
}

@article{burnetasOptimalAdaptivePolicies1997,
  title = {Optimal {{Adaptive Policies}} for {{Markov Decision Processes}}},
  author = {Burnetas, Apostolos N. and Katehakis, Michael N.},
  date = {1997-02},
  journaltitle = {Mathematics of Operations Research},
  shortjournal = {Mathematics of OR},
  volume = {22},
  number = {1},
  pages = {222--255},
  issn = {0364-765X, 1526-5471},
  doi = {10.1287/moor.22.1.222},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.22.1.222},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/Q89WG45E/Burnetas and Katehakis - 1997 - Optimal Adaptive Policies for Markov Decision Proc.pdf}
}

@article{bushRippleBandPhase,
  title = {Ripple {{Band Phase Precession}} of {{Place Cell Firing}} during {{Replay}}},
  author = {Bush, Daniel},
  pages = {22},
  abstract = {Phase coding offers several theoretical advantages for information transmission compared to an equivalent rate code. Phase coding is shown by place cells in the rodent hippocampal formation, which fire at progressively earlier phases of the movement related 6-12Hz theta rhythm as their spatial receptive fields are traversed. Importantly, however, phase coding is independent of carrier frequency, and so we asked whether it might also be exhibited by place cells during 150-250Hz ripple band activity, when they are thought to replay information to neocortex. We demonstrate that place cells which fire multiple spikes during candidate replay events do so at progressively earlier ripple phases, and that spikes fired across all replay events exhibit a negative relationship between decoded location within the firing field and ripple phase. These results provide insights into the mechanisms underlying phase coding and place cell replay, as well as the neural code propagated to downstream neurons.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/SZ6SHPEE/Bush - Ripple Band Phase Precession of Place Cell Firing .pdf}
}

@article{buzsakiHippocampalSharpWaveripple2015,
  title = {Hippocampal Sharp Wave-Ripple: {{A}} Cognitive Biomarker for Episodic Memory and Planning},
  shorttitle = {Hippocampal Sharp Wave-Ripple},
  author = {Buzsáki, György},
  date = {2015},
  journaltitle = {Hippocampus},
  volume = {25},
  number = {10},
  pages = {1073--1188},
  issn = {1098-1063},
  doi = {10.1002/hipo.22488},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.22488},
  urldate = {2022-10-13},
  abstract = {Sharp wave ripples (SPW-Rs) represent the most synchronous population pattern in the mammalian brain. Their excitatory output affects a wide area of the cortex and several subcortical nuclei. SPW-Rs occur during “off-line” states of the brain, associated with consummatory behaviors and non-REM sleep, and are influenced by numerous neurotransmitters and neuromodulators. They arise from the excitatory recurrent system of the CA3 region and the SPW-induced excitation brings about a fast network oscillation (ripple) in CA1. The spike content of SPW-Rs is temporally and spatially coordinated by a consortium of interneurons to replay fragments of waking neuronal sequences in a compressed format. SPW-Rs assist in transferring this compressed hippocampal representation to distributed circuits to support memory consolidation; selective disruption of SPW-Rs interferes with memory. Recently acquired and pre-existing information are combined during SPW-R replay to influence decisions, plan actions and, potentially, allow for creative thoughts. In addition to the widely studied contribution to memory, SPW-Rs may also affect endocrine function via activation of hypothalamic circuits. Alteration of the physiological mechanisms supporting SPW-Rs leads to their pathological conversion, “p-ripples,” which are a marker of epileptogenic tissue and can be observed in rodent models of schizophrenia and Alzheimer's Disease. Mechanisms for SPW-R genesis and function are discussed in this review. © 2015 The Authors Hippocampus Published by Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {epilepsy,imagining,learning,memory,planning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.22488},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KAAKPL26/Buzsáki - 2015 - Hippocampal sharp wave-ripple A cognitive biomark.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ZS75DCWP/hipo.html}
}

@article{buzsakiMemoryNavigationTheta2013,
  title = {Memory, Navigation and Theta Rhythm in the Hippocampal-Entorhinal System},
  author = {Buzsáki, György and Moser, Edvard I},
  date = {2013-02},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {16},
  number = {2},
  pages = {130--138},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3304},
  url = {http://www.nature.com/articles/nn.3304},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/2MCP5I8I/Buzsáki and Moser - 2013 - Memory, navigation and theta rhythm in the hippoca.pdf}
}

@article{buzsakiSpaceTimeBrain2017,
  title = {Space and Time in the Brain},
  author = {Buzsáki, György and Llinás, Rodolfo},
  date = {2017-10-27},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {358},
  number = {6362},
  pages = {482--485},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aan8869},
  url = {https://www.science.org/doi/10.1126/science.aan8869},
  urldate = {2021-12-08},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/WMXVZU4P/Buzsáki and Llinás - 2017 - Space and time in the brain.pdf}
}

@article{buzsakiSpaceTimeHippocampus2018,
  title = {Space and {{Time}}: {{The Hippocampus}} as a {{Sequence Generator}}},
  shorttitle = {Space and {{Time}}},
  author = {Buzsáki, György and Tingley, David},
  date = {2018-10},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {22},
  number = {10},
  pages = {853--869},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.07.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661318301669},
  urldate = {2021-12-08},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/VUH2TDZM/Buzsaki_Tingley2018.pdf}
}

@unpublished{callawayLearningSelectComputations2018,
  title = {Learning to Select Computations},
  author = {Callaway, Frederick and Gul, Sayan and Krueger, Paul M. and Griffiths, Thomas L. and Lieder, Falk},
  date = {2018-08-07},
  eprint = {1711.06892},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.06892},
  urldate = {2022-04-04},
  abstract = {The efficient use of limited computational resources is an essential ingredient of intelligence. Selecting computations optimally according to rational metareasoning would achieve this, but this is computationally intractable. Inspired by psychology and neuroscience, we propose the first concrete and domain-general learning algorithm for approximating the optimal selection of computations: Bayesian metalevel policy search (BMPS). We derive this general, sample-efficient search algorithm for a computation-selecting metalevel policy based on the insight that the value of information lies between the myopic value of information and the value of perfect information. We evaluate BMPS on three increasingly difficult metareasoning problems: when to terminate computation, how to allocate computation between competing options, and planning. Across all three domains, BMPS achieved near-optimal performance and compared favorably to previously proposed metareasoning heuristics. Finally, we demonstrate the practical utility of BMPS in an emergency management scenario, even accounting for the overhead of metareasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DMFZUVA9/Callaway et al. - 2018 - Learning to select computations.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/5JU4UPRU/1711.html}
}

@article{carpenterGridCellsForm2015,
  title = {Grid {{Cells Form}} a {{Global Representation}} of {{Connected Environments}}},
  author = {Carpenter, Francis and Manson, Daniel and Jeffery, Kate and Burgess, Neil and Barry, Caswell},
  date = {2015-05-04},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {25},
  number = {9},
  pages = {1176--1182},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.02.037},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982215002067},
  urldate = {2022-01-27},
  abstract = {The firing patterns of grid cells in medial entorhinal cortex (mEC) and associated brain areas form triangular arrays that tessellate the environment [1, 2] and maintain constant spatial offsets to each other between environments [3, 4]. These cells are thought to provide an efficient metric for navigation in large-scale space [5, 6, 7, 8]. However, an accurate and universal metric requires grid cell firing patterns to uniformly cover the space to be navigated, in contrast to recent demonstrations that environmental features such as~boundaries can distort [9, 10, 11] and fragment [12] grid~patterns. To establish whether grid firing is determined by local environmental cues, or provides a coherent global representation, we recorded mEC grid cells in rats foraging in an environment containing two perceptually identical compartments connected via a corridor. During initial exposures to the multicompartment environment, grid firing patterns were dominated by local environmental cues, replicating between the two compartments. However, with prolonged experience, grid cell firing patterns formed a single, continuous representation that spanned both compartments. Thus, we provide the first evidence that in a complex environment, grid cell firing can form the coherent global pattern necessary for them to act as a metric capable of supporting large-scale spatial navigation.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HQRN6KNT/Carpenter et al. - 2015 - Grid Cells Form a Global Representation of Connect.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/8JWAQF9Z/S0960982215002067.html}
}

@unpublished{caselles-dupreSTRIGGERContinualState2021,
  title = {S-{{TRIGGER}}: {{Continual State Representation Learning}} via {{Self-Triggered Generative Replay}}},
  shorttitle = {S-{{TRIGGER}}},
  author = {Caselles-Dupré, Hugo and Garcia-Ortiz, Michael and Filliat, David},
  date = {2021-07-05},
  eprint = {1902.09434},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.09434},
  urldate = {2021-12-08},
  abstract = {We consider the problem of building a state representation model for control, in a continual learning setting. As the environment changes, the aim is to efficiently compress the sensory state’s information without losing past knowledge, and then use Reinforcement Learning on the resulting features for efficient policy learning. To this end, we propose S-TRIGGER, a general method for Continual State Representation Learning applicable to Variational Auto-Encoders and its many variants. The method is based on Generative Replay, i.e. the use of generated samples to maintain past knowledge. It comes along with a statistically sound method for environment change detection, which self-triggers the Generative Replay. Our experiments on VAEs show that S-TRIGGER learns state representations that allows fast and high-performing Reinforcement Learning, while avoiding catastrophic forgetting. The resulting system is capable of autonomously learning new information without using past data and with a bounded system size. Code for our experiments is attached in Appendix.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CSNSDZM6/Caselles-Dupré et al. - 2021 - S-TRIGGER Continual State Representation Learning.pdf}
}

@article{cazeHippocampalReplaysScrutiny2018,
  title = {Hippocampal Replays under the Scrutiny of Reinforcement Learning Models},
  author = {Cazé, Romain and Khamassi, Mehdi and Aubin, Lise and Girard, Benoît},
  date = {2018-12-01},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  volume = {120},
  number = {6},
  pages = {2877--2896},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00145.2018},
  url = {https://www.physiology.org/doi/10.1152/jn.00145.2018},
  urldate = {2021-12-07},
  abstract = {Multiple in vivo studies have shown that place cells from the hippocampus replay previously experienced trajectories. These replays are commonly considered to mainly reflect memory consolidation processes. Some data, however, have highlighted a functional link between replays and reinforcement learning (RL). This theory, extensively used in machine learning, has introduced efficient algorithms and can explain various behavioral and physiological measures from different brain regions. RL algorithms could constitute a mechanistic description of replays and explain how replays can reduce the number of iterations required to explore the environment during learning. We review the main findings concerning the different hippocampal replay types and the possible associated RL models (either model-based, model-free, or hybrid model types). We conclude by tying these frameworks together. We illustrate the link between data and RL through a series of model simulations. This review, at the frontier between informatics and biology, paves the way for future work on replays.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KSVDJV9G/Cazé et al. - 2018 - Hippocampal replays under the scrutiny of reinforc.pdf}
}

@article{cazeHippocampalReplaysScrutiny2018a,
  title = {Hippocampal Replays under the Scrutiny of Reinforcement Learning Models},
  author = {Cazé, Romain and Khamassi, Mehdi and Aubin, Lise and Girard, Benoît},
  date = {2018-12-01},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  volume = {120},
  number = {6},
  pages = {2877--2896},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00145.2018},
  url = {https://www.physiology.org/doi/10.1152/jn.00145.2018},
  urldate = {2021-12-07},
  abstract = {Multiple in vivo studies have shown that place cells from the hippocampus replay previously experienced trajectories. These replays are commonly considered to mainly reflect memory consolidation processes. Some data, however, have highlighted a functional link between replays and reinforcement learning (RL). This theory, extensively used in machine learning, has introduced efficient algorithms and can explain various behavioral and physiological measures from different brain regions. RL algorithms could constitute a mechanistic description of replays and explain how replays can reduce the number of iterations required to explore the environment during learning. We review the main findings concerning the different hippocampal replay types and the possible associated RL models (either model-based, model-free, or hybrid model types). We conclude by tying these frameworks together. We illustrate the link between data and RL through a series of model simulations. This review, at the frontier between informatics and biology, paves the way for future work on replays.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ISG8VH3D/Cazé et al. - 2018 - Hippocampal replays under the scrutiny of reinforc.pdf}
}

@article{ceiReversedThetaSequences2014,
  title = {Reversed Theta Sequences of Hippocampal Cell Assemblies during Backward Travel},
  author = {Cei, Anne and Girardeau, Gabrielle and Drieu, Céline and Kanbi, Karim El and Zugaro, Michaël},
  date = {2014-05},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {17},
  number = {5},
  pages = {719--724},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3698},
  url = {http://www.nature.com/articles/nn.3698},
  urldate = {2021-12-08},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FCZ8J76Y/Cei et al. - 2014 - Reversed theta sequences of hippocampal cell assem.pdf}
}

@inproceedings{cesa-bianchiBoltzmannExplorationDone2017,
  title = {Boltzmann {{Exploration Done Right}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cesa-Bianchi, Nicolò and Gentile, Claudio and Lugosi, Gabor and Neu, Gergely},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/b299ad862b6f12cb57679f0538eca514-Abstract.html},
  urldate = {2022-02-09},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/L2UMDULU/Cesa-Bianchi et al. - 2017 - Boltzmann Exploration Done Right.pdf}
}

@article{chafeePrefrontalCortex2022,
  title = {Prefrontal Cortex},
  author = {Chafee, Matthew V. and Heilbronner, Sarah R.},
  date = {2022-04-25},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {32},
  number = {8},
  pages = {R346-R351},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2022.02.071},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982222003414},
  urldate = {2022-05-04},
  abstract = {The prefrontal cortex is a well-studied but, in terms of understanding what it is for, deeply divisive part of the brain located at the front of the head. Perhaps the least controversial feature of the prefrontal cortex is its complexity. The prefrontal cortex is anatomically, functionally, and computationally complex. It is anatomically complex, containing a number of subregions each sending and receiving projections to a unique set of other cortical and subcortical areas. This interconnectivity presents a serious challenge to efforts to localize function to prefrontal cortex, because it can seem as though information flows everywhere all at once in prefrontal networks. Perhaps as a result, prefrontal cortex is also computationally complex: working memory, abstraction, sensory attention, value-based decision making, planning, and motor control are all functions that have been attributed to the prefrontal cortex. This diversity of functions is likely to reflect the diversity of brain regions that prefrontal cortex communicates with while carrying out the computations it performs to influence behavior.},
  langid = {english},
  keywords = {PFC},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/VI58GNGH/Chafee and Heilbronner - 2022 - Prefrontal cortex.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/77QYRQQK/S0960982222003414.html}
}

@article{chapelleEmpiricalEvaluationThompson,
  title = {An {{Empirical Evaluation}} of {{Thompson Sampling}}},
  author = {Chapelle, Olivier and Li, Lihong},
  pages = {9},
  abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/C9VQQ5KV/Chapelle and Li - An Empirical Evaluation of Thompson Sampling.pdf}
}

@article{chaudhuriComputationalPrinciplesMemory2016,
  title = {Computational Principles of Memory},
  author = {Chaudhuri, Rishidev and Fiete, Ila},
  date = {2016-03},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {3},
  pages = {394--403},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4237},
  url = {https://www.nature.com/articles/nn.4237},
  urldate = {2022-03-27},
  abstract = {What are the challenges associated with storing information over time in the brain? Here the authors explore the computational principles by which biological memory might be built. They develop a high-level view of shared problems and themes in short- and long-term memory and highlight questions for future research.},
  issue = {3},
  langid = {english},
  keywords = {Dynamical systems,Learning algorithms,Long-term memory,Short-term memory,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/L4QX3WU5/Chaudhuri and Fiete - 2016 - Computational principles of memory.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/X7T43UEF/nn.html}
}

@article{chenDifferentialInfluencesEnvironment2019,
  title = {Differential Influences of Environment and Self-Motion on Place and Grid Cell Firing},
  author = {Chen, Guifen and Lu, Yi and King, John A and Cacucci, Francesca and Burgess, Neil},
  date = {2019-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {630},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-08550-1},
  url = {http://www.nature.com/articles/s41467-019-08550-1},
  urldate = {2021-12-08},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XB6BK9M8/Chen et al. - 2019 - Differential influences of environment and self-mo.pdf}
}

@article{choiComparisonDopaminergicCholinergic2020,
  title = {A {{Comparison}} of {{Dopaminergic}} and {{Cholinergic Populations Reveals Unique Contributions}} of {{VTA Dopamine Neurons}} to {{Short-Term Memory}}},
  author = {Choi, Jung Yoon and Jang, Hee Jae and Ornelas, Sharon and Fleming, Weston T. and Fürth, Daniel and Au, Jennifer and Bandi, Akhil and Engel, Esteban A. and Witten, Ilana B.},
  date = {2020-12-15},
  journaltitle = {Cell Reports},
  shortjournal = {Cell Reports},
  volume = {33},
  number = {11},
  eprint = {33326775},
  eprinttype = {pmid},
  publisher = {{Elsevier}},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2020.108492},
  url = {https://www.cell.com/cell-reports/abstract/S2211-1247(20)31481-9},
  urldate = {2022-01-10},
  langid = {english},
  keywords = {acetylcholine,dopamine,Inverted-U,Medial septum,Neuromodulation,Nucleus basalis,Short-term memory,SNc,To read,VTA,Working memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6439U348/Choi et al. - 2020 - A Comparison of Dopaminergic and Cholinergic Popul.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/MQKGY5C8/S2211-1247(20)31481-9.html}
}

@article{cisekEvolutionBehaviouralControl2022,
  title = {Evolution of Behavioural Control from Chordates to Primates},
  author = {Cisek, Paul},
  date = {2022-02-14},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {377},
  number = {1844},
  pages = {20200522},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2020.0522},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0522},
  urldate = {2022-01-10},
  abstract = {This article outlines a hypothetical sequence of evolutionary innovations, along the lineage that produced humans, which extended behavioural control from simple feedback loops to sophisticated control of diverse species-typical actions. I begin with basic feedback mechanisms of ancient mobile animals and follow the major niche transitions from aquatic to terrestrial life, the retreat into nocturnality in early mammals, the transition to arboreal life and the return to diurnality. Along the way, I propose a sequence of elaboration and diversification of the behavioural repertoire and associated neuroanatomical substrates. This includes midbrain control of approach versus escape actions, telencephalic control of local versus long-range foraging, detection of affordances by the dorsal pallium, diversified control of nocturnal foraging in the mammalian neocortex and expansion of primate frontal, temporal and parietal cortex to support a wide variety of primate-specific behavioural strategies. The result is a proposed functional architecture consisting of parallel control systems, each dedicated to specifying the affordances for guiding particular species-typical actions, which compete against each other through a hierarchy of selection mechanisms. This article is part of the theme issue ‘Systems neuroscience through the lens of evolutionary theory’.},
  keywords = {action maps,affordances,ethology,phylogenetic refinement,sensorimotor control,To read,vertebrate evolution},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/P8G6S3IQ/Cisek - 2022 - Evolution of behavioural control from chordates to.pdf}
}

@article{collinsDichotomiesReinforcementLearning2020,
  title = {Beyond Dichotomies in Reinforcement Learning},
  author = {Collins, Anne G. E. and Cockburn, Jeffrey},
  date = {2020-10},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {21},
  number = {10},
  pages = {576--586},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0355-6},
  url = {https://www.nature.com/articles/s41583-020-0355-6},
  urldate = {2021-12-08},
  abstract = {Reinforcement learning (RL) is a framework of particular importance to psychology, neuroscience and machine learning. Interactions between these fields, as promoted through the common hub of RL, has facilitated paradigm shifts that relate multiple levels of analysis in a singular framework (for example, relating dopamine function to a computationally defined RL signal). Recently, more sophisticated RL algorithms have been proposed to better account for human learning, and in particular its oft-documented reliance on two separable systems: a model-based (MB) system and a model-free (MF) system. However, along with many benefits, this dichotomous lens can distort questions, and may contribute to an unnecessarily narrow perspective on learning and decision-making. Here, we outline some of the consequences that come from overconfidently mapping algorithms, such as MB versus MF RL, with putative cognitive processes. We argue that the field is well positioned to move beyond simplistic dichotomies, and we propose a means of refocusing research questions towards the rich and complex components that comprise learning and decision-making.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UI8R4S48/Collins and Cockburn - 2020 - Beyond dichotomies in reinforcement learning.pdf}
}

@article{constantinescuOrganizingConceptualKnowledge2016,
  title = {Organizing Conceptual Knowledge in Humans with a Gridlike Code},
  author = {Constantinescu, Alexandra O. and O’Reilly, Jill X. and Behrens, Timothy E. J.},
  date = {2016-06-17},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {352},
  number = {6292},
  pages = {1464--1468},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf0941},
  url = {https://www.science.org/doi/10.1126/science.aaf0941},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/P8IQFVIY/Constantinescu et al. - 2016 - Organizing conceptual knowledge in humans with a g.pdf}
}

@article{coolsSerotoninergicRegulationEmotional2008,
  title = {Serotoninergic Regulation of Emotional and Behavioural Control Processes},
  author = {Cools, Roshan and Roberts, Angela C. and Robbins, Trevor W.},
  date = {2008-01-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {12},
  number = {1},
  pages = {31--40},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2007.10.011},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661307003051},
  urldate = {2022-04-04},
  abstract = {5-Hydroxytryptamine (5-HT, serotonin) has long been implicated in a wide variety of emotional, cognitive and behavioural control processes. However, its precise contribution is still not well understood. Depletion of 5-HT enhances behavioural and brain responsiveness to punishment or other aversive signals, while disinhibiting previously rewarded but now punished behaviours. Findings suggest that 5-HT modulates the impact of punishment-related signals on learning and emotion (aversion), but also promotes response inhibition. Exaggerated aversive processing and deficient response inhibition could underlie distinct symptoms of a range of affective disorders, namely stress- or threat-vulnerability and compulsive behaviour, respectively. We review evidence from studies with human volunteers and experimental animals that begins to elucidate the neurobiological systems underlying these different effects.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/33PI2T33/Cools et al. - 2008 - Serotoninergic regulation of emotional and behavio.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/KZDWTTLJ/S1364661307003051.html}
}

@report{costaRoleOrbitofrontalCortex2022,
  type = {preprint},
  title = {The Role of the Orbitofrontal Cortex in Creating Cognitive Maps},
  author = {Costa, Kauê Machado and Scholz, Robert and Lloyd, Kevin and Moreno-Castilla, Perla and Gardner, Matthew P. H. and Dayan, Peter and Schoenbaum, Geoffrey},
  date = {2022-01-26},
  institution = {{Neuroscience}},
  doi = {10.1101/2022.01.25.477716},
  url = {http://biorxiv.org/lookup/doi/10.1101/2022.01.25.477716},
  urldate = {2022-10-05},
  abstract = {Abstract                        We use internal models of the external world to guide behavior, but little is known about how these cognitive maps are             created             . The orbitofrontal cortex (OFC) is typically thought to access these maps to support model-based decision-making, but it has recently been proposed that its critical contribution may be instead to integrate information into existing and new models. We tested between these alternatives using an outcome-specific devaluation task and a high-potency chemogenetic approach. We found that selectively inactivating OFC principal neurons when rats learned distinct cue-outcome associations, but prior to outcome devaluation, disrupted subsequent model-based inference, confirming that the OFC is critical for creating new cognitive maps. However, OFC inactivation surprisingly led to generalized devaluation. Using a novel reinforcement learning framework, we demonstrate that this effect is best explained not by a switch to a model-free system, as would be traditionally assumed, but rather by a circumscribed deficit in defining credit assignment precision during model construction. We conclude that the critical contribution of the OFC to learning is regulating the specificity of associations that comprise cognitive maps.                                   One Sentence Summary             OFC inactivation impairs learning of new specific cue-outcome associations without disrupting model-based learning in general.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HBMTE24A/Costa et al. - 2022 - The role of the orbitofrontal cortex in creating c.pdf}
}

@article{costaSubcorticalSubstratesExploreexploit2019,
  title = {Subcortical Substrates of Explore-Exploit Decisions in Primates},
  author = {Costa, Vincent D. and Mitz, Andrew R. and Averbeck, Bruno B.},
  date = {2019-08-07},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {103},
  number = {3},
  eprint = {31196672},
  eprinttype = {pmid},
  pages = {533-545.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.05.017},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6687547/},
  urldate = {2022-10-13},
  abstract = {The explore-exploit dilemma refers to the challenge of deciding when to forego immediate rewards and explore new opportunities that could lead to greater rewards in the future. While motivational neural circuits facilitate reinforcement learning based on past choices and outcomes, it is unclear if they also support computations relevant for deciding when to explore. We recorded neural activity in the amygdala and ventral striatum of rhesus macaques, as they solved a task that required them to balance novelty driven exploration with exploitation of what they had already learned. Using a partially-bserved Markov decision process model to quantify explore-exploit trade-offs, we identified that the ventral striatum and amygdala differ in how they represent the immediate value of exploitative choices and the future value of exploratory choices. These findings show that subcortical motivational circuits are important in guiding exploratory decisions., How do we decide when to explore a new opportunity or stick with what we know? Costa et al. reveal that neurons in amygdala and ventral striatum, motivational centers of the brain, help to solve this complex reinforcement learning problem.},
  pmcid = {PMC6687547},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NPKHNQ3X/Costa et al. - 2019 - Subcortical substrates of explore-exploit decision.pdf}
}

@article{cozzolinoMarkovianDecisionProcesses1965,
  title = {Markovian {{Decision Processes}} with {{Uncertain Transition Probabilities}}},
  author = {Cozzolino, John M and Gonzalez-Zubieta, Romulo and Miller, Ralph L},
  date = {1965-03-01},
  pages = {117},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JDKTFL2C/Cozzolino et al. - MARKOVIAN DECISION PROCESSES WITH UNCERTAIN TRANSI.pdf}
}

@article{cruzActionSuppressionReveals2022,
  title = {Action Suppression Reveals Opponent Parallel Control via Striatal Circuits},
  author = {Cruz, Bruno F. and Guiomar, Gonçalo and Soares, Sofia and Motiwala, Asma and Machens, Christian K. and Paton, Joseph J.},
  date = {2022-07},
  journaltitle = {Nature},
  volume = {607},
  number = {7919},
  pages = {521--526},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-04894-9},
  url = {https://www.nature.com/articles/s41586-022-04894-9},
  urldate = {2022-10-12},
  abstract = {The direct and indirect pathways of the basal ganglia are classically thought to promote and suppress action, respectively1. However, the observed co-activation of striatal direct and indirect medium spiny neurons2 (dMSNs and iMSNs, respectively) has challenged this view. Here we study these circuits in mice performing an interval categorization task that requires a series of self-initiated and cued actions and, critically, a sustained period of dynamic action suppression. Although movement produced the co-activation of iMSNs and dMSNs in the sensorimotor, dorsolateral striatum (DLS), fibre photometry and photo-identified electrophysiological recordings revealed signatures of functional opponency between the two pathways during action suppression. Notably, optogenetic inhibition showed that DLS circuits were largely engaged to suppress—and not promote—action. Specifically, iMSNs on a given hemisphere were dynamically engaged to suppress tempting contralateral action. To understand how such regionally specific circuit function arose, we constructed a computational reinforcement learning model that reproduced key features of behaviour, neural activity and optogenetic inhibition. The model predicted that parallel striatal circuits outside the DLS learned the action-promoting functions, generating the temptation to act. Consistent with this, optogenetic inhibition experiments revealed that dMSNs in the associative, dorsomedial striatum, in contrast to those in the DLS, promote contralateral actions. These data highlight how opponent interactions between multiple circuit- and region-specific basal ganglia processes can lead to behavioural control, and establish a critical role for the sensorimotor indirect pathway in the proactive suppression of tempting actions.},
  issue = {7919},
  langid = {english},
  keywords = {Basal ganglia,Learning algorithms},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BRXGFAQN/Cruz et al. - 2022 - Action suppression reveals opponent parallel contr.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/L8BEPJ7M/s41586-022-04894-9.html}
}

@article{curiEfficientModelBasedReinforcement,
  title = {Efficient {{Model-Based Reinforcement Learning}} through {{Optimistic Policy Search}} and {{Planning}}},
  author = {Curi, Sebastian and Berkenkamp, Felix and Krause, Andreas},
  pages = {15},
  abstract = {Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these two uncertainties for learning the model, they ignore it when optimizing the policy, which leads to greedy and insufficient exploration. At the same time, there are no practical solvers for optimistic exploration algorithms. In this paper, we propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL reparameterizes the set of plausible models and hallucinates control directly on the epistemic uncertainty. By augmenting the input space with the hallucinated inputs, H-UCRL can be solved using standard greedy planners. Furthermore, we analyze H-UCRL and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration significantly speeds-up learning when there are penalties on actions, a setting that is notoriously difficult for existing model-based reinforcement learning algorithms.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5RWBIGVR/Curi et al. - Efﬁcient Model-Based Reinforcement Learning throug.pdf}
}

@article{dabneyDistributionalReinforcementLearning2018,
  title = {Distributional {{Reinforcement Learning With Quantile Regression}}},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc and Munos, Rémi},
  date = {2018-04-29},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11791},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11791},
  urldate = {2022-07-26},
  abstract = {In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/MZCXKCJ2/Dabney et al. - 2018 - Distributional Reinforcement Learning With Quantil.pdf}
}

@inproceedings{dadashiValueFunctionPolytope2019,
  title = {The {{Value Function Polytope}} in {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Dadashi, Robert and Taiga, Adrien Ali and Roux, Nicolas Le and Schuurmans, Dale and Bellemare, Marc G.},
  date = {2019-05-24},
  pages = {1486--1495},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/dadashi19a.html},
  urldate = {2022-03-10},
  abstract = {We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective and introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/B3I2IZ5R/Dadashi et al. - 2019 - The Value Function Polytope in Reinforcement Learn.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/M66XIVBG/Dadashi et al. - 2019 - The Value Function Polytope in Reinforcement Learn.pdf}
}

@article{dagNeuronalReactivationPostlearning2019,
  title = {Neuronal Reactivation during Post-Learning Sleep Consolidates Long-Term Memory in {{Drosophila}}},
  author = {Dag, Ugur and Lei, Zhengchang and Le, Jasmine Q and Wong, Allan and Bushey, Daniel and Keleman, Krystyna},
  editor = {Calabrese, Ronald L and Ramaswami, Mani and Ramaswami, Mani and Montell, Craig},
  date = {2019-02-25},
  journaltitle = {eLife},
  volume = {8},
  pages = {e42786},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.42786},
  url = {https://doi.org/10.7554/eLife.42786},
  urldate = {2022-10-27},
  abstract = {Animals consolidate some, but not all, learning experiences into long-term memory. Across the animal kingdom, sleep has been found to have a beneficial effect on the consolidation of recently formed memories into long-term storage. However, the underlying mechanisms of sleep dependent memory consolidation are poorly understood. Here, we show that consolidation of courtship long-term memory in Drosophila is mediated by reactivation during sleep of dopaminergic neurons that were earlier involved in memory acquisition. We identify specific fan-shaped body neurons that induce sleep after the learning experience and activate dopaminergic neurons for memory consolidation. Thus, we provide a direct link between sleep, neuronal reactivation of dopaminergic neurons, and memory consolidation.},
  keywords = {long-term memory consolidation,neuronal reactivation,sleep},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/Y5VE2ND7/Dag et al. - 2019 - Neuronal reactivation during post-learning sleep c.pdf}
}

@article{davidsonHippocampalReplayExtended2009,
  title = {Hippocampal {{Replay}} of {{Extended Experience}}},
  author = {Davidson, Thomas J. and Kloosterman, Fabian and Wilson, Matthew A.},
  date = {2009-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {63},
  number = {4},
  pages = {497--507},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.07.027},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627309005820},
  urldate = {2021-12-07},
  abstract = {During pauses in exploration, ensembles of place cells in the rat hippocampus re-express firing sequences corresponding to recent spatial experience. Such ‘‘replay’’ co-occurs with ripple events: shortlasting (\$50–120 ms), high-frequency (\$200 Hz) oscillations that are associated with increased hippocampal-cortical communication. In previous studies, rats exploring small environments showed replay anchored to the rat’s current location and compressed in time into a single ripple event. Here, we show, using a neural decoding approach, that firing sequences corresponding to long runs through a large environment are replayed with high fidelity and that such replay can begin at remote locations on the track. Extended replay proceeds at a characteristic virtual speed of \$8 m/s and remains coherent across trains of ripple events. These results suggest that extended replay is composed of chains of shorter subsequences, which may reflect a strategy for the storage and flexible expression of memories of prolonged experience.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZFIW3FFP/Davidson et al. - 2009 - Hippocampal Replay of Extended Experience.pdf}
}

@article{dawCorticalSubstratesExploratory2006,
  title = {Cortical Substrates for Exploratory Decisions in Humans},
  author = {Daw, Nathaniel D. and O'Doherty, John P. and Dayan, Peter and Seymour, Ben and Dolan, Raymond J.},
  date = {2006-06},
  journaltitle = {Nature},
  volume = {441},
  number = {7095},
  pages = {876--879},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature04766},
  url = {https://www.nature.com/articles/nature04766},
  urldate = {2022-08-16},
  abstract = {Humans are remarkably curious, and that is useful in helping us to learn about new environments and possibilities. But curiosity killed the cat, they say, and it also carries with it substantial potential risks and costs for us. Statisticians, engineers and economists have long considered ways of balancing the costs and benefits of exploration. Tests involving a gambling task and an fMRI brain scanner now show that humans appear to obey similar principles when considering their options. The players had to balance the desire to select the richest option based on accumulated experience against the desire to choose a less familiar option that might have a larger payoff. The frontopolar cortex, a brain area known to be involved in cognitive control, was preferentially active during exploratory decisions. The results suggest a neurobiological account of human exploration and point to a new area for behavioural and neural investigations.},
  issue = {7095},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BIG22U36/Daw et al. - 2006 - Cortical substrates for exploratory decisions in h.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/QUQD8TRF/nature04766.html}
}

@article{dawTrialbytrialDataAnalysis,
  title = {Trial-by-Trial Data Analysis Using Computational Models},
  author = {Daw, Nathaniel D},
  pages = {26},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IXJFT7G9/Daw - Trial-by-trial data analysis using computational m.pdf}
}

@article{dawUncertaintybasedCompetitionPrefrontal2005,
  title = {Uncertainty-Based Competition between Prefrontal and Dorsolateral Striatal Systems for Behavioral Control},
  author = {Daw, Nathaniel D. and Niv, Yael and Dayan, Peter},
  date = {2005-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {8},
  number = {12},
  pages = {1704--1711},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1560},
  url = {https://www.nature.com/articles/nn1560},
  urldate = {2022-08-16},
  abstract = {A broad range of neural and behavioral data suggests that the brain contains multiple systems for behavioral choice, including one associated with prefrontal cortex and another with dorsolateral striatum. However, such a surfeit of control raises an additional choice problem: how to arbitrate between the systems when they disagree. Here, we consider dual-action choice systems from a normative perspective, using the computational theory of reinforcement learning. We identify a key trade-off pitting computational simplicity against the flexible and statistically efficient use of experience. The trade-off is realized in a competition between the dorsolateral striatal and prefrontal systems. We suggest a Bayesian principle of arbitration between them according to uncertainty, so each controller is deployed when it should be most accurate. This provides a unifying account of a wealth of experimental evidence about the factors favoring dominance by either system.},
  issue = {12},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/C7I94BJF/Daw et al. - 2005 - Uncertainty-based competition between prefrontal a.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/9NDIMJC7/nn1560.html}
}

@article{dayanDecisionTheoryReinforcement2008,
  title = {Decision Theory, Reinforcement Learning, and the Brain},
  author = {Dayan, P. and Daw, N. D.},
  date = {2008-12-01},
  journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
  shortjournal = {Cognitive, Affective, \& Behavioral Neuroscience},
  volume = {8},
  number = {4},
  pages = {429--453},
  issn = {1530-7026, 1531-135X},
  doi = {10.3758/CABN.8.4.429},
  url = {http://link.springer.com/10.3758/CABN.8.4.429},
  urldate = {2021-12-08},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5WN3N73K/Dayan and Daw - 2008 - Decision theory, reinforcement learning, and the b.pdf}
}

@article{dayanDecisionTheoryReinforcement2008a,
  title = {Decision Theory, Reinforcement Learning, and the Brain},
  author = {Dayan, Peter and Daw, Nathaniel D.},
  date = {2008-12-01},
  journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
  shortjournal = {Cognitive, Affective, \& Behavioral Neuroscience},
  volume = {8},
  number = {4},
  pages = {429--453},
  issn = {1531-135X},
  doi = {10.3758/CABN.8.4.429},
  url = {https://doi.org/10.3758/CABN.8.4.429},
  urldate = {2022-05-20},
  abstract = {Decision making is a core competence for animals and humans acting and surviving in environments they only partially comprehend, gaining rewards and punishments for their troubles. Decision-theoretic concepts permeate experiments and computational models in ethology, psychology, and neuroscience. Here, we review a well-known, coherent Bayesian approach to decision making, showing how it unifies issues in Markovian decision problems, signal detection psychophysics, sequential sampling, and optimal exploration and discuss paradigmatic psychological and neural examples of each problem. We discuss computational issues concerning what subjects know about their task and how ambitious they are in seeking optimal solutions; we address algorithmic topics concerning model-based and model-free methods for making choices; and we highlight key aspects of the neural implementation of decision making.},
  langid = {english},
  keywords = {Belief State,Ective State,Lateral Intraparietal Area,Markov Decision Problem,Temporal Difference Model},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZI98DP2T/Dayan and Daw - 2008 - Decision theory, reinforcement learning, and the b.pdf}
}

@article{dayanExplorationBonusesDual1996,
  title = {Exploration Bonuses and Dual Control},
  author = {Dayan, Peter and Sejnowski, Terrence J.},
  date = {1996-10},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {25},
  number = {1},
  pages = {5--22},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00115298},
  url = {http://link.springer.com/10.1007/BF00115298},
  urldate = {2021-12-08},
  abstract = {Finding the Bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable. This paper shows how to compute suboptimal estimates based on a certainty equivalence approximation (Cozzolino, Gonzalez-Zubieta \& Miller, 1965) arising from a form of dual control. This systematizes and extends existing uses of exploration bonuses in reinforcement learning (Sutton, 1990). The approach has two components: a statistical model of uncertainty in the world and a way of turning this into exploratory behavior. This general approach is applied to two-dimensional mazes with moveable barriers and its performance is compared with Sutton’s DYNA system.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QPR9VAB2/Dayan and Sejnowski - 1996 - Exploration bonuses and dual control.pdf}
}

@article{dayanExplorationBonusesDual1996a,
  title = {Exploration Bonuses and Dual Control},
  author = {Dayan, Peter and Sejnowski, Terrence J.},
  date = {1996-10},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {25},
  number = {1},
  pages = {5--22},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00115298},
  url = {http://link.springer.com/10.1007/BF00115298},
  urldate = {2022-08-15},
  abstract = {Finding the Bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable. This paper shows how to compute suboptimal estimates based on a certainty equivalence approximation (Cozzolino, Gonzalez-Zubieta \& Miller, 1965) arising from a form of dual control. This systematizes and extends existing uses of exploration bonuses in reinforcement learning (Sutton, 1990). The approach has two components: a statistical model of uncertainty in the world and a way of turning this into exploratory behavior. This general approach is applied to two-dimensional mazes with moveable barriers and its performance is compared with Sutton’s DYNA system.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EDPBXFLQ/Dayan and Sejnowski - 1996 - Exploration bonuses and dual control.pdf}
}

@article{dayanImprovingGeneralizationTemporal1993,
  title = {Improving {{Generalization}} for {{Temporal Difference Learning}}: {{The Successor Representation}}},
  shorttitle = {Improving {{Generalization}} for {{Temporal Difference Learning}}},
  author = {Dayan, Peter},
  date = {1993-07},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {5},
  number = {4},
  pages = {613--624},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1993.5.4.613},
  url = {https://direct.mit.edu/neco/article/5/4/613-624/5736},
  urldate = {2021-12-07},
  abstract = {Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations. Appropriate generalization between states is determined by how similar their successors are, and representations should follow suit. This paper shows how TD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/P4HTKQN2/Dayan - 1993 - Improving Generalization for Temporal Difference L.pdf}
}

@article{dayanMisbehaviorValueDiscipline2006,
  title = {The Misbehavior of Value and the Discipline of the Will},
  author = {Dayan, Peter and Niv, Yael and Seymour, Ben and D. Daw, Nathaniel},
  date = {2006-10-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  series = {Neurobiology of {{Decision Making}}},
  volume = {19},
  number = {8},
  pages = {1153--1160},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2006.03.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608006001481},
  urldate = {2022-01-27},
  abstract = {Most reinforcement learning models of animal conditioning operate under the convenient, though fictive, assumption that Pavlovian conditioning concerns prediction learning whereas instrumental conditioning concerns action learning. However, it is only through Pavlovian responses that Pavlovian prediction learning is evident, and these responses can act against the instrumental interests of the subjects. This can be seen in both experimental and natural circumstances. In this paper we study the consequences of importing this competition into a reinforcement learning context, and demonstrate the resulting effects in an omission schedule and a maze navigation task. The misbehavior created by Pavlovian values can be quite debilitating; we discuss how it may be disciplined.},
  langid = {english},
  keywords = {Autoshaping,Classical conditioning,Instrumental conditioning,Negative automaintenance,Reinforcement learning,To read,Will},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/PX3M57ZH/Dayan et al. - 2006 - The misbehavior of value and the discipline of the.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/MUH9ZSSP/S0893608006001481.html}
}

@article{dayanModelbasedModelfreePavlovian2014,
  title = {Model-Based and Model-Free {{Pavlovian}} Reward Learning: {{Revaluation}}, Revision, and Revelation},
  shorttitle = {Model-Based and Model-Free {{Pavlovian}} Reward Learning},
  author = {Dayan, Peter and Berridge, Kent C.},
  date = {2014-06},
  journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
  shortjournal = {Cogn Affect Behav Neurosci},
  volume = {14},
  number = {2},
  pages = {473--492},
  issn = {1530-7026, 1531-135X},
  doi = {10.3758/s13415-014-0277-8},
  url = {http://link.springer.com/10.3758/s13415-014-0277-8},
  urldate = {2021-12-08},
  abstract = {Evidence supports at least two methods for learning about reward and punishment and making predictions for guiding actions. One method, called model-free, progressively acquires cached estimates of the long-run values of circumstances and actions from retrospective experience. The other method, called model-based, uses representations of the environment, expectations, and prospective calculations to make cognitive predictions of future value. Extensive attention has been paid to both methods in computational analyses of instrumental learning. By contrast, although a full computational analysis has been lacking, Pavlovian learning and prediction has typically been presumed to be solely model-free. Here, we revise that presumption and review compelling evidence from Pavlovian revaluation experiments showing that Pavlovian predictions can involve their own form of model-based evaluation. In model-based Pavlovian evaluation, prevailing states of the body and brain influence value computations, and thereby produce powerful incentive motivations that can sometimes be quite new. We consider the consequences of this revised Pavlovian view for the computational landscape of prediction, response, and choice. We also revisit differences between Pavlovian and instrumental learning in the control of incentive motivation.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NYSPNEYE/Dayan and Berridge - 2014 - Model-based and model-free Pavlovian reward learni.pdf}
}

@article{dayanSerotoninAffectiveControl2009,
  title = {Serotonin in {{Affective Control}}},
  author = {Dayan, Peter and Huys, Quentin J.M.},
  date = {2009-06-01},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {32},
  number = {1},
  pages = {95--126},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.051508.135607},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.051508.135607},
  urldate = {2022-05-06},
  abstract = {Serotonin is a neuromodulator that is extensively entangled in fundamental aspects of brain function and behavior. We present a computational view of its involvement in the control of appetitively and aversively motivated actions. We first describe a range of its effects in invertebrates, endowing specific structurally fixed networks with plasticity at multiple spatial and temporal scales. We then consider its rather widespread distribution in the mammalian brain. We argue that this is associated with a more unified representational and functional role in aversive processing that is amenable to computational analyses with the kinds of reinforcement learning techniques that have helped elucidate dopamine’s role in appetitive behavior. Finally, we suggest that it is only a partial reflection of dopamine because of essential asymmetries between the natural statistics of rewards and punishments.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/L8JXNSN3/Dayan and Huys - 2009 - Serotonin in Affective Control.pdf}
}

@article{deardenModelBasedBayesian,
  title = {Model Based {{Bayesian Exploration}}},
  author = {Dearden, Richard and Friedman, Nir and Andre, David},
  pages = {10},
  abstract = {Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classi­ cal notion of Value of Information- the expected im­ provement in future decision quality arising from the tnformation acquired by exploration. Estimating this quantity requires an assessment of the agent's uncer­ tainty about its current value estimates for states.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AN2R4YQF/Dearden et al. - Model based Bayesian Exploration.pdf}
}

@article{deisenrothGaussianProcessDynamic2009,
  title = {Gaussian Process Dynamic Programming},
  author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward and Peters, Jan},
  date = {2009-03-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  series = {Advances in {{Machine Learning}} and {{Computational Intelligence}}},
  volume = {72},
  number = {7},
  pages = {1508--1524},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2008.12.019},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231209000162},
  urldate = {2022-07-25},
  abstract = {Reinforcement learning (RL) and optimal control of systems with continuous states and actions require approximation techniques in most interesting cases. In this article, we introduce Gaussian process dynamic programming (GPDP), an approximate value function-based RL algorithm. We consider both a classic optimal control problem, where problem-specific prior knowledge is available, and a classic RL problem, where only very general priors can be used. For the classic optimal control problem, GPDP models the unknown value functions with Gaussian processes and generalizes dynamic programming to continuous-valued states and actions. For the RL problem, GPDP starts from a given initial state and explores the state space using Bayesian active learning. To design a fast learner, available data have to be used efficiently. Hence, we propose to learn probabilistic models of the a priori unknown transition dynamics and the value functions on the fly. In both cases, we successfully apply the resulting continuous-valued controllers to the under-actuated pendulum swing up and analyze the performances of the suggested algorithms. It turns out that GPDP uses data very efficiently and can be applied to problems, where classic dynamic programming would be cumbersome.},
  langid = {english},
  keywords = {Bayesian active learning,Dynamic programming,Gaussian processes,Optimal control,Policy learning,Reinforcement learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6K6MTEH3/Deisenroth et al. - 2009 - Gaussian process dynamic programming.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ADX5YFH4/S0925231209000162.html}
}

@article{dengVariableClockUnderlies2022,
  title = {A {{Variable Clock Underlies Internally Generated Hippocampal Sequences}}},
  author = {Deng, Xinyi and Chen, Shizhe and Sosa, Marielena and Karlsson, Mattias P. and Wei, Xue-Xin and Frank, Loren M.},
  date = {2022-05-04},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {42},
  number = {18},
  eprint = {35351831},
  eprinttype = {pmid},
  pages = {3797--3810},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1120-21.2022},
  url = {https://www.jneurosci.org/content/42/18/3797},
  urldate = {2022-05-05},
  abstract = {Humans have the ability to store and retrieve memories with various degrees of specificity, and recent advances in reinforcement learning have identified benefits to learning when past experience is represented at different levels of temporal abstraction. How this flexibility might be implemented in the brain remains unclear. We analyzed the temporal organization of male rat hippocampal population spiking to identify potential substrates for temporally flexible representations. We examined activity both during locomotion and during memory-associated population events known as sharp-wave ripples (SWRs). We found that spiking during SWRs is rhythmically organized with higher event-to-event variability than spiking during locomotion-associated population events. Decoding analyses using clusterless methods further indicate that a similar spatial experience can be replayed in multiple SWRs, each time with a different rhythmic structure whose periodicity is sampled from a log-normal distribution. This variability increases with experience despite the decline in SWR rates that occurs as environments become more familiar. We hypothesize that the variability in temporal organization of hippocampal spiking provides a mechanism for storing experiences with various degrees of specificity. SIGNIFICANCE STATEMENT One of the most remarkable properties of memory is its flexibility: the brain can retrieve stored representations at varying levels of detail where, for example, we can begin with a memory of an entire extended event and then zoom in on a particular episode. The neural mechanisms that support this flexibility are not understood. Here we show that hippocampal sharp-wave ripples, which mark the times of memory replay and are important for memory storage, have a highly variable temporal structure that is well suited to support the storage of memories at different levels of detail.},
  langid = {english},
  keywords = {flexibility,hippocampus,learning,memory,replay,sharp-wave ripple,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FZE7U54T/Deng et al. - 2022 - A Variable Clock Underlies Internally Generated Hi.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/THTP5WKD/3797.html}
}

@article{denovellisHippocampalReplayExperience,
  title = {Hippocampal Replay of Experience at Real-World Speeds},
  author = {Denovellis, Eric L and Gillespie, Anna K and Coulter, Michael E and Sosa, Marielena and Chung, Jason E and Eden, Uri T and Frank, Loren M},
  pages = {41},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7ZS7Q4FU/Denovellis et al. - Hippocampal replay of experience at real-world spe.pdf}
}

@article{denovellisHippocampalReplayExperience2021,
  title = {Hippocampal Replay of Experience at Real-World Speeds},
  author = {Denovellis, Eric L and Gillespie, Anna K and Coulter, Michael E and Sosa, Marielena and Chung, Jason E and Eden, Uri T and Frank, Loren M},
  editor = {Peyrache, Adrien and Behrens, Timothy E and Liu, Yunzhe and Ólafsdóttir, H Freyja},
  date = {2021-09-27},
  journaltitle = {eLife},
  volume = {10},
  pages = {e64505},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.64505},
  url = {https://doi.org/10.7554/eLife.64505},
  urldate = {2022-03-27},
  abstract = {Representations related to past experiences play a critical role in memory and decision-making processes. The rat hippocampus expresses these types of representations during sharp-wave ripple (SWR) events, and previous work identified a minority of SWRs that contain ‘replay’ of spatial trajectories at ∼20x the movement speed of the animal. Efforts to understand replay typically make multiple assumptions about which events to examine and what sorts of representations constitute replay. We therefore lack a clear understanding of both the prevalence and the range of representational dynamics associated with replay. Here, we develop a state space model that uses a combination of movement dynamics of different speeds to capture the spatial content and time evolution of replay during SWRs. Using this model, we find that the large majority of replay events contain spatially coherent, interpretable content. Furthermore, many events progress at real-world, rather than accelerated, movement speeds, consistent with actual experiences.},
  keywords = {dynamics,hippocampus,memory,replay,sharp-wave ripple,state space},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KSY7SCDN/Denovellis et al. - 2021 - Hippocampal replay of experience at real-world spe.pdf}
}

@article{denovellisHippocampalReplayExperience2021a,
  title = {Hippocampal Replay of Experience at Real-World Speeds},
  author = {Denovellis, Eric L and Gillespie, Anna K and Coulter, Michael E and Sosa, Marielena and Chung, Jason E and Eden, Uri T and Frank, Loren M},
  date = {2021-09-27},
  journaltitle = {eLife},
  volume = {10},
  pages = {e64505},
  issn = {2050-084X},
  doi = {10.7554/eLife.64505},
  url = {https://elifesciences.org/articles/64505},
  urldate = {2022-08-12},
  abstract = {Representations related to past experiences play a critical role in memory and decision-making processes. The rat hippocampus expresses these types of representations during sharp-wave ripple (SWR) events, and previous work identified a minority of SWRs that contain ‘replay’ of spatial trajectories at \textasciitilde 20x the movement speed of the animal. Efforts to understand replay typically make multiple assumptions about which events to examine and what sorts of representations constitute replay. We therefore lack a clear understanding of both the prevalence and the range of representational dynamics associated with replay. Here, we develop a state space model that uses a combination of movement dynamics of different speeds to capture the spatial content and time evolution of replay during SWRs. Using this model, we find that the large majority of replay events contain spatially coherent, interpretable content. Furthermore, many events progress at real-world, rather than accelerated, movement speeds, consistent with actual experiences.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7PAQDQYR/Denovellis et al. - 2021 - Hippocampal replay of experience at real-world spe.pdf}
}

@article{derdikmanFragmentationGridCell2009,
  title = {Fragmentation of Grid Cell Maps in a Multicompartment Environment},
  author = {Derdikman, Dori and Whitlock, Jonathan R. and Tsao, Albert and Fyhn, Marianne and Hafting, Torkel and Moser, May-Britt and Moser, Edvard I.},
  date = {2009-10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {12},
  number = {10},
  pages = {1325--1332},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2396},
  url = {https://www.nature.com/articles/nn.2396},
  urldate = {2022-03-28},
  abstract = {The authors recorded neural activity in grid cells while rats ran through a hairpin maze. Their results demonstrate that spatial environments are represented in the entorhinal cortex and hippocampus as a mosaic of discrete submaps corresponding to the geometric structure of the space.},
  issue = {10},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CXFKLPQW/Derdikman et al. - 2009 - Fragmentation of grid cell maps in a multicompartm.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/WI9T2TXI/nn.html}
}

@article{desernoDopamineEnhancesModelfree2021,
  title = {Dopamine Enhances Model-Free Credit Assignment through Boosting of Retrospective Model-Based Inference},
  author = {Deserno, Lorenz and Moran, Rani and Michely, Jochen and Lee, Ying and Dayan, Peter and Dolan, Raymond J},
  editor = {Kahnt, Thorsten},
  date = {2021-12-09},
  journaltitle = {eLife},
  volume = {10},
  pages = {e67778},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.67778},
  url = {https://doi.org/10.7554/eLife.67778},
  urldate = {2021-12-21},
  abstract = {Dopamine is implicated in representing model-free (MF) reward prediction errors a as well as influencing model-based (MB) credit assignment and choice. Putative cooperative interactions between MB and MF systems include a guidance of MF credit assignment by MB inference. Here, we used a double-blind, placebo-controlled, within-subjects design to test an hypothesis that enhancing dopamine levels boosts the guidance of MF credit assignment by MB inference. In line with this, we found that levodopa enhanced guidance of MF credit assignment by MB inference, without impacting MF and MB influences directly. This drug effect correlated negatively with a dopamine-dependent change in purely MB credit assignment, possibly reflecting a trade-off between these two MB components of behavioural control. Our findings of a dopamine boost in MB inference guidance of MF learning highlights a novel DA influence on MB-MF cooperative interactions.},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TXHZ5G6Q/Deserno et al. - 2021 - Dopamine enhances model-free credit assignment thr.pdf}
}

@article{desernoDopamineEnhancesModelfree2021a,
  title = {Dopamine Enhances Model-Free Credit Assignment through Boosting of Retrospective Model-Based Inference},
  author = {Deserno, Lorenz and Moran, Rani and Michely, Jochen and Lee, Ying and Dayan, Peter and Dolan, Raymond J},
  editor = {Kahnt, Thorsten and Büchel, Christian and Cools, Roshan},
  date = {2021-12-09},
  journaltitle = {eLife},
  volume = {10},
  pages = {e67778},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.67778},
  url = {https://doi.org/10.7554/eLife.67778},
  urldate = {2022-04-25},
  abstract = {Dopamine is implicated in representing model-free (MF) reward prediction errors a as well as influencing model-based (MB) credit assignment and choice. Putative cooperative interactions between MB and MF systems include a guidance of MF credit assignment by MB inference. Here, we used a double-blind, placebo-controlled, within-subjects design to test an hypothesis that enhancing dopamine levels boosts the guidance of MF credit assignment by MB inference. In line with this, we found that levodopa enhanced guidance of MF credit assignment by MB inference, without impacting MF and MB influences directly. This drug effect correlated negatively with a dopamine-dependent change in purely MB credit assignment, possibly reflecting a trade-off between these two MB components of behavioural control. Our findings of a dopamine boost in MB inference guidance of MF learning highlight a novel DA influence on MB-MF cooperative interactions.},
  keywords = {dopamine,model-free/model-based,reinforcement learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/A93BQYPM/Deserno et al. - 2021 - Dopamine enhances model-free credit assignment thr.pdf}
}

@article{dibaForwardReverseHippocampal2007,
  title = {Forward and Reverse Hippocampal Place-Cell Sequences during Ripples},
  author = {Diba, Kamran and Buzsáki, György},
  date = {2007-10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {10},
  number = {10},
  pages = {1241--1242},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1961},
  url = {http://www.nature.com/articles/nn1961},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JXBQM2N5/Diba and Buzsáki - 2007 - Forward and reverse hippocampal place-cell sequenc.pdf}
}

@misc{diekmannModelHippocampalReplay2022,
  title = {A {{Model}} of {{Hippocampal Replay Driven}} by {{Experience}} and {{Environmental Structure Facilitates Spatial Learning}}},
  author = {Diekmann, Nicolas and Cheng, Sen},
  date = {2022-07-28},
  pages = {2022.07.26.501588},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.07.26.501588},
  url = {https://www.biorxiv.org/content/10.1101/2022.07.26.501588v1},
  urldate = {2022-08-29},
  abstract = {Replay of neuronal sequences in the hippocampus during resting states and sleep play an important role in learning and memory consolidation. Consistent with these functions, replay sequences have been shown to obey current spatial constraints. Nevertheless, replay does not necessarily reflect previous behavior and can construct never-experienced sequences. Here we propose a stochastic replay mechanism that prioritizes experiences based on three variables: 1. Experience strength, 2. experience similarity, and 3. inhibition of return. Using this prioritized replay mechanism to train reinforcement learning agents leads to far better performance than using random replay. Its performance is close to the state-of-the-art, but biologically unrealistic, algorithm by Mattar \& Daw (2018). Importantly, our model reproduces diverse types of replay because of the stochasticity of the replay mechanism and experience-dependent differences between the three variables. In conclusion, a unified replay mechanism generates diverse replay statistics and is efficient in driving spatial learning.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/U5WWP2L9/Diekmann and Cheng - 2022 - A Model of Hippocampal Replay Driven by Experience.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/7TQNYBXU/2022.07.26.501588v1.html}
}

@inproceedings{dimitrakakisTreeExplorationBayesian2008,
  title = {Tree {{Exploration}} for {{Bayesian RL Exploration}}},
  booktitle = {2008 {{International Conference}} on {{Computational Intelligence}} for {{Modelling Control Automation}}},
  author = {Dimitrakakis, Christos},
  date = {2008-12},
  pages = {1029--1034},
  doi = {10.1109/CIMCA.2008.32},
  abstract = {Research in reinforcement learning has produced algorithms for optimal decision making under uncertainty that fall within two main types. The first employs a Bayesian framework, where optimality improves with increased computational time. This is because the resulting planning task takes the form of a dynamic programming problem on a be-lief tree with an infinite number of states. The second type employs relatively simple algorithm which are shown to suffer small regret within a distribution-free framework. This paper presents a lower bound and a high probability up-per bound on the optimal value function for the nodes in the Bayesian belief tree, which are analogous to similar bounds in POMDPs. The bounds are then used to create more efficient strategies for exploring the tree. The resulting algorithms are compared with the distribution-free algorithm UCB1, as well as a simpler baseline algorithm on multi-armed bandit problems.},
  eventtitle = {2008 {{International Conference}} on {{Computational Intelligence}} for {{Modelling Control Automation}}},
  keywords = {Algorithm design and analysis,Bayesian,Bayesian methods,Computational intelligence,Decision making,Dynamic programming,Exploration,Intelligent systems,Laboratories,Learning,Reinforcement Learning,Tree search,Uncertainty,Upper bound},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JFCBRNBM/Dimitrakakis - 2008 - Tree Exploration for Bayesian RL Exploration.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/MTY4IDT3/5172767.html}
}

@online{DistributionalReinforcementLearning,
  title = {Distributional {{Reinforcement Learning}}},
  url = {https://www.distributional-rl.org/},
  urldate = {2022-03-02},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/L5DXVDGC/www.distributional-rl.org.html}
}

@article{dongDistinctPlaceCell2021,
  title = {Distinct Place Cell Dynamics in {{CA1}} and {{CA3}} Encode Experience in New Environments},
  author = {Dong, Can and Madar, Antoine D. and Sheffield, Mark E. J.},
  date = {2021-05-20},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {2977},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23260-3},
  url = {https://www.nature.com/articles/s41467-021-23260-3},
  urldate = {2022-08-15},
  abstract = {When exploring new environments animals form spatial memories that are updated with experience and retrieved upon re-exposure to the same environment. The hippocampus is thought to support these memory processes, but how this is achieved by different subnetworks such as CA1 and CA3 remains unclear. To understand how hippocampal spatial representations emerge and evolve during familiarization, we performed 2-photon calcium imaging in mice running in new virtual environments and compared the trial-to-trial dynamics of place cells in CA1 and CA3 over days. We find that place fields in CA1 emerge rapidly but tend to shift backwards from trial-to-trial and remap upon re-exposure to the environment a day later. In contrast, place fields in CA3 emerge gradually but show more stable trial-to-trial and day-to-day dynamics. These results reflect different roles in CA1 and CA3 in spatial memory processing during familiarization to new environments and constrain the potential mechanisms that support them.},
  issue = {1},
  langid = {english},
  keywords = {Hippocampus,Spatial memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/F86CA2EX/Dong et al. - 2021 - Distinct place cell dynamics in CA1 and CA3 encode.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/NGH2ZU27/s41467-021-23260-3.html}
}

@article{dordekExtractingGridCell2016,
  title = {Extracting Grid Cell Characteristics from Place Cell Inputs Using Non-Negative Principal Component Analysis},
  author = {Dordek, Yedidyah and Soudry, Daniel and Meir, Ron and Derdikman, Dori},
  editor = {Frank, Michael J},
  date = {2016-03-08},
  journaltitle = {eLife},
  volume = {5},
  pages = {e10094},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.10094},
  url = {https://doi.org/10.7554/eLife.10094},
  urldate = {2022-01-26},
  abstract = {Many recent models study the downstream projection from grid cells to place cells, while recent data have pointed out the importance of the feedback projection. We thus asked how grid cells are affected by the nature of the input from the place cells. We propose a single-layer neural network with feedforward weights connecting place-like input cells to grid cell outputs. Place-to-grid weights are learned via a generalized Hebbian rule. The architecture of this network highly resembles neural networks used to perform Principal Component Analysis (PCA). Both numerical results and analytic considerations indicate that if the components of the feedforward neural network are non-negative, the output converges to a hexagonal lattice. Without the non-negativity constraint, the output converges to a square lattice. Consistent with experiments, grid spacing ratio between the first two consecutive modules is −1.4. Our results express a possible linkage between place cell to grid cell interactions and PCA.},
  keywords = {bat,entorhinal,grid cell,hippocampus,navigation,place cell},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HCQ22R3U/Dordek et al. - 2016 - Extracting grid cell characteristics from place ce.pdf}
}

@article{dragoiDistinctPreplayMultiple2013,
  title = {Distinct Preplay of Multiple Novel Spatial Experiences in the Rat},
  author = {Dragoi, George and Tonegawa, Susumu},
  date = {2013-05-28},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {22},
  pages = {9100--9105},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1306031110},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1306031110},
  urldate = {2022-08-12},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ULNTF6NW/Dragoi and Tonegawa - 2013 - Distinct preplay of multiple novel spatial experie.pdf}
}

@article{drieuHippocampalSequencesExploration2019,
  title = {Hippocampal {{Sequences During Exploration}}: {{Mechanisms}} and {{Functions}}},
  shorttitle = {Hippocampal {{Sequences During Exploration}}},
  author = {Drieu, Céline and Zugaro, Michaël},
  date = {2019},
  journaltitle = {Frontiers in Cellular Neuroscience},
  volume = {13},
  issn = {1662-5102},
  url = {https://www.frontiersin.org/article/10.3389/fncel.2019.00232},
  urldate = {2022-03-07},
  abstract = {Although the hippocampus plays a critical role in spatial and episodic memories, the mechanisms underlying memory formation, stabilization, and recall for adaptive behavior remain relatively unknown. During exploration, within single cycles of the ongoing theta rhythm that dominates hippocampal local field potentials, place cells form precisely ordered sequences of activity. These neural sequences result from the integration of both external inputs conveying sensory-motor information, and intrinsic network dynamics possibly related to memory processes. Their endogenous replay during subsequent sleep is critical for memory consolidation. The present review discusses possible mechanisms and functions of hippocampal theta sequences during exploration. We present several lines of evidence suggesting that these neural sequences play a key role in information processing and support the formation of initial memory traces, and discuss potential functional distinctions between neural sequences emerging during theta vs. awake sharp-wave ripples.},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/48VQ49YH/Drieu and Zugaro - 2019 - Hippocampal Sequences During Exploration Mechanis.pdf}
}

@article{dubreuilRolePopulationStructure2022,
  title = {The Role of Population Structure in Computations through Neural Dynamics},
  author = {Dubreuil, Alexis and Valente, Adrian and Beiran, Manuel and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  date = {2022-06},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {25},
  number = {6},
  pages = {783--794},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01088-4},
  url = {https://www.nature.com/articles/s41593-022-01088-4},
  urldate = {2022-07-12},
  abstract = {Neural computations are currently investigated using two separate approaches: sorting neurons into functional subpopulations or examining the low-dimensional dynamics of collective activity. Whether and how these two aspects interact to shape computations is currently unclear. Using a novel approach to extract computational mechanisms from networks trained on neuroscience tasks, here we show that the dimensionality of the dynamics and subpopulation structure play fundamentally complementary roles. Although various tasks can be implemented by increasing the dimensionality in networks with fully random population structure, flexible input–output mappings instead require a non-random population structure that can be described in terms of multiple subpopulations. Our analyses revealed that such a subpopulation structure enables flexible computations through a mechanism based on gain-controlled modulations that flexibly shape the collective dynamics. Our results lead to task-specific predictions for the structure of neural selectivity, for inactivation experiments and for the implication of different neurons in multi-tasking.},
  issue = {6},
  langid = {english},
  keywords = {Dynamical systems,Network models},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/R5D58GI5/Dubreuil et al. - 2022 - The role of population structure in computations t.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/HLWH56KZ/s41593-022-01088-4.html}
}

@article{duffOptimalLearning2002,
  title = {Optimal Learning: Computational Procedures for {{Bayes-adaptive Markov}} Decision Processes},
  author = {{Michael O'Gordon Duff}},
  date = {2002-02},
  journaltitle = {PhD Thesis},
  url = {https://scholarworks.umass.edu/dissertations/AAI3039353/},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/Z56IWG38/Optimal learning computational procedures for Bayes-adaptive Markov decision processes.pdf}
}

@incollection{duffQLearningBanditProblems1995,
  title = {Q-{{Learning}} for {{Bandit Problems}}},
  booktitle = {Machine {{Learning Proceedings}} 1995},
  author = {Duff, Michael O.},
  editor = {Prieditis, Armand and Russell, Stuart},
  date = {1995-01-01},
  pages = {209--217},
  publisher = {{Morgan Kaufmann}},
  location = {{San Francisco (CA)}},
  doi = {10.1016/B978-1-55860-377-6.50034-7},
  url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500347},
  urldate = {2021-12-12},
  abstract = {Multi-armed bandits may be viewed as decompositionally-structured Markov decision processes (MDP's) with potentially very-large state sets. A particularly elegant methodology for computing optimal policies was developed over twenty ago by Gittins [Gittins \& Jones, 1974]. Gittins' approach reduces the problem of finding optimal policies for the original MDP to a sequence of low-dimensional stopping problems whose solutions determine the optimal policy through the so-called “Gittins indices.” Katehakis and Veinott [Katehakis \& Veinott, 1987] have shown that the Gittins index for a process in state i may be interpreted as a particular component of the maximum-value function associated with the “restart-in-i” process, a simple MDP to which standard solution methods for computing optimal policies, such as successive approximation, apply. This paper explores the problem of learning the Gittins indices on-line without the aid of a process model; it suggests utilizing process-state- specific Q-learning agents to solve their respective restart-in-state-i subproblems, and includes an example in which the online reinforcement learning approach is applied to a problem of stochastic scheduling-one instance drawn from a wide class of problems that may be formulated as bandit problems.},
  isbn = {978-1-55860-377-6},
  langid = {english},
  file = {/home/georgy/Documents/Papers/RL/Duff2015.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/43EQNUK8/B9781558603776500347.html}
}

@article{dupretReorganizationReactivationHippocampal2010,
  title = {The Reorganization and Reactivation of Hippocampal Maps Predict Spatial Memory Performance},
  author = {Dupret, David and O'Neill, Joseph and Pleydell-Bouverie, Barty and Csicsvari, Jozsef},
  date = {2010-08},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {13},
  number = {8},
  pages = {995--1002},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2599},
  url = {https://www.nature.com/articles/nn.2599},
  urldate = {2022-05-17},
  abstract = {The hippocampus has place cells that preferentially fire at a particular location of spatial arena. Dupret et al. report that place fields remapped as a result of goal-directed spatial learning and that sharp wave/ripple reactivation events seen during memory consolidation predicted the strength of subsequent spatial memory. Jeffery and Cacucci highlight this work in their News and View.},
  issue = {8},
  langid = {english},
  keywords = {Learning and memory,Neuronal physiology,Sleep},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/PEZIVSJM/Dupret et al. - 2010 - The reorganization and reactivation of hippocampal.pdf}
}

@article{duvelleInsensitivityPlaceCells2019,
  title = {Insensitivity of Place Cells to the Value of Spatial Goals in a Two-Choice Flexible Navigation Task},
  author = {Duvelle, Éléonore and Grieves, Roddy M. and Hok, Vincent and Poucet, Bruno and Arleo, Angelo and Jeffery, Kate and Save, Etienne},
  date = {2019-01-29},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  pages = {1578--18},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1578-18.2018},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1578-18.2018},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5PHJI6PX/Duvelle et al. - 2019 - Insensitivity of place cells to the value of spati.pdf}
}

@article{ecoffetFirstReturnThen2021,
  title = {First Return, Then Explore},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  date = {2021-02-25},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {590},
  number = {7847},
  pages = {580--586},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03157-9},
  url = {http://www.nature.com/articles/s41586-020-03157-9},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XD4DSCSB/Ecoffet et al. - 2021 - First return, then explore.pdf}
}

@article{edvardsenNavigatingGridPlace2020,
  title = {Navigating with Grid and Place Cells in Cluttered Environments},
  author = {Edvardsen, Vegard and Bicanski, Andrej and Burgess, Neil},
  date = {2020},
  journaltitle = {Hippocampus},
  volume = {30},
  number = {3},
  pages = {220--232},
  issn = {1098-1063},
  doi = {10.1002/hipo.23147},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.23147},
  urldate = {2022-05-16},
  abstract = {Hippocampal formation contains several classes of neurons thought to be involved in navigational processes, in particular place cells and grid cells. Place cells have been associated with a topological strategy for navigation, while grid cells have been suggested to support metric vector navigation. Grid cell-based vector navigation can support novel shortcuts across unexplored territory by providing the direction toward the goal. However, this strategy is insufficient in natural environments cluttered with obstacles. Here, we show how navigation in complex environments can be supported by integrating a grid cell-based vector navigation mechanism with local obstacle avoidance mediated by border cells and place cells whose interconnections form an experience-dependent topological graph of the environment. When vector navigation and object avoidance fail (i.e., the agent gets stuck), place cell replay events set closer subgoals for vector navigation. We demonstrate that this combined navigation model can successfully traverse environments cluttered by obstacles and is particularly useful where the environment is underexplored. Finally, we show that the model enables the simulated agent to successfully navigate experimental maze environments from the animal literature on cognitive mapping. The proposed model is sufficiently flexible to support navigation in different environments, and may inform the design of experiments to relate different navigational abilities to place, grid, and border cell firing.},
  langid = {english},
  keywords = {entorhinal cortex,grid cells,hippocampus,place cells,spatial navigation,To read},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.23147},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EQPNIANM/Edvardsen et al. - 2020 - Navigating with grid and place cells in cluttered .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/5GF25M8R/hipo.html}
}

@article{edwardsExplorationExploitationBayes,
  title = {Exploration and {{Exploitation}} in {{Bayes Sequential Decision Problems}}},
  author = {Edwards, James Anthony},
  pages = {171},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/S4N4C4J3/Edwards - Exploration and Exploitation in Bayes Sequential D.pdf}
}

@article{efroniOnlinePlanningLookahead,
  title = {Online {{Planning}} with {{Lookahead Policies}}},
  author = {Efroni, Yonathan and Ghavamzadeh, Mohammad and Mannor, Shie},
  pages = {10},
  abstract = {Real Time Dynamic Programming (RTDP) is an online algorithm based on Dynamic Programming (DP) that acts by 1-step greedy planning. Unlike DP, RTDP does not require access to the entire state space, i.e., it explicitly handles the exploration. This fact makes RTDP particularly appealing when the state space is large and it is not possible to update all states simultaneously. In this we devise a multi-step greedy RTDP algorithm, which we call h-RTDP, that replaces the 1-step greedy policy with a h-step lookahead policy. We analyze h-RTDP in its exact form and establish that increasing the lookahead horizon, h, results in an improved sample complexity, with the cost of additional computations. This is the first work that proves improved sample complexity as a result of increasing the lookahead horizon in online planning. We then analyze the performance of h-RTDP in three approximate settings: approximate model, approximate value updates, and approximate state representation. For these cases, we prove that the asymptotic performance of h-RTDP remains the same as that of a corresponding approximate DP algorithm, the best one can hope for without further assumptions on the approximation errors.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ETP64AKF/Efroni et al. - Online Planning with Lookahead Policies.pdf}
}

@article{ego-stengelDisruptionRippleassociatedHippocampal2009,
  title = {Disruption of Ripple-Associated Hippocampal Activity during Rest Impairs Spatial Learning in the Rat},
  author = {Ego-Stengel, Valérie and Wilson, Matthew A.},
  date = {2009},
  journaltitle = {Hippocampus},
  shortjournal = {Hippocampus},
  pages = {NA-NA},
  issn = {10509631, 10981063},
  doi = {10.1002/hipo.20707},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/hipo.20707},
  urldate = {2021-12-07},
  abstract = {The hippocampus plays a key role in the acquisition of new memories for places and events. Evidence suggests that the consolidation of these memories is enhanced during sleep. At the neuronal level, reactivation of awake experience in the hippocampus during sharp-wave ripple events, characteristic of slow-wave sleep, has been proposed as a neural mechanism for sleep-dependent memory consolidation. However, a causal relation between sleep reactivation and memory consolidation has not been established. Here we show that disrupting neuronal activity during ripple events impairs spatial learning. We trained rats daily in two identical spatial navigation tasks followed each by a 1-hour rest period. After one of the tasks, stimulation of hippocampal afferents selectively disrupted neuronal activity associated with ripple events without changing the sleep-wake structure. Rats learned the control task significantly faster than the task followed by rest stimulation, indicating that interfering with hippocampal processing during sleep led to decreased learning. VC 2009 Wiley-Liss, Inc.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5S6KLLTJ/Ego-Stengel and Wilson - 2009 - Disruption of ripple-associated hippocampal activi.pdf}
}

@article{eldarRolesOnlineOffline2020,
  title = {The Roles of Online and Offline Replay in Planning},
  author = {Eldar, Eran and Lièvre, Gaëlle and Dayan, Peter and Dolan, Raymond J},
  editor = {Kahnt, Thorsten and Wassum, Kate M and Gershman, Samuel J},
  date = {2020-06-17},
  journaltitle = {eLife},
  volume = {9},
  pages = {e56911},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.56911},
  url = {https://doi.org/10.7554/eLife.56911},
  urldate = {2022-02-03},
  abstract = {Animals and humans replay neural patterns encoding trajectories through their environment, both whilst they solve decision-making tasks and during rest. Both on-task and off-task replay are believed to contribute to flexible decision making, though how their relative contributions differ remains unclear. We investigated this question by using magnetoencephalography (MEG) to study human subjects while they performed a decision-making task that was designed to reveal the decision algorithms employed. We characterised subjects in terms of how flexibly each adjusted their choices to changes in temporal, spatial and reward structure. The more flexible a subject, the more they replayed trajectories during task performance, and this replay was coupled with re-planning of the encoded trajectories. The less flexible a subject, the more they replayed previously preferred trajectories during rest periods between task epochs. The data suggest that online and offline replay both participate in planning but support distinct decision strategies.},
  keywords = {magnetoencephalography,model-based planning,model-free planning,reinforcement learning,replay},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/647ZG226/Eldar et al. - 2020 - The roles of online and offline replay in planning.pdf}
}

@article{eliavMultiscaleRepresentationVery2021,
  title = {Multiscale Representation of Very Large Environments in the Hippocampus of Flying Bats},
  author = {Eliav, Tamir and Maimon, Shir R. and Aljadeff, Johnatan and Tsodyks, Misha and Ginosar, Gily and Las, Liora and Ulanovsky, Nachum},
  date = {2021-05-28},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {372},
  number = {6545},
  pages = {eabg4020},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abg4020},
  url = {https://www.science.org/doi/10.1126/science.abg4020},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JMB34X6U/Eliav et al. - 2021 - Multiscale representation of very large environmen.pdf}
}

@article{ewellReplayRememberBoost2014,
  title = {Replay to Remember: A Boost from Dopamine},
  shorttitle = {Replay to Remember},
  author = {Ewell, Laura A. and Leutgeb, Stefan},
  date = {2014-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {17},
  number = {12},
  pages = {1629--1631},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3875},
  url = {https://www.nature.com/articles/nn.3875},
  urldate = {2022-02-18},
  abstract = {A study links transient activation of the brain's reward system during a novel experience to frequent reactivation of memory traces during sleep and shows that artificial activation of the reward circuit can strengthen memories.},
  issue = {12},
  langid = {english},
  keywords = {Hippocampus,Neurophysiology,Replay},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/S3QWYILZ/Ewell and Leutgeb - 2014 - Replay to remember a boost from dopamine.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/4Z467BW7/nn.html}
}

@inproceedings{eysenbachSearchReplayBuffer2019,
  title = {Search on the {{Replay Buffer}}: {{Bridging Planning}} and {{Reinforcement Learning}}},
  shorttitle = {Search on the {{Replay Buffer}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Eysenbach, Ben and Salakhutdinov, Russ R and Levine, Sergey},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2019/hash/5c48ff18e0a47baaf81d8b8ea51eec92-Abstract.html},
  urldate = {2022-08-07},
  abstract = {The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and relative values of states, but fails to plan over long horizons. Despite the successes of each method on various tasks, long horizon, sparse reward tasks with high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid injecting bias through reward shaping. We introduce a general-purpose control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our main idea is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a particular subgoal. We use goal-conditioned RL to learn a policy to reach each waypoint and to learn a distance metric for search. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over hundreds of steps, and generalizes substantially better than standard RL algorithms.},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QAZXA3UP/Eysenbach et al. - 2019 - Search on the Replay Buffer Bridging Planning and.pdf}
}

@article{fakhariDetourProblemStochastic2018,
  title = {The Detour Problem in a Stochastic Environment: {{Tolman}} Revisited},
  shorttitle = {The Detour Problem in a Stochastic Environment},
  author = {Fakhari, Pegah and Khodadadi, Arash and Busemeyer, Jerome R.},
  date = {2018-03-01},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {101},
  pages = {29--49},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2017.12.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0010028517301858},
  urldate = {2022-05-05},
  abstract = {We designed a grid world task to study human planning and re-planning behavior in an unknown stochastic environment. In our grid world, participants were asked to travel from a random starting point to a random goal position while maximizing their reward. Because they were not familiar with the environment, they needed to learn its characteristics from experience to plan optimally. Later in the task, we randomly blocked the optimal path to investigate whether and how people adjust their original plans to find a detour. To this end, we developed and compared 12 different models. These models were different on how they learned and represented the environment and how they planned to catch the goal. The majority of our participants were able to plan optimally. We also showed that people were capable of revising their plans when an unexpected event occurred. The result from the model comparison showed that the model-based reinforcement learning approach provided the best account for the data and outperformed heuristics in explaining the behavioral data in the re-planning trials.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ASLU8HN8/Fakhari et al. - 2018 - The detour problem in a stochastic environment To.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/K5VB2CWA/S0010028517301858.html}
}

@inproceedings{fedusRevisitingFundamentalsExperience2020,
  title = {Revisiting {{Fundamentals}} of {{Experience Replay}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  date = {2020-11-21},
  pages = {3061--3071},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/fedus20a.html},
  urldate = {2022-10-31},
  abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay \{—\} greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AMLMDKJ6/Fedus et al. - 2020 - Revisiting Fundamentals of Experience Replay.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/KR5QD45R/Fedus et al. - 2020 - Revisiting Fundamentals of Experience Replay.pdf}
}

@article{feldbaumDualControlTheory1965,
  title = {Dual control theory.\textasciitilde I},
  author = {Feldbaum, A A},
  date = {1965},
  pages = {11},
  langid = {russian},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QJJLLSTI/Feldbaum - Dual control theory.~I.pdf}
}

@article{fengDissociationExperienceDependentDevelopment2015,
  title = {Dissociation between the {{Experience-Dependent Development}} of {{Hippocampal Theta Sequences}} and {{Single-Trial Phase Precession}}},
  author = {Feng, T. and Silva, D. and Foster, D. J.},
  date = {2015-03-25},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {35},
  number = {12},
  pages = {4890--4902},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2614-14.2015},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2614-14.2015},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/534SVKZ4/Feng et al. - 2015 - Dissociation between the Experience-Dependent Deve.pdf}
}

@article{fernandez-ruizEntorhinalCA3DualInputControl2017,
  title = {Entorhinal-{{CA3 Dual-Input Control}} of {{Spike Timing}} in the {{Hippocampus}} by {{Theta-Gamma Coupling}}},
  author = {Fernández-Ruiz, Antonio and Oliva, Azahara and Nagy, Gergő A. and Maurer, Andrew P. and Berényi, Antal and Buzsáki, György},
  date = {2017-03-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {93},
  number = {5},
  pages = {1213-1226.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.02.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627317301010},
  urldate = {2022-01-18},
  abstract = {Theta-gamma phase coupling and spike timing within theta oscillations are prominent features of the hippocampus and are often related to navigation and memory. However, the mechanisms that give rise to these relationships are not well understood. Using high spatial resolution electrophysiology, we investigated the influence of CA3 and entorhinal inputs on the timing of CA1 neurons. The theta-phase preference and excitatory strength of the afferent CA3 and entorhinal inputs effectively timed the principal neuron activity, as well as regulated distinct CA1 interneuron populations in multiple tasks and behavioral states. Feedback potentiation of distal dendritic inhibition by CA1 place cells attenuated the excitatory entorhinal input at place field entry, coupled with feedback depression of proximal dendritic and perisomatic inhibition, allowing the CA3 input to gain control toward the exit. Thus, upstream inputs interact with local mechanisms to determine theta-phase timing of hippocampal neurons to support memory and spatial navigation.},
  langid = {english},
  keywords = {cross-frequency coupling,high-density recordings,inhibition,memory encoding,memory recall,oscillations,phase coupling,phase precession,place coding,temporal coding},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/N9ELCEYW/Fernández-Ruiz et al. - 2017 - Entorhinal-CA3 Dual-Input Control of Spike Timing .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/7HKCYAT9/S0896627317301010.html}
}

@article{filippiOptimismReinforcementLearning2010,
  title = {Optimism in {{Reinforcement Learning}} and {{Kullback-Leibler Divergence}}},
  author = {Filippi, Sarah and Cappé, Olivier and Garivier, Aurélien},
  date = {2010-09},
  journaltitle = {2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  eprint = {1004.5229},
  eprinttype = {arxiv},
  pages = {115--122},
  doi = {10.1109/ALLERTON.2010.5706896},
  url = {http://arxiv.org/abs/1004.5229},
  urldate = {2021-12-07},
  abstract = {We consider model-based reinforcement learning in finite Markov Decision Processes (MDPs), focussing on so-called optimistic strategies. In MDPs, optimism can be implemented by carrying out extended value iterations under a constraint of consistency with the estimated model transition probabilities. The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this strategy, has recently been shown to guarantee near-optimal regret bounds. In this paper, we strongly argue in favor of using the Kullback-Leibler (KL) divergence for this purpose. By studying the linear maximization problem under KL constraints, we provide an efficient algorithm, termed KL-UCRL, for solving KL-optimistic extended value iteration. Using recent deviation bounds on the KL divergence, we prove that KL-UCRL provides the same guarantees as UCRL2 in terms of regret. However, numerical experiments on classical benchmarks show a significantly improved behavior, particularly when the MDP has reduced connectivity. To support this observation, we provide elements of comparison between the two algorithms based on geometric considerations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/V7226N6W/Filippi et al. - 2010 - Optimism in Reinforcement Learning and Kullback-Le.pdf}
}

@inproceedings{fonteneauOptimisticPlanningBeliefaugmented2013,
  title = {Optimistic Planning for Belief-Augmented {{Markov Decision Processes}}},
  booktitle = {2013 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  author = {Fonteneau, Raphael and Busoniu, Lucian and Munos, Remi},
  date = {2013-04},
  pages = {77--84},
  publisher = {{IEEE}},
  location = {{Singapore, Singapore}},
  doi = {10.1109/ADPRL.2013.6614992},
  url = {http://ieeexplore.ieee.org/document/6614992/},
  urldate = {2021-12-07},
  abstract = {This paper presents the Bayesian Optimistic Planning (BOP) algorithm, a novel model-based Bayesian reinforcement learning approach. BOP extends the planning approach of the Optimistic Planning for Markov Decision Processes (OPMDP) algorithm [10], [9] to contexts where the transition model of the MDP is initially unknown and progressively learned through interactions within the environment. The knowledge about the unknown MDP is represented with a probability distribution over all possible transition models using Dirichlet distributions, and the BOP algorithm plans in the belief-augmented state space constructed by concatenating the original state vector with the current posterior distribution over transition models. We show that BOP becomes Bayesian optimal when the budget parameter increases to infinity. Preliminary empirical validations show promising performance.},
  eventtitle = {2013 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  isbn = {978-1-4673-5925-2},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AI4HVP93/Fonteneau et al. - 2013 - Optimistic planning for belief-augmented Markov De.pdf}
}

@inproceedings{fonteneauOptimisticPlanningBeliefaugmented2013a,
  title = {Optimistic Planning for Belief-Augmented {{Markov Decision Processes}}},
  booktitle = {2013 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  author = {Fonteneau, Raphael and Busoniu, Lucian and Munos, Remi},
  date = {2013-04},
  pages = {77--84},
  publisher = {{IEEE}},
  location = {{Singapore, Singapore}},
  doi = {10.1109/ADPRL.2013.6614992},
  url = {http://ieeexplore.ieee.org/document/6614992/},
  urldate = {2021-12-07},
  abstract = {This paper presents the Bayesian Optimistic Planning (BOP) algorithm, a novel model-based Bayesian reinforcement learning approach. BOP extends the planning approach of the Optimistic Planning for Markov Decision Processes (OPMDP) algorithm [10], [9] to contexts where the transition model of the MDP is initially unknown and progressively learned through interactions within the environment. The knowledge about the unknown MDP is represented with a probability distribution over all possible transition models using Dirichlet distributions, and the BOP algorithm plans in the belief-augmented state space constructed by concatenating the original state vector with the current posterior distribution over transition models. We show that BOP becomes Bayesian optimal when the budget parameter increases to infinity. Preliminary empirical validations show promising performance.},
  eventtitle = {2013 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  isbn = {978-1-4673-5925-2},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/MKIC4LK5/Fonteneau et al. - 2013 - Optimistic planning for belief-augmented Markov De.pdf}
}

@article{fosterReplayComesAge2017,
  title = {Replay {{Comes}} of {{Age}}},
  author = {Foster, David J.},
  date = {2017-07-25},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {40},
  number = {1},
  pages = {581--602},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-072116-031538},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-072116-031538},
  urldate = {2022-10-05},
  abstract = {Hippocampal place cells take part in sequenced patterns of reactivation after behavioral experience, known as replay. Since replay was first reported, nearly 20 years ago, many new results have been found, necessitating revision of the original interpretations. We review some of these results with a focus on the phenomenology of replay.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LIWC4VY9/Foster - 2017 - Replay Comes of Age.pdf}
}

@article{fosterReverseReplayBehavioural2006,
  title = {Reverse Replay of Behavioural Sequences in Hippocampal Place Cells during the Awake State},
  author = {Foster, David J. and Wilson, Matthew A.},
  date = {2006-03},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {440},
  number = {7084},
  pages = {680--683},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature04587},
  url = {http://www.nature.com/articles/nature04587},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QAHHZRXX/Foster and Wilson - 2006 - Reverse replay of behavioural sequences in hippoca.pdf}
}

@article{fosterStructureSpaceValue2002,
  title = {Structure in the {{Space}} of {{Value Functions}}},
  author = {Foster, David and Dayan, Peter},
  date = {2002-11-01},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  volume = {49},
  number = {2},
  pages = {325--346},
  issn = {1573-0565},
  doi = {10.1023/A:1017944732463},
  url = {https://doi.org/10.1023/A:1017944732463},
  urldate = {2022-03-01},
  abstract = {Solving in an efficient manner many different optimal control tasks within the same underlying environment requires decomposing the environment into its computationally elemental fragments. We suggest how to find fragmentations using unsupervised, mixture model, learning methods on data derived from optimal value functions for multiple tasks, and show that these fragmentations are in accord with observable structure in the environments. Further, we present evidence that such fragments can be of use in a practical reinforcement learning context, by facilitating online, actor-critic learning of multiple goals MDPs.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IE75XFIJ/Foster and Dayan - 2002 - Structure in the Space of Value Functions.pdf}
}

@article{fristonFreeEnergyPrinciple,
  title = {The Free Energy Principle Made Simpler but Not Too Simple},
  author = {Friston, Karl and Costa, Lancelot Da and Sajid, Noor and Heins, Conor and Ueltzhöffer, Kai and Pavliotis, Grigorios A and Parr, Thomas},
  pages = {42},
  abstract = {This paper provides a concise description of the free energy principle, starting from a formulation of random dynamical systems in terms of a Langevin equation and ending with a Bayesian mechanics that can be read as a physics of sentience. It rehearses the key steps using standard results from statistical physics. These steps entail (i) establishing a particular partition of states based upon conditional independencies that inherit from sparsely coupled dynamics, (ii) unpacking the implications of this partition in terms of Bayesian inference and (iii) describing the paths of particular states with a variational principle of least action. Teleologically, the free energy principle offers a normative account of self-organisation in terms of optimal Bayesian design and decision-making, in the sense of maximising marginal likelihood or Bayesian model evidence. In summary, starting from a description of the world in terms of random dynamical systems, we end up with a description of self-organisation as sentient behaviour that can be interpreted as self-evidencing; namely, self-assembly, autopoiesis or active inference.},
  langid = {english},
  keywords = {free energy},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JX7A38E5/Friston et al. - The free energy principle made simpler but not too.pdf}
}

@unpublished{fusiMemoryCapacityNeural2021,
  title = {Memory Capacity of Neural Network Models},
  author = {Fusi, Stefano},
  date = {2021-08-17},
  eprint = {2108.07839},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  url = {http://arxiv.org/abs/2108.07839},
  urldate = {2021-12-13},
  abstract = {Memory is a complex phenomenon that involves several distinct mechanisms. These mechanisms operate at different spatial and temporal levels. This chapter focuses on the theoretical framework and the mathematical models that have been developed to understand how these mechanisms are orchestrated to store, preserve and retrieve a large number of memories. In particular, this chapter reviews the theoretical studies on memory capacity, in which the investigators estimated how the number of storable memories scales with the number of neurons and synapses in the neural circuitry. The memory capacity depends on the complexity of the synapses, the sparseness of the representations, the spatial and temporal correlations between memories and the specific way memories are retrieved. Complexity is important when the synapses can only be modified with a limited precision, as in the case of biological synapses, and sparseness can greatly increase memory capacity and be particularly beneficial when memories are structured (correlated to each other). The theoretical tools discussed by this chapter can be harnessed to identify the important computational principles that underlie memory storage, preservation and retrieval and provide guidance in designing and interpreting memory experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5PC5IYYU/Fusi - 2021 - Memory capacity of neural network models.pdf}
}

@article{fyhnSpatialRepresentationEntorhinal2004,
  title = {Spatial {{Representation}} in the {{Entorhinal Cortex}}},
  author = {Fyhn, Marianne and Molden, Sturla and Witter, Menno P. and Moser, Edvard I. and Moser, May-Britt},
  date = {2004-08-27},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {305},
  number = {5688},
  pages = {1258--1264},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1099901},
  url = {https://www.science.org/doi/10.1126/science.1099901},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/63EK9RBA/Fyhn et al. - 2004 - Spatial Representation in the Entorhinal Cortex.pdf}
}

@report{gagnePerilPrudencePlanning2021,
  type = {preprint},
  title = {Peril, {{Prudence}} and {{Planning}} as {{Risk}}, {{Avoidance}} and {{Worry}}},
  author = {Gagne, Christopher and Dayan, Peter},
  date = {2021-05-11},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/tcn7e},
  url = {https://osf.io/tcn7e},
  urldate = {2021-12-07},
  abstract = {Risk occupies a central role in both the theory and practice of decisionmaking. Although it is deeply implicated in many conditions involving dysfunctional behavior and thought, modern theoretical approaches to understanding and mitigating risk, in either one-shot or sequential settings, have yet to permeate fully the fields of neural reinforcement learning and computational psychiatry. Here we use one prominent approach, called conditional value-at-risk (CVaR), to examine optimal risk-sensitive choice and one form of optimal, risk-sensitive offline planning. We relate the former to both a justified form of the gambler’s fallacy and extremely risk-avoidant behavior resembling that observed in anxiety disorders. We relate the latter to worry and rumination.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/GZMZRF6S/Gagne and Dayan - 2021 - Peril, Prudence and Planning as Risk, Avoidance an.pdf}
}

@article{gardnerToroidalTopologyPopulation2022,
  title = {Toroidal Topology of Population Activity in Grid Cells},
  author = {Gardner, Richard J. and Hermansen, Erik and Pachitariu, Marius and Burak, Yoram and Baas, Nils A. and Dunn, Benjamin A. and Moser, May-Britt and Moser, Edvard I.},
  date = {2022-01-12},
  journaltitle = {Nature},
  pages = {1--6},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04268-7},
  url = {https://www.nature.com/articles/s41586-021-04268-7},
  urldate = {2022-01-13},
  abstract = {The medial entorhinal cortex is part of a neural system for mapping the position of an individual within a physical environment1. Grid cells, a key component of this system, fire in a characteristic hexagonal pattern of locations2, and are organized in modules3 that collectively form a population code for the animal’s allocentric position1. The invariance of the correlation structure of this population code across environments4,5 and behavioural states6,7, independent of specific sensory inputs, has pointed to intrinsic, recurrently connected continuous attractor networks (CANs) as a possible substrate of the grid pattern1,8–11. However, whether grid cell networks show continuous attractor dynamics, and how they interface with inputs from the environment, has remained unclear owing to the small samples of cells obtained so far. Here, using simultaneous recordings from many hundreds of grid cells and subsequent topological data analysis, we show that the joint activity of grid cells from an individual module resides on a toroidal manifold, as expected in a two-dimensional CAN. Positions on the torus correspond to positions of the moving animal in the environment. Individual cells are preferentially active at singular positions on the torus. Their positions are maintained between environments and from wakefulness to sleep, as predicted by CAN models for grid cells but not by alternative feedforward models12. This demonstration of network dynamics on a toroidal manifold provides a population-level visualization of CAN dynamics in grid cells.},
  langid = {english},
  keywords = {Network models,Neural circuits,To read},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Network models;Neural circuits Subject\_term\_id: network-models;neural-circuit},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BSH3XJNJ/Gardner et al. - 2022 - Toroidal topology of population activity in grid c.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ZFDW6WYJ/s41586-021-04268-7.html}
}

@article{gauthierDedicatedPopulationReward2018,
  title = {A {{Dedicated Population}} for {{Reward Coding}} in the {{Hippocampus}}},
  author = {Gauthier, Jeffrey L. and Tank, David W.},
  date = {2018-07},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {99},
  number = {1},
  pages = {179-193.e7},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.06.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318304768},
  urldate = {2021-12-07},
  abstract = {The hippocampus plays a critical role in goaldirected navigation. Across different environments, however, hippocampal maps are randomized, making it unclear how goal locations could be encoded consistently. To address this question, we developed a virtual reality task with shifting reward contingencies to distinguish place versus reward encoding. In mice performing the task, large-scale recordings in CA1 and subiculum revealed a small, specialized cell population that was only active near reward yet whose activity could not be explained by sensory cues or stereotyped reward anticipation behavior. Across different virtual environments, most cells remapped randomly, but reward encoding consistently arose from a single pool of cells, suggesting that they formed a dedicated channel for reward. These observations represent a significant departure from the current understanding of CA1 as a relatively homogeneous ensemble without fixed coding properties and provide a new candidate for the cellular basis of goal memory in the hippocampus.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZLJXCD3J/Gauthier and Tank - 2018 - A Dedicated Population for Reward Coding in the Hi.pdf}
}

@article{geertsGeneralModelHippocampal2020,
  title = {A General Model of Hippocampal and Dorsal Striatal Learning and Decision Making},
  author = {Geerts, Jesse P. and Chersi, Fabian and Stachenfeld, Kimberly L. and Burgess, Neil},
  date = {2020-12-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc Natl Acad Sci USA},
  volume = {117},
  number = {49},
  pages = {31427--31437},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2007981117},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.2007981117},
  urldate = {2021-12-07},
  abstract = {Humans and other animals use multiple strategies for making decisions. Reinforcement-learning theory distinguishes between stimulus–response (model-free; MF) learning and deliberative (model-based; MB) planning. The spatial-navigation literature presents a parallel dichotomy between navigation strategies. In “response learning,” associated with the dorsolateral striatum (DLS), decisions are anchored to an egocentric reference frame. In “place learning,” associated with the hippocampus, decisions are anchored to an allocentric reference frame. Emerging evidence suggests that the contribution of hippocampus to place learning may also underlie its contribution to MB learning by representing relational structure in a cognitive map. Here, we introduce a computational model in which hippocampus subserves place and MB learning by learning a “successor representation” of relational structure between states; DLS implements model-free response learning by learning associations between actions and egocentric representations of landmarks; and action values from either system are weighted by the reliability of its predictions. We show that this model reproduces a range of seemingly disparate behavioral findings in spatial and nonspatial decision tasks and explains the effects of lesions to DLS and hippocampus on these tasks. Furthermore, modeling place cells as driven by boundaries explains the observation that, unlike navigation guided by landmarks, navigation guided by boundaries is robust to “blocking” by prior state–reward associations due to learned associations between place cells. Our model, originally shaped by detailed constraints in the spatial literature, successfully characterizes the hippocampal–striatal system as a general system for decision making via adaptive combination of stimulus–response learning and the use of a cognitive map.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CBA35ZLH/Geerts et al. - 2020 - A general model of hippocampal and dorsal striatal.pdf}
}

@article{geffnerComputationalModelsPlanning2013,
  title = {Computational Models of Planning},
  author = {Geffner, Hector},
  date = {2013},
  journaltitle = {WIREs Cognitive Science},
  volume = {4},
  number = {4},
  pages = {341--356},
  issn = {1939-5086},
  doi = {10.1002/wcs.1233},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1233},
  urldate = {2022-09-28},
  abstract = {The selection of the action to do next is one of the central problems faced by autonomous agents. Natural and artificial systems address this problem in various ways: action responses can be hardwired, they can be learned, or they can be computed from a model of the situation, the actions, and the goals. Planning is the model-based approach to action selection and a fundamental ingredient of intelligent behavior in both humans and machines. Planning, however, is computationally hard as the consideration of all possible courses of action is not computationally feasible. The problem has been addressed by research in Artificial Intelligence that in recent years has uncovered simple but powerful computational principles that make planning feasible. The principles take the form of domain-independent methods for computing heuristics or appraisals that enable the effective generation of goal-directed behavior even over huge spaces. In this paper, we look at several planning models, at methods that have been shown to scale up to large problems, and at what these methods may suggest about the human mind. WIREs Cogn Sci 2013, 4:341–356. doi: 10.1002/wcs.1233 This article is categorized under: Computer Science {$>$} Artificial Intelligence},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1233},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5A94KHFH/Geffner - 2013 - Computational models of planning.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/PFC4JUFD/wcs.html}
}

@article{georgeClonestructuredGraphRepresentations2021,
  title = {Clone-Structured Graph Representations Enable Flexible Learning and Vicarious Evaluation of Cognitive Maps},
  author = {George, Dileep and Rikhye, Rajeev V. and Gothoskar, Nishad and Guntupalli, J. Swaroop and Dedieu, Antoine and Lázaro-Gredilla, Miguel},
  date = {2021-04-22},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {2392},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22559-5},
  url = {https://www.nature.com/articles/s41467-021-22559-5},
  urldate = {2022-10-20},
  abstract = {Cognitive maps are mental representations of spatial and conceptual relationships in an environment, and are~critical for flexible behavior. To form these abstract maps, the~hippocampus has to learn to separate or merge aliased observations appropriately in different contexts in a manner that enables generalization and efficient planning. Here we propose a specific higher-order graph structure, clone-structured cognitive graph (CSCG), which forms clones of an observation for different contexts as a representation that addresses these problems. CSCGs can be learned efficiently using a probabilistic sequence model that is inherently robust to uncertainty. We show that CSCGs can explain a variety of cognitive map phenomena such as discovering spatial relations from aliased sensations, transitive inference between disjoint episodes, and formation of transferable schemas. Learning different clones for different contexts explains the emergence of splitter cells observed in maze navigation and event-specific responses in lap-running experiments. Moreover, learning and inference dynamics of CSCGs offer a coherent explanation for disparate place cell remapping phenomena. By lifting aliased observations into a hidden space, CSCGs reveal latent modularity useful for hierarchical abstraction and planning. Altogether, CSCG provides a simple unifying framework for understanding hippocampal function, and could be a pathway for forming relational abstractions in artificial intelligence.},
  issue = {1},
  langid = {english},
  keywords = {Dynamical systems,Hippocampus,Learning algorithms,Network models},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8AD7NG8Q/George et al. - 2021 - Clone-structured graph representations enable flex.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/3VBXLHPQ/s41467-021-22559-5.html}
}

@article{gershmanBelievingDopamine2019,
  title = {Believing in Dopamine},
  author = {Gershman, Samuel J. and Uchida, Naoshige},
  date = {2019-11},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {20},
  number = {11},
  pages = {703--714},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-019-0220-7},
  url = {https://www.nature.com/articles/s41583-019-0220-7},
  urldate = {2022-02-18},
  abstract = {Midbrain dopamine signals are widely thought to report reward prediction errors that drive learning in the basal ganglia. However, dopamine has also been implicated in various probabilistic computations, such as encoding uncertainty and controlling exploration. Here, we show how these different facets of dopamine signalling can be brought together under a common reinforcement learning framework. The key idea is that multiple sources of uncertainty impinge on reinforcement learning computations: uncertainty about the state of the environment, the parameters of the value function and the optimal action policy. Each of these sources plays a distinct role in the prefrontal cortex–basal ganglia circuit for reinforcement learning and is ultimately reflected in dopamine activity. The view that dopamine plays a central role in the encoding and updating of beliefs brings the classical prediction error theory into alignment with more recent theories of Bayesian reinforcement learning.},
  issue = {11},
  langid = {english},
  keywords = {Classical conditioning,Learning algorithms,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/2Q3K9SMD/Gershman and Uchida - 2019 - Believing in dopamine.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/RMAMGH3R/s41583-019-0220-7.html}
}

@article{gershmanRetrospectiveRevaluationSequential2014,
  title = {Retrospective Revaluation in Sequential Decision Making: {{A}} Tale of Two Systems.},
  shorttitle = {Retrospective Revaluation in Sequential Decision Making},
  author = {Gershman, Samuel J. and Markman, Arthur B. and Otto, A. Ross},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  shortjournal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {1},
  pages = {182--194},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0030844},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0030844},
  urldate = {2022-09-16},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/YCYIDRHP/Gershman et al. - 2014 - Retrospective revaluation in sequential decision m.pdf}
}

@article{gershmanSuccessorRepresentationIts2018,
  title = {The {{Successor Representation}}: {{Its Computational Logic}} and {{Neural Substrates}}},
  shorttitle = {The {{Successor Representation}}},
  author = {Gershman, Samuel J.},
  date = {2018-08-15},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {38},
  number = {33},
  pages = {7193--7200},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0151-18.2018},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.0151-18.2018},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BBF9GB6Q/Gershman - 2018 - The Successor Representation Its Computational Lo.pdf}
}

@article{gershmanSuccessorRepresentationIts2018a,
  title = {The {{Successor Representation}}: {{Its Computational Logic}} and {{Neural Substrates}}},
  shorttitle = {The {{Successor Representation}}},
  author = {Gershman, Samuel J.},
  date = {2018-08-15},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {38},
  number = {33},
  eprint = {30006364},
  eprinttype = {pmid},
  pages = {7193--7200},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0151-18.2018},
  url = {https://www.jneurosci.org/content/38/33/7193},
  urldate = {2022-01-25},
  abstract = {Reinforcement learning is the process by which an agent learns to predict long-term future reward. We now understand a great deal about the brain's reinforcement learning algorithms, but we know considerably less about the representations of states and actions over which these algorithms operate. A useful starting point is asking what kinds of representations we would want the brain to have, given the constraints on its computational architecture. Following this logic leads to the idea of the successor representation, which encodes states of the environment in terms of their predictive relationships with other states. Recent behavioral and neural studies have provided evidence for the successor representation, and computational studies have explored ways to extend the original idea. This paper reviews progress on these fronts, organizing them within a broader framework for understanding how the brain negotiates tradeoffs between efficiency and flexibility for reinforcement learning.},
  langid = {english},
  keywords = {cognitive map,dopamine,hippocampus,reinforcement learning,reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/SKSAPMMW/Gershman - 2018 - The Successor Representation Its Computational Lo.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/UG6ZACAP/7193.html}
}

@article{gershmanSuccessorRepresentationTemporal2012,
  title = {The {{Successor Representation}} and {{Temporal Context}}},
  author = {Gershman, Samuel J. and Moore, Christopher D. and Todd, Michael T. and Norman, Kenneth A. and Sederberg, Per B.},
  date = {2012-06},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {24},
  number = {6},
  pages = {1553--1568},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00282},
  url = {https://direct.mit.edu/neco/article/24/6/1553-1568/7771},
  urldate = {2022-03-02},
  abstract = {The successor representation was introduced into reinforcement learning by Dayan ( 1993 ) as a means of facilitating generalization between states with similar successors. Although reinforcement learning in general has been used extensively as a model of psychological and neural processes, the psychological validity of the successor representation has yet to be explored. An interesting possibility is that the successor representation can be used not only for reinforcement learning but for episodic learning as well. Our main contribution is to show that a variant of the temporal context model (TCM; Howard \& Kahana, 2002 ), an influential model of episodic memory, can be understood as directly estimating the successor representation using the temporal difference learning algorithm (Sutton \& Barto, 1998 ). This insight leads to a generalization of TCM and new experimental predictions. In addition to casting a new normative light on TCM, this equivalence suggests a previously unexplored point of contact between different learning systems.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HJML9SYD/Gershman et al. - 2012 - The Successor Representation and Temporal Context.pdf}
}

@article{ghavamzadehBayesianReinforcementLearning2015,
  title = {Bayesian {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Bayesian {{Reinforcement Learning}}},
  author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  date = {2015},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {8},
  number = {5-6},
  eprint = {1609.04436},
  eprinttype = {arxiv},
  pages = {359--483},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000049},
  url = {http://arxiv.org/abs/1609.04436},
  urldate = {2021-12-07},
  abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3GNJ3YTL/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf}
}

@article{gillespieHippocampalReplayReflects2021,
  title = {Hippocampal Replay Reflects Specific Past Experiences Rather than a Plan for Subsequent Choice},
  author = {Gillespie, Anna K. and Maya, Daniela A. Astudillo and Denovellis, Eric L. and Liu, Daniel F. and Kastner, David B. and Coulter, Michael E. and Roumis, Demetris K. and Eden, Uri T. and Frank, Loren M.},
  date = {2021-10-06},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {109},
  number = {19},
  eprint = {34450026},
  eprinttype = {pmid},
  pages = {3149-3163.e6},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.07.029},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(21)00573-0},
  urldate = {2022-02-01},
  langid = {english},
  keywords = {consolidation,decoding,hippocampus,learning,memory,navigation,planning,replay,sharp wave ripples},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QQA8X4TY/Gillespie et al. - 2021 - Hippocampal replay reflects specific past experien.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/S338T4IH/S0896-6273(21)00573-0.html}
}

@article{girardeauSelectiveSuppressionHippocampal2009,
  title = {Selective Suppression of Hippocampal Ripples Impairs Spatial Memory},
  author = {Girardeau, Gabrielle and Benchenane, Karim and Wiener, Sidney I and Buzsáki, György and Zugaro, Michaël B},
  date = {2009-10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {12},
  number = {10},
  pages = {1222--1223},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2384},
  url = {http://www.nature.com/articles/nn.2384},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/YDIU5W9S/Girardeau et al. - 2009 - Selective suppression of hippocampal ripples impai.pdf}
}

@article{girardeauSelectiveSuppressionHippocampal2009a,
  title = {Selective Suppression of Hippocampal Ripples Impairs Spatial Memory},
  author = {Girardeau, Gabrielle and Benchenane, Karim and Wiener, Sidney I. and Buzsáki, György and Zugaro, Michaël B.},
  date = {2009-10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {12},
  number = {10},
  pages = {1222--1223},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2384},
  url = {https://www.nature.com/articles/nn.2384},
  urldate = {2022-05-06},
  abstract = {Sharp wave-ripple (SPW-R) complexes during sleep or rest have yet to be causally linked to memory consolidation. Here, the authors show that suppressing hippocampal SPW-Rs during post-training sleep in rats impairs the consolidation of a hippocampus-dependent spatial memory task.},
  issue = {10},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8UJ7EMHY/Girardeau et al. - 2009 - Selective suppression of hippocampal ripples impai.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/WAEGZMUQ/nn.html}
}

@article{gittinsBanditProcessesDynamic1979,
  title = {Bandit {{Processes}} and {{Dynamic Allocation Indices}}},
  author = {Gittins, J. C.},
  date = {1979-01},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  shortjournal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {41},
  number = {2},
  pages = {148--164},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1979.tb01068.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1979.tb01068.x},
  urldate = {2021-12-07},
  abstract = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XP5VEX69/Gittins - 1979 - Bandit Processes and Dynamic Allocation Indices.pdf}
}

@article{glascherStatesRewardsDissociable2010,
  title = {States versus {{Rewards}}: {{Dissociable Neural Prediction Error Signals Underlying Model-Based}} and {{Model-Free Reinforcement Learning}}},
  shorttitle = {States versus {{Rewards}}},
  author = {Gläscher, Jan and Daw, Nathaniel and Dayan, Peter and O'Doherty, John P.},
  date = {2010-05},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {66},
  number = {4},
  pages = {585--595},
  issn = {08966273},
  doi = {10.1016/j.neuron.2010.04.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627310002874},
  urldate = {2022-10-31},
  abstract = {Reinforcement learning (RL) uses sequential experience with situations (‘‘states’’) and outcomes to assess actions. Whereas model-free RL uses this experience directly, in the form of a reward prediction error (RPE), model-based RL uses it indirectly, building a model of the state transition and outcome structure of the environment, and evaluating actions by searching this model. A state prediction error (SPE) plays a central role, reporting discrepancies between the current model and the observed state transitions. Using functional magnetic resonance imaging in humans solving a probabilistic Markov decision task, we found the neural signature of an SPE in the intraparietal sulcus and lateral prefrontal cortex, in addition to the previously well-characterized RPE in the ventral striatum. This finding supports the existence of two unique forms of learning signal in humans, which may form the basis of distinct computational strategies for guiding behavior.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HIBPR4MQ/Gläscher et al. - 2010 - States versus Rewards Dissociable Neural Predicti.pdf}
}

@report{gnadtSOVEREIGNAutonomousNeural2007,
  type = {Technical Report},
  title = {{{SOVEREIGN}}: {{An Autonomous Neural System}} for {{Incrementally Learning Planned Action Sequences}} to {{Navigate Towards}} a {{Rewarded Goal}}},
  shorttitle = {{{SOVEREIGN}}},
  author = {Gnadt, William and Grossberg, Stephen},
  date = {2007-09},
  institution = {{Boston University Center for Adaptive Systems and Department of Cognitive and Neural Systems}},
  url = {https://open.bu.edu/handle/2144/1953},
  urldate = {2022-01-28},
  abstract = {How do reactive and planned behaviors interact in real time? How are sequences of such behaviors released at appropriate times during autonomous navigation to realize valued goals? Controllers for both animals and mobile robots, or animats, need reactive mechanisms for exploration, and learned plans to reach goal objects once an environment becomes familiar. The SOVEREIGN (Self-Organizing, Vision, Expectation, Recognition,  Emotion, Intelligent, Goaloriented Navigation) animat model embodies these capabilities, and is tested in a 3D virtual reality environment. SOVEREIGN includes several interacting subsystems which model complementary properties of cortical What and Where processing streams and which clarify similarities between mechanisms for navigation and arm movement control. As the animat explores an environment, visual inputs are processed by networks that are sensitive to visual form and motion in the What and Where streams, respectively. Position-invariant and sizeinvariant recognition categories are learned by real-time incremental learning in the What stream. Estimates of target position relative to the animat are computed in the Where stream, and can activate approach movements toward the target. Motion cues from animat locomotion can elicit head-orienting movements to bring a new target into view. Approach and orienting movements are alternately performed during animat navigation. Cumulative estimates of each movement are derived from interacting proprioceptive and visual cues. Movement sequences are stored within a motor working memory. Sequences of visual categories are stored in a sensory working memory. These working memories trigger learning of sensory and motor sequence categories, or plans, which together control planned movements. Predictively effective chunk combinations are selectively enhanced via reinforcement learning when the animat is rewarded. Selected planning chunks effect a gradual transition from variable reactive exploratory movements to efficient goal-oriented planned movement sequences. Volitional signals gate interactions between model subsystems and the release of overt behaviors. The model can control different motor sequences under different motivational states and learns more efficient sequences to rewarded goals as exploration proceeds.},
  langid = {american},
  keywords = {To read},
  annotation = {Accepted: 2011-11-14T18:17:06Z},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/G9TVJ76N/Gnadt and Grossberg - 2007 - SOVEREIGN An Autonomous Neural System for Increme.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/2BZ3FYXA/1953.html}
}

@article{gompertsVTANeuronsCoordinate2015,
  title = {{{VTA}} Neurons Coordinate with the Hippocampal Reactivation of Spatial Experience},
  author = {Gomperts, Stephen N and Kloosterman, Fabian and Wilson, Matthew A},
  editor = {Eichenbaum, Howard},
  date = {2015-10-14},
  journaltitle = {eLife},
  volume = {4},
  pages = {e05360},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.05360},
  url = {https://doi.org/10.7554/eLife.05360},
  urldate = {2022-02-18},
  abstract = {Spatial learning requires the hippocampus, and the replay of spatial sequences during hippocampal sharp wave-ripple (SPW-R) events of quiet wakefulness and sleep is believed to play a crucial role. To test whether the coordination of VTA reward prediction error signals with these replayed spatial sequences could contribute to this process, we recorded from neuronal ensembles of the hippocampus and VTA as rats performed appetitive spatial tasks and subsequently slept. We found that many reward responsive (RR) VTA neurons coordinated with quiet wakefulness-associated hippocampal SPW-R events that replayed recent experience. In contrast, coordination between RR neurons and SPW-R events in subsequent slow wave sleep was diminished. Together, these results indicate distinct contributions of VTA reinforcement activity associated with hippocampal spatial replay to the processing of wake and SWS-associated spatial memory.},
  keywords = {dopamine,electrophysiology,hippocampus,replay,sleep,To read,VTA},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BPBZZI88/Gomperts et al. - 2015 - VTA neurons coordinate with the hippocampal reacti.pdf}
}

@unpublished{goyalRecurrentIndependentMechanisms2020,
  title = {Recurrent {{Independent Mechanisms}}},
  author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Schölkopf, Bernhard},
  date = {2020-11-17},
  eprint = {1909.10893},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.10893},
  urldate = {2021-12-07},
  abstract = {We explore the hypothesis that learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes that only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and compete with each other so they are updated only at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for remarkably improved generalization on tasks where some factors of variation differ systematically between training and evaluation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/GFG3MFXT/Goyal et al. - 2020 - Recurrent Independent Mechanisms.pdf}
}

@article{grellaReactivatingHippocampalmediatedMemories2022,
  title = {Reactivating Hippocampal-Mediated Memories during Reconsolidation to Disrupt Fear},
  author = {Grella, Stephanie L. and Fortin, Amanda H. and Ruesch, Evan and Bladon, John H. and Reynolds, Leanna F. and Gross, Abby and Shpokayte, Monika and Cincotta, Christine and Zaki, Yosif and Ramirez, Steve},
  date = {2022-09-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {4733},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-32246-8},
  url = {https://www.nature.com/articles/s41467-022-32246-8},
  urldate = {2022-09-12},
  abstract = {Memories are stored in the brain as cellular ensembles activated during learning and reactivated during retrieval. Using the Tet-tag system in mice, we label dorsal dentate gyrus neurons activated by positive, neutral or negative experiences with channelrhodopsin-2. Following fear-conditioning, these cells are artificially reactivated during fear memory recall. Optical stimulation of a competing positive memory is sufficient to update the memory during reconsolidation, thereby reducing conditioned fear acutely and enduringly. Moreover, mice demonstrate operant responding for reactivation of a positive memory, confirming its rewarding properties. These results show that interference from a rewarding experience can counteract negative affective states. While memory-updating, induced by memory reactivation, involves a relatively small set of neurons, we also find that activating a large population of randomly labeled dorsal dentate gyrus neurons is effective in promoting reconsolidation. Importantly, memory-updating is specific to the fear memory. These findings implicate the dorsal dentate gyrus as a potential therapeutic node for modulating memories to suppress fear.},
  issue = {1},
  langid = {english},
  keywords = {Extinction,Fear conditioning,Hippocampus},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LCAJKYGP/Grella et al. - 2022 - Reactivating hippocampal-mediated memories during .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/FZ4SLJJX/s41467-022-32246-8.html}
}

@article{gridchynAssemblySpecificDisruptionHippocampal2020,
  title = {Assembly-{{Specific Disruption}} of {{Hippocampal Replay Leads}} to {{Selective Memory Deficit}}},
  author = {Gridchyn, Igor and Schoenenberger, Philipp and O’Neill, Joseph and Csicsvari, Jozsef},
  date = {2020-04},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {106},
  number = {2},
  pages = {291-300.e6},
  issn = {08966273},
  doi = {10.1016/j.neuron.2020.01.021},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627320300477},
  urldate = {2021-12-07},
  abstract = {Memory consolidation is thought to depend on the reactivation of waking hippocampal firing patterns during sleep. Following goal learning, the reactivation of place cell firing can represent goals and predicts subsequent memory recall. However, it is unclear whether reactivation promotes the recall of the reactivated memories only or triggers wider reorganization. We trained animals to locate goals at fixed locations in two different environments. Following learning, by performing online assembly content decoding, the reactivation of only one environment was disrupted, leading to recall deficit in that environment. The place map of the disrupted environment was destabilized but re-emerged once the goal was relearned. These data demonstrate that sleep reactivation facilitates goal-memory retrieval by strengthening memories that enable the selection of context-specific hippocampal maps. However, sleep reactivation may not be needed for the stabilization of place maps considering that the map of the disrupted environment re-emerged after the retraining of goals.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EXCEVVBJ/Gridchyn et al. - 2020 - Assembly-Specific Disruption of Hippocampal Replay.pdf}
}

@article{grimmValueEquivalencePrinciple,
  title = {The {{Value Equivalence Principle}} for {{Model-Based Reinforcement Learning}}},
  author = {Grimm, Christopher and Barreto, André and Singh, Satinder and Silver, David},
  pages = {12},
  abstract = {Learning models of the environment from data is often viewed as an essential component to building intelligent reinforcement learning (RL) agents. The common practice is to separate the learning of the model from its use, by constructing a model of the environment’s dynamics that correctly predicts the observed state transitions. In this paper we argue that the limited representational resources of model-based RL agents are better used to build models that are directly useful for value-based planning. As our main contribution, we introduce the principle of value equivalence: two models are value equivalent with respect to a set of functions and policies if they yield the same Bellman updates. We propose a formulation of the model learning problem based on the value equivalence principle and analyze how the set of feasible solutions is impacted by the choice of policies and functions. Specifically, we show that, as we augment the set of policies and functions considered, the class of value equivalent models shrinks, until eventually collapsing to a single point corresponding to a model that perfectly describes the environment. In many problems, directly modelling state-to-state transitions may be both difficult and unnecessary. By leveraging the value-equivalence principle one may find simpler models without compromising performance, saving computation and memory. We illustrate the benefits of value-equivalent model learning with experiments comparing it against more traditional counterparts like maximum likelihood estimation. More generally, we argue that the principle of value equivalence underlies a number of recent empirical successes in RL, such as Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and MuZero, and provides a first theoretical underpinning of those results.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7B2USKDZ/Grimm et al. - The Value Equivalence Principle for Model-Based Re.pdf}
}

@article{grosmarkReactivationPredictsConsolidation2021,
  title = {Reactivation Predicts the Consolidation of Unbiased Long-Term Cognitive Maps},
  author = {Grosmark, Andres D. and Sparks, Fraser T. and Davis, Matt J. and Losonczy, Attila},
  date = {2021-11},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {24},
  number = {11},
  pages = {1574--1585},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-021-00920-7},
  url = {https://www.nature.com/articles/s41593-021-00920-7},
  urldate = {2021-12-21},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JPD8Z8AZ/Grosmark et al. - 2021 - Reactivation predicts the consolidation of unbiase.pdf}
}

@article{grossbergNeuralModelIntrinsic2021,
  title = {A {{Neural Model}} of {{Intrinsic}} and {{Extrinsic Hippocampal Theta Rhythms}}: {{Anatomy}}, {{Neurophysiology}}, and {{Function}}},
  shorttitle = {A {{Neural Model}} of {{Intrinsic}} and {{Extrinsic Hippocampal Theta Rhythms}}},
  author = {Grossberg, Stephen},
  date = {2021},
  journaltitle = {Frontiers in Systems Neuroscience},
  volume = {15},
  issn = {1662-5137},
  url = {https://www.frontiersin.org/article/10.3389/fnsys.2021.665052},
  urldate = {2022-01-28},
  abstract = {This article describes a neural model of the anatomy, neurophysiology, and functions of intrinsic and extrinsic theta rhythms in the brains of multiple species. Topics include how theta rhythms were discovered; how theta rhythms organize brain information processing into temporal series of spatial patterns; how distinct theta rhythms occur within area CA1 of the hippocampus and between the septum and area CA3 of the hippocampus; what functions theta rhythms carry out in different brain regions, notably CA1-supported functions like learning, recognition, and memory that involve visual, cognitive, and emotional processes; how spatial navigation, adaptively timed learning, and category learning interact with hippocampal theta rhythms; how parallel cortical streams through the lateral entorhinal cortex (LEC) and the medial entorhinal cortex (MEC) represent the end-points of the What cortical stream for perception and cognition and the Where cortical stream for spatial representation and action; how the neuromodulator acetylcholine interacts with the septo-hippocampal theta rhythm and modulates category learning; what functions are carried out by other brain rhythms, such as gamma and beta oscillations; and how gamma and beta oscillations interact with theta rhythms. Multiple experimental facts about theta rhythms are unified and functionally explained by this theoretical synthesis.},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CEJMKG43/Grossberg - 2021 - A Neural Model of Intrinsic and Extrinsic Hippocam.pdf}
}

@inproceedings{guezEfficientBayesAdaptiveReinforcement2012,
  title = {Efficient {{Bayes-Adaptive Reinforcement Learning}} Using {{Sample-Based Search}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Guez, Arthur and Silver, David and Dayan, Peter},
  date = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2012/hash/35051070e572e47d2c26c241ab88307f-Abstract.html},
  urldate = {2022-02-09},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3UDRNKB7/Guez et al. - 2012 - Efficient Bayes-Adaptive Reinforcement Learning us.pdf}
}

@article{guezInvestigationModelFreePlanning,
  title = {An {{Investigation}} of {{Model-Free Planning}}},
  author = {Guez, Arthur and Mirza, Mehdi and Gregor, Karol and Kabra, Rishabh and Racanière, Sébastien and Weber, Théophane and Raposo, David and Santoro, Adam and Orseau, Laurent and Eccles, Tom and Wayne, Greg and Silver, David and Lillicrap, Timothy},
  pages = {10},
  abstract = {The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a modelfree RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent’s effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-ofthe-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NPUMFP7D/Guez et al. - An Investigation of Model-Free Planning.pdf}
}

@thesis{guezSampleBasedSearchMethods2015,
  title = {Sample-{{Based Search Methods}} for {{Bayes-Adaptive Planning}}},
  author = {Guez, Arthur},
  date = {2015},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/K3Z23WNB/Guez - Sample-Based Search Methods for Bayes-Adaptive Pla.pdf}
}

@article{guptaHippocampalReplayNot2010,
  title = {Hippocampal {{Replay Is Not}} a {{Simple Function}} of {{Experience}}},
  author = {Gupta, Anoopum S. and van der Meer, Matthijs A.A. and Touretzky, David S. and Redish, A. David},
  options = {useprefix=true},
  date = {2010-03},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {65},
  number = {5},
  pages = {695--705},
  issn = {08966273},
  doi = {10.1016/j.neuron.2010.01.034},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627310000607},
  urldate = {2021-12-07},
  abstract = {Replay of behavioral sequences in the hippocampus during sharp wave ripple complexes (SWRs) provides a potential mechanism for memory consolidation and the learning of knowledge structures. Current hypotheses imply that replay should straightforwardly reflect recent experience. However, we find these hypotheses to be incompatible with the content of replay on a task with two distinct behavioral sequences (A and B). We observed forward and backward replay of B even when rats had been performing A for {$>$}10 min. Furthermore, replay of nonlocal sequence B occurred more often when B was infrequently experienced. Neither forward nor backward sequences preferentially represented highly experienced trajectories within a session. Additionally, we observed the construction of never-experienced novel-path sequences. These observations challenge the idea that sequence activation during SWRs is a simple replay of recent experience. Instead, replay reflected all physically available trajectories within the environment, suggesting a potential role in active learning and maintenance of the cognitive map.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ISXGV3H4/Gupta et al. - 2010 - Hippocampal Replay Is Not a Simple Function of Exp.pdf}
}

@inproceedings{guptaMetaReinforcementLearningStructured2018,
  title = {Meta-{{Reinforcement Learning}} of {{Structured Exploration Strategies}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/4de754248c196c85ee4fbdcee89179bd-Abstract.html},
  urldate = {2022-02-14},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ECC2KEDI/Gupta et al. - 2018 - Meta-Reinforcement Learning of Structured Explorat.pdf}
}

@article{hagaRecurrentNetworkModel2018,
  title = {Recurrent Network Model for Learning Goal-Directed Sequences through Reverse Replay},
  author = {Haga, Tatsuya and Fukai, Tomoki},
  editor = {Bhalla, Upinder Singh and Frank, Michael J},
  date = {2018-07-03},
  journaltitle = {eLife},
  volume = {7},
  pages = {e34171},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.34171},
  url = {https://doi.org/10.7554/eLife.34171},
  urldate = {2022-04-25},
  abstract = {Reverse replay of hippocampal place cells occurs frequently at rewarded locations, suggesting its contribution to goal-directed path learning. Symmetric spike-timing dependent plasticity (STDP) in CA3 likely potentiates recurrent synapses for both forward (start to goal) and reverse (goal to start) replays during sequential activation of place cells. However, how reverse replay selectively strengthens forward synaptic pathway is unclear. Here, we show computationally that firing sequences bias synaptic transmissions to the opposite direction of propagation under symmetric STDP in the co-presence of short-term synaptic depression or afterdepolarization. We demonstrate that significant biases are created in biologically realistic simulation settings, and this bias enables reverse replay to enhance goal-directed spatial memory on a W-maze. Further, we show that essentially the same mechanism works in a two-dimensional open field. Our model for the first time provides the mechanistic account for the way reverse replay contributes to hippocampal sequence learning for reward-seeking spatial navigation.},
  keywords = {goal-directed learning,hippocampus,reverse replay,sequence learning,short-term plasticity,spike-timing-dependent plasticity,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/I87SFWYC/Haga and Fukai - 2018 - Recurrent network model for learning goal-directed.pdf}
}

@unpublished{hamrickRolePlanningModelbased2021,
  title = {On the Role of Planning in Model-Based Deep Reinforcement Learning},
  author = {Hamrick, Jessica B. and Friesen, Abram L. and Behbahani, Feryal and Guez, Arthur and Viola, Fabio and Witherspoon, Sims and Anthony, Thomas and Buesing, Lars and Veličković, Petar and Weber, Théophane},
  date = {2021-03-17},
  eprint = {2011.04021},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.04021},
  urldate = {2021-12-12},
  abstract = {Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero [58], a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CVC9QDLM/Hamrick et al. - 2021 - On the role of planning in model-based deep reinfo.pdf}
}

@article{hardtPatternsPredictionsActions,
  title = {Patterns, {{Predictions}}, and {{Actions}}},
  author = {Hardt, Moritz and Recht, Benjamin},
  pages = {309},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HPPBNR2Q/Hardt and Recht - Patterns, Predictions, and Actions.pdf}
}

@article{hayesReplayDeepLearning2021,
  title = {Replay in {{Deep Learning}}: {{Current Approaches}} and {{Missing Biological Elements}}},
  shorttitle = {Replay in {{Deep Learning}}},
  author = {Hayes, Tyler L. and Krishnan, Giri P. and Bazhenov, Maxim and Siegelmann, Hava T. and Sejnowski, Terrence J. and Kanan, Christopher},
  date = {2021-08-30},
  journaltitle = {Neural Computation},
  pages = {1--44},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01433},
  url = {https://direct.mit.edu/neco/article/doi/10.1162/neco_a_01433/107071/Replay-in-Deep-Learning-Current-Approaches-and},
  urldate = {2022-04-25},
  abstract = {Abstract             Replay is the reactivation of one or more neural patterns that are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated in deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this letter, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be used to improve artificial neural networks.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/RZWNAIVV/Hayes et al. - 2021 - Replay in Deep Learning Current Approaches and Mi.pdf}
}

@article{hayMetareasoningMonteCarlo2011,
  title = {Metareasoning for {{Monte Carlo Tree Search}}},
  author = {Hay, Nicholas and Russell, Stuart},
  date = {2011},
  pages = {30},
  abstract = {Sequential decision problems are often approximately solvable by simulating possible future action sequences; such methods are a staple of gameplaying algorithms, robot path planners, model-predictive control systems, and logistical planners in operations research. Since the 1960s, researchers have sought effective metareasoning methods for selecting which action sequences to simulate, basing their approach on some estimate of the expected improvement in decision quality resulting from any particular simulation. Recently, this approach has been applied successfully in the context of Monte Carlo tree search, where each simulation takes the form of a randomized sequence of actions leading to a terminal state. In particular, the UCT algorithm borrows asymptotically optimal selection rules from the theory of bandit problems and has led to a new generation of master-level Go programs such as MoGo. We argue that, despite this success, the bandit framework is inappropriate as a basis for selecting computations. We propose instead a theoretical framework for metareasoning that is isomorphic to the statistical framework of ranking and selection. In this framework, we describe two apparently distinct conceptual approaches to the forward search metareasoning problem and prove them to be equivalent. We derive a number of basic results applicable to simple Monte Carlo selection problems, including asymptotic regret bounds, and discuss prospects for their extension to combinatorial settings.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/YHR3EHH9/Hay and Russell - Metareasoning for Monte Carlo Tree Search.pdf}
}

@article{hayPrinciplesMetalevelControl,
  title = {Principles of {{Metalevel Control}}},
  author = {Hay, Nicholas James},
  pages = {118},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/WFV4QDVG/Hay - Principles of Metalevel Control.pdf}
}

@unpublished{haySelectingComputationsTheory2014,
  title = {Selecting {{Computations}}: {{Theory}} and {{Applications}}},
  shorttitle = {Selecting {{Computations}}},
  author = {Hay, Nicholas and Russell, Stuart and Tolpin, David and Shimony, Solomon Eyal},
  date = {2014-08-09},
  eprint = {1408.2048},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1408.2048},
  urldate = {2022-03-11},
  abstract = {Sequential decision problems are often approximately solvable by simulating possible future action sequences. Metalevel decision procedures have been developed for selecting which action sequences to simulate, based on estimating the expected improvement in decision quality that would result from any particular simulation; an example is the recent work on using bandit algorithms to control Monte Carlo tree search in the game of Go. In this paper we develop a theoretical basis for metalevel decisions in the statistical framework of Bayesian selection problems, arguing (as others have done) that this is more appropriate than the bandit framework. We derive a number of basic results applicable to Monte Carlo selection problems, including the first finite sampling bounds for optimal policies in certain cases; we also provide a simple counterexample to the intuitive conjecture that an optimal policy will necessarily reach a decision in all cases. We then derive heuristic approximations in both Bayesian and distribution-free settings and demonstrate their superiority to bandit-based heuristics in one-shot decision problems and in Go.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/247RURET/Hay et al. - 2014 - Selecting Computations Theory and Applications.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/P4MV3VQI/1408.html}
}

@article{hazonNoiseCorrelationsNeural2022,
  title = {Noise Correlations in Neural Ensemble Activity Limit the Accuracy of Hippocampal Spatial Representations},
  author = {Hazon, Omer and Minces, Victor H. and Tomàs, David P. and Ganguli, Surya and Schnitzer, Mark J. and Jercog, Pablo E.},
  date = {2022-07-25},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {4276},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-31254-y},
  url = {https://www.nature.com/articles/s41467-022-31254-y},
  urldate = {2022-09-06},
  abstract = {Neurons in the CA1 area of the mouse hippocampus encode the position of the animal in an environment. However, given the variability in individual neurons responses, the accuracy of this code is still poorly understood. It was proposed that downstream areas could achieve high spatial accuracy by integrating the activity of thousands of neurons, but theoretical studies point to shared fluctuations in the firing rate as a potential limitation. Using high-throughput calcium imaging in freely moving mice, we demonstrated the limiting factors in the accuracy of the CA1 spatial code. We found that noise correlations in the hippocampus bound the estimation error of spatial coding to \textasciitilde 10\,cm (the size of a mouse). Maximal accuracy was obtained using approximately [300–1400] neurons, depending on the animal. These findings reveal intrinsic limits in the brain’s representations of space and suggest that single neurons downstream of the~hippocampus can extract maximal spatial information from several hundred inputs.},
  issue = {1},
  langid = {english},
  keywords = {Hippocampus,Spatial memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/RLLKRAYK/Hazon et al. - 2022 - Noise correlations in neural ensemble activity lim.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ZS45N3UW/s41467-022-31254-y.html}
}

@unpublished{hoControlMentalRepresentations2021,
  title = {Control of Mental Representations in Human Planning},
  author = {Ho, Mark K. and Abel, David and Correa, Carlos G. and Littman, Michael L. and Cohen, Jonathan D. and Griffiths, Thomas L.},
  date = {2021-05-14},
  eprint = {2105.06948},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.06948},
  urldate = {2021-12-07},
  abstract = {One of the most striking features of human cognition is the capacity to plan. Two aspects of human planning stand out: its efficiency, even in complex environments, and its flexibility, even in changing environments. Efficiency is especially impressive because directly computing an optimal plan is intractable, even for modestly complex tasks, and yet people successfully solve myriad everyday problems despite limited cognitive resources. Standard accounts in psychology, economics, and artificial intelligence have suggested this is because people have a mental representation of a task and then use heuristics to plan in that representation. However, this approach generally assumes that mental representations are fixed. Here, we propose that mental representations can be controlled and that this provides opportunities to adaptively simplify problems so they can be more easily reasoned about -- a process we refer to as construal. We construct a formal model of this process and, in a series of large, pre-registered behavioral experiments, show both that construal is subject to online cognitive control and that people form value-guided construals that optimally balance the complexity of a representation and its utility for planning and acting. These results demonstrate how strategically perceiving and conceiving problems facilitates the effective use of limited cognitive resources.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ISYIWTHC/Ho et al. - 2021 - Control of mental representations in human plannin.pdf}
}

@article{hodgesMazeProceduresRadialarm1996,
  title = {Maze Procedures: The Radial-Arm and Water Maze Compared},
  shorttitle = {Maze Procedures},
  author = {Hodges, Helen},
  date = {1996-06-01},
  journaltitle = {Cognitive Brain Research},
  shortjournal = {Cognitive Brain Research},
  series = {Proceedings of the {{Fifth International Meeting}} of the {{European Behavioural Pharmacology Society}} ({{EBPS}})},
  volume = {3},
  number = {3},
  pages = {167--181},
  issn = {0926-6410},
  doi = {10.1016/0926-6410(96)00004-3},
  url = {https://www.sciencedirect.com/science/article/pii/0926641096000043},
  urldate = {2022-05-16},
  abstract = {Open mazes are primarily designed to measure place learning and memory, using environmental visuospatial cues. However, maze tasks differ along many dimensions, including (1) types of apparatus, which vary from arenas (water maze: WM) to highly structured routes (radial-arm maze: RAM); (2) availability of visuospatial, associative or sensory cues; (3) task requirements which range from spontaneous exploration to complex sequences of choices; and (4) motivation which may involve aversive escape, the opportunity to shelter or to discover novel objects or food at particular locations. Given this diversity, it is likely that mazes tap a variety of processes that contribute to, or affect spatial learning. Hence ‘spatial’ abilities measured in one procedure may not resemble those engaged in another, posing problems for the interpretation of drug- or lesion-induced deficits. This review compares two types of maze that exemplify key differences in procedure: the RAM and the WM. (1) Visuospatial, associative and sensory factors contributing to place learning in the two mazes are discussed, together with the types of search strategy that they foster, their differing motivation and vulnerability to effects of non-spatial factors, such as stress and training regime. (2) The equivalence of memory processes (acquisition, working and reference memory) assessed in different mazes is considered, and the extent that these may generalise to non-spatial tasks. (3) Differences in application of the two mazes are evaluated. The WM is well-adapted to the study of selective visuospatial factors in place learning and working memory, but less suitable for repeated measures or for assessment of long-term memory deficits. The RAM detects steady-state reference and working-memory deficits, and is suitable for repeated measures, at the expense of precise analysis of the nature of the processes involved.},
  langid = {english},
  keywords = {Associative learning,Cognitive map,Radial-arm maze,Spatial learning and memory,To read,Water maze},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TLLK8W28/Hodges - 1996 - Maze procedures the radial-arm and water maze com.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/G9MJEN4K/0926641096000043.html}
}

@article{hollupAccumulationHippocampalPlace2001,
  title = {Accumulation of {{Hippocampal Place Fields}} at the {{Goal Location}} in an {{Annular Watermaze Task}}},
  author = {Hollup, Stig A. and Molden, Sturla and Donnett, James G. and Moser, May-Britt and Moser, Edvard I.},
  date = {2001-03-01},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {21},
  number = {5},
  eprint = {11222654},
  eprinttype = {pmid},
  pages = {1635--1644},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.21-05-01635.2001},
  url = {https://www.jneurosci.org/content/21/5/1635},
  urldate = {2022-01-26},
  abstract = {To explore the plastic representation of information in spatially selective hippocampal pyramidal neurons, we made multiple single-unit recordings in rats trained to find a hidden platform at a constant location in a hippocampal-dependent annular watermaze task. Hippocampal pyramidal cells exhibited place-related firing in the watermaze. Place fields tended to accumulate near the platform, even in probe trials without immediate escape. The percentage of cells with peak activity around the hidden platform was more than twice the percentage firing in equally large areas elsewhere in the arena. The effect was independent of the actual position of the platform in the room frame. It was dissociable from ongoing motor behavior and was not related to linear or angular speed, swim direction, or variation in hippocampal theta activity. There was no accumulation of firing in any particular region in rats that were trained with a variable platform location. These training-dependent effects suggest that regions of particular behavioral significance may be over-represented in the hippocampal spatial map, even when these regions are completely unmarked.},
  langid = {english},
  keywords = {hippocampus,learning,memory,place cells,plasticity,rat,recognition,spatial},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/PUSWRR8G/Hollup et al. - 2001 - Accumulation of Hippocampal Place Fields at the Go.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/EKEKJY98/1635.html}
}

@article{hollupAccumulationHippocampalPlace2001a,
  title = {Accumulation of {{Hippocampal Place Fields}} at the {{Goal Location}} in an {{Annular Watermaze Task}}},
  author = {Hollup, Stig A. and Molden, Sturla and Donnett, James G. and Moser, May-Britt and Moser, Edvard I.},
  date = {2001-03-01},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {21},
  number = {5},
  eprint = {11222654},
  eprinttype = {pmid},
  pages = {1635--1644},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.21-05-01635.2001},
  url = {https://www.jneurosci.org/content/21/5/1635},
  urldate = {2022-05-17},
  abstract = {To explore the plastic representation of information in spatially selective hippocampal pyramidal neurons, we made multiple single-unit recordings in rats trained to find a hidden platform at a constant location in a hippocampal-dependent annular watermaze task. Hippocampal pyramidal cells exhibited place-related firing in the watermaze. Place fields tended to accumulate near the platform, even in probe trials without immediate escape. The percentage of cells with peak activity around the hidden platform was more than twice the percentage firing in equally large areas elsewhere in the arena. The effect was independent of the actual position of the platform in the room frame. It was dissociable from ongoing motor behavior and was not related to linear or angular speed, swim direction, or variation in hippocampal theta activity. There was no accumulation of firing in any particular region in rats that were trained with a variable platform location. These training-dependent effects suggest that regions of particular behavioral significance may be over-represented in the hippocampal spatial map, even when these regions are completely unmarked.},
  langid = {english},
  keywords = {hippocampus,learning,memory,place cells,plasticity,rat,recognition,spatial,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6C7WIXJ4/Hollup et al. - 2001 - Accumulation of Hippocampal Place Fields at the Go.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/KIDQLX7M/1635.html}
}

@article{hoPeopleConstructSimplified2022,
  title = {People Construct Simplified Mental Representations to Plan},
  author = {Ho, Mark K. and Abel, David and Correa, Carlos G. and Littman, Michael L. and Cohen, Jonathan D. and Griffiths, Thomas L.},
  date = {2022-05-19},
  journaltitle = {Nature},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-04743-9},
  url = {https://www.nature.com/articles/s41586-022-04743-9},
  urldate = {2022-05-23},
  abstract = {One of the most striking features of human cognition is the ability to plan. Two aspects of human planning stand out—its efficiency and flexibility. Efficiency is especially impressive because plans must often be made in complex environments, and yet people successfully plan solutions to many everyday problems despite having limited cognitive resources1–3. Standard accounts in psychology, economics and artificial intelligence have suggested that human planning succeeds because people have a complete representation of a task and then use heuristics to plan future actions in that representation4–11. However, this approach generally assumes that task representations are fixed. Here we propose that task representations can be controlled and that such control provides opportunities to quickly simplify problems and more easily reason about them. We propose a computational account of this simplification process and, in a series of preregistered behavioural experiments, show that it is subject to online cognitive control12–14 and that people optimally balance the complexity of a task representation and its utility for planning and acting. These results demonstrate how strategically perceiving and conceiving problems facilitates the effective use of limited cognitive resources.},
  langid = {english},
  keywords = {Decision making,Human behaviour,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BBYJY7U9/Ho et al. - 2022 - People construct simplified mental representations.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/W7T4ND93/s41586-022-04743-9.html}
}

@inproceedings{houthooftVIMEVariationalInformation2016,
  title = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  shorttitle = {{{VIME}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Houthooft, Rein and Chen, Xi and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2016/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html},
  urldate = {2022-02-09},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TEC4XUPR/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf}
}

@article{huangInternallyGeneratedPopulation2022,
  title = {Internally Generated Population Activity in Cortical Networks Hinders Information Transmission},
  author = {Huang, Chengcheng and Pouget, Alexandre and Doiron, Brent},
  date = {2022-06},
  journaltitle = {Science Advances},
  volume = {8},
  number = {22},
  pages = {eabg5244},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.abg5244},
  url = {https://www.science.org/doi/10.1126/sciadv.abg5244},
  urldate = {2022-07-04},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TSBH2ZGN/Huang et al. - 2022 - Internally generated population activity in cortic.pdf}
}

@article{huntFormalizingPlanningInformation2021,
  title = {Formalizing Planning and Information Search in Naturalistic Decision-Making},
  author = {Hunt, L. T. and Daw, N. D. and Kaanders, P. and MacIver, M. A. and Mugan, U. and Procyk, E. and Redish, A. D. and Russo, E. and Scholl, J. and Stachenfeld, K. and Wilson, C. R. E. and Kolling, N.},
  date = {2021-08},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {24},
  number = {8},
  pages = {1051--1064},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00866-w},
  url = {https://www.nature.com/articles/s41593-021-00866-w},
  urldate = {2022-10-31},
  abstract = {Decisions made by mammals and birds are often temporally extended. They require planning and sampling of decision-relevant information. Our understanding of such decision-making remains in its infancy compared with simpler, forced-choice paradigms. However, recent advances in algorithms supporting planning and information search provide a lens through which we can explain neural and behavioral data in these tasks. We review these advances to obtain a clearer understanding for why planning and curiosity originated in certain species but not others; how activity in the medial temporal lobe, prefrontal and cingulate cortices may support these behaviors; and how planning and information search may complement each other as means to improve future action selection.},
  issue = {8},
  langid = {english},
  keywords = {Decision,Learning algorithms,Motivation,Psychology,Reward,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XFLME2BQ/Hunt et al. - 2021 - Formalizing planning and information search in nat.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/999VACRS/s41593-021-00866-w.html}
}

@article{huysComputationalPsychiatryBridge2016,
  title = {Computational Psychiatry as a Bridge from Neuroscience to Clinical Applications},
  author = {Huys, Quentin J M and Maia, Tiago V and Frank, Michael J},
  date = {2016-03},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {3},
  pages = {404--413},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4238},
  url = {http://www.nature.com/articles/nn.4238},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HWLNTQ43/Huys et al. - 2016 - Computational psychiatry as a bridge from neurosci.pdf}
}

@article{huysInterplayApproximatePlanning2015,
  title = {Interplay of Approximate Planning Strategies},
  author = {Huys, Quentin J. M. and Lally, Níall and Faulkner, Paul and Eshel, Neir and Seifritz, Erich and Gershman, Samuel J. and Dayan, Peter and Roiser, Jonathan P.},
  date = {2015-03-10},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {10},
  pages = {3098--3103},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1414219112},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.1414219112},
  urldate = {2022-09-16},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JY4J4VWT/Huys et al. - 2015 - Interplay of approximate planning strategies.pdf}
}

@article{igataPrioritizedExperienceReplays2021,
  title = {Prioritized Experience Replays on a Hippocampal Predictive Map for Learning},
  author = {Igata, Hideyoshi and Ikegaya, Yuji and Sasaki, Takuya},
  date = {2021-01-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {118},
  number = {1},
  eprint = {33443144},
  eprinttype = {pmid},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2011266118},
  url = {https://www.pnas.org/content/118/1/e2011266118},
  urldate = {2022-01-11},
  abstract = {Hippocampal cells are central to spatial and predictive representations, and experience replays by place cells are crucial for learning and memory. Nonetheless, how hippocampal replay patterns dynamically change during the learning process remains to be elucidated. Here, we designed a spatial task in which rats learned a new behavioral trajectory for reward. We found that as rats updated their behavioral strategies for a novel salient location, hippocampal cell ensembles increased theta-sequences and sharp wave ripple-associated synchronous spikes that preferentially replayed salient locations and reward-related contexts in reverse order. The directionality and contents of the replays progressively varied with learning, including an optimized path that had never been exploited by the animals, suggesting prioritized replays of significant experiences on a predictive map. Online feedback blockade of sharp wave ripples during a learning process inhibited stabilizing optimized behavior. These results implicate learning-associated experience replays that act to learn and reinforce specific behavioral strategies.},
  langid = {english},
  keywords = {hippocampus,learning,place cell,replay,ripple,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8D8N3JFA/Igata et al. - 2021 - Prioritized experience replays on a hippocampal pr.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/Y2PP2AR9/e2011266118.html}
}

@article{igataPrioritizedExperienceReplays2021a,
  title = {Prioritized Experience Replays on a Hippocampal Predictive Map for Learning},
  author = {Igata, Hideyoshi and Ikegaya, Yuji and Sasaki, Takuya},
  date = {2021-01-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {1},
  pages = {e2011266118},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2011266118},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.2011266118},
  urldate = {2022-08-12},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/G4YCMY8A/Igata et al. - 2021 - Prioritized experience replays on a hippocampal pr.pdf}
}

@article{jakschNearoptimalRegretBounds,
  title = {Near-Optimal {{Regret Bounds}} for {{Reinforcement Learning}}},
  author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter and At, Unileoben Ac and At, Unileoben Ac and At, Unileoben Ac},
  pages = {38},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NAUMQPCK/Jaksch et al. - Near-optimal Regret Bounds for Reinforcement Learn.pdf}
}

@article{janzSuccessorUncertaintiesExploration,
  title = {Successor {{Uncertainties}}: {{Exploration}} and {{Uncertainty}} in {{Temporal Difference Learning}}},
  author = {Janz, David and Hron, Jiri and Mazur, Przemysław and Hofmann, Katja and Hernández-Lobato, José Miguel and Tschiatschek, Sebastian},
  pages = {10},
  abstract = {Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8H9L8SWM/Janz et al. - Successor Uncertainties Exploration and Uncertain.pdf}
}

@article{johnsonNeuralEnsemblesCA32007,
  title = {Neural {{Ensembles}} in {{CA3 Transiently Encode Paths Forward}} of the {{Animal}} at a {{Decision Point}}},
  author = {Johnson, Adam and Redish, A. David},
  date = {2007-11-07},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {27},
  number = {45},
  eprint = {17989284},
  eprinttype = {pmid},
  pages = {12176--12189},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3761-07.2007},
  url = {https://www.jneurosci.org/content/27/45/12176},
  urldate = {2022-07-18},
  abstract = {Neural ensembles were recorded from the CA3 region of rats running on T-based decision tasks. Examination of neural representations of space at fast time scales revealed a transient but repeatable phenomenon as rats made a decision: the location reconstructed from the neural ensemble swept forward, first down one path and then the other. Estimated representations were coherent and preferentially swept ahead of the animal rather than behind the animal, implying it represented future possibilities rather than recently traveled paths. Similar phenomena occurred at other important decisions (such as in recovery from an error). Local field potentials from these sites contained pronounced theta and gamma frequencies, but no sharp wave frequencies. Forward-shifted spatial representations were influenced by task demands and experience. These data suggest that the hippocampus does not represent space as a passive computation, but rather that hippocampal spatial processing is an active process likely regulated by cognitive mechanisms.},
  langid = {english},
  keywords = {cognition,decision making,hippocampus,neural ensemble,place cell,To read,vicarious trial,visual error},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ERACNW5M/Johnson and Redish - 2007 - Neural Ensembles in CA3 Transiently Encode Paths F.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/TGD2WNDB/12176.html}
}

@article{kadotaIndexPoliciesGittins,
  title = {Index {{Policies}}: {{Gittins}} and {{Whittle Indices}}},
  author = {Kadota, Igor},
  pages = {73},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NS5XDBT8/Kadota - Index Policies Gittins and Whittle Indices.pdf}
}

@article{kaelblingPlanningActingPartially1998,
  title = {Planning and Acting in Partially Observable Stochastic Domains},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  date = {1998-05},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {101},
  number = {1-2},
  pages = {99--134},
  issn = {00043702},
  doi = {10.1016/S0004-3702(98)00023-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
  urldate = {2021-12-07},
  abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AK9AXRPD/Kaelbling et al. - 1998 - Planning and acting in partially observable stocha.pdf}
}

@article{kaelblingPlanningActingPartially1998a,
  title = {Planning and Acting in Partially Observable Stochastic Domains},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  date = {1998-05},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {101},
  number = {1-2},
  pages = {99--134},
  issn = {00043702},
  doi = {10.1016/S0004-3702(98)00023-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
  urldate = {2021-12-07},
  abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9VYJDIQN/Kaelbling et al. - 1998 - Planning and acting in partially observable stocha.pdf}
}

@article{kaliReplayRepairConsolidation,
  title = {Replay, {{Repair}} and {{Consolidation}}},
  author = {Káli, Szabolcs and Dayan, Peter},
  pages = {8},
  abstract = {A standard view of memory consolidation is that episodes are stored temporarily in the hippocampus, and are transferred to the neocortex through replay. Various recent experimental challenges to the idea of transfer, particularly for human memory, are forcing its re-evaluation. However, although there is independent neurophysiological evidence for replay, short of transfer, there are few theoretical ideas for what it might be doing. We suggest and demonstrate two important computational roles associated with neocortical indices.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/RYYEEG5M/Káli and Dayan - Replay, Repair and Consolidation.pdf}
}

@article{karinroelofsFrozenNoradrenergicCholinergic,
  title = {Frozen: {{Noradrenergic}} and Cholinergic Co-Contraction in Threat-Provoked Action Preparation},
  author = {{Karin Roelofs} and {Peter Dayan}},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/E9JYRUGQ/Karin Roelofs and Peter Dayan - Frozen Noradrenergic and cholinergic co-contracti.pdf}
}

@article{karlssonAwakeReplayRemote2009,
  title = {Awake Replay of Remote Experiences in the Hippocampus},
  author = {Karlsson, Mattias P and Frank, Loren M},
  date = {2009-07},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {12},
  number = {7},
  pages = {913--918},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2344},
  url = {http://www.nature.com/articles/nn.2344},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3YMIURQH/Karlsson and Frank - 2009 - Awake replay of remote experiences in the hippocam.pdf}
}

@article{kastnerMemoryAloneDoes2020,
  title = {Memory {{Alone Does Not Account}} for the {{Way Rats Learn}} a {{Simple Spatial Alternation Task}}},
  author = {Kastner, David B. and Gillespie, Anna K. and Dayan, Peter and Frank, Loren M.},
  date = {2020-09-16},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {40},
  number = {38},
  eprint = {32753514},
  eprinttype = {pmid},
  pages = {7311--7317},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0972-20.2020},
  url = {https://www.jneurosci.org/content/40/38/7311},
  urldate = {2022-08-14},
  abstract = {Animal behavior provides context for understanding disease models and physiology. However, that behavior is often characterized subjectively, creating opportunity for misinterpretation and misunderstanding. For example, spatial alternation tasks are treated as paradigmatic tools for examining memory; however, that link is actually an assumption. To test this assumption, we simulated a reinforcement learning (RL) agent equipped with a perfect memory process. We found that it learns a simple spatial alternation task more slowly and makes different errors than a group of male rats, illustrating that memory alone may not be sufficient to capture the behavior. We demonstrate that incorporating spatial biases permits rapid learning and enables the model to fit rodent behavior accurately. Our results suggest that even simple spatial alternation behaviors reflect multiple cognitive processes that need to be taken into account when studying animal behavior. SIGNIFICANCE STATEMENT Memory is a critical function for cognition whose impairment has significant clinical consequences. Experimental systems aimed at testing various sorts of memory are therefore also central. However, experimental designs to test memory are typically based on intuition about the underlying processes. We tested this using a popular behavioral paradigm: a spatial alternation task. Using behavioral modeling, we show that the straightforward intuition that these tasks just probe spatial memory fails to account for the speed at which rats learn or the types of errors they make. Only when memory-independent dynamic spatial preferences are added can the model learn like the rats. This highlights the importance of respecting the complexity of animal behavior to interpret neural function and validate disease models.},
  langid = {english},
  keywords = {behavioral modeling,learning and memory,reinforcement learning,rodent behavior},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/SBHG35TM/Kastner et al. - 2020 - Memory Alone Does Not Account for the Way Rats Lea.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/WEPWAGGL/7311.html}
}

@inproceedings{kattLearningPOMDPsMonte2017,
  title = {Learning in {{POMDPs}} with {{Monte Carlo Tree Search}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Katt, Sammie and Oliehoek, Frans A. and Amato, Christopher},
  date = {2017-07-17},
  pages = {1819--1827},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/katt17a.html},
  urldate = {2022-03-01},
  abstract = {The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DFRXL85P/Katt et al. - 2017 - Learning in POMDPs with Monte Carlo Tree Search.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/SPP8AHSY/Katt et al. - 2017 - Learning in POMDPs with Monte Carlo Tree Search.pdf}
}

@article{kaufmannBayesianUpperConfidence,
  title = {On {{Bayesian Upper Conﬁdence Bounds}} for {{Bandit Problems}}},
  author = {Kaufmann, Emilie and Cappe, Olivier and Garivier, Aurelien},
  pages = {9},
  abstract = {Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed BayesUCB, satisfies finite-time regret bounds that imply its asymptotic optimality. More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/V3SMUFSW/Kaufmann et al. - On Bayesian Upper Conﬁdence Bounds for Bandit Prob.pdf}
}

@article{keLEARNINGIMPROVEDDYNAMICS2019,
  title = {{{LEARNING IMPROVED DYNAMICS MODEL IN RE- INFORCEMENT LEARNING BY INCORPORATING THE LONG TERM FUTURE}}},
  author = {Ke, Nan Rosemary and Singh, Amanpreet and Touati, Ahmed and Goyal, Anirudh and Bengio, Yoshua and Parikh, Devi and Batra, Dhruv},
  date = {2019},
  pages = {14},
  abstract = {In model-based reinforcement learning, the agent interleaves between model learning and planning. These two components are inextricably intertwined. If the model is not able to provide sensible long-term prediction, the executed planner would exploit model flaws, which can yield catastrophic failures. This paper focuses on building a model that reasons about the long-term future and demonstrates how to use this for efficient planning and exploration. To this end, we build a latent-variable autoregressive model by leveraging recent ideas in variational inference. We argue that forcing latent variables to carry future information through an auxiliary task substantially improves long-term predictions. Moreover, by planning in the latent space, the planner’s solution is ensured to be within regions where the model is valid. An exploration strategy can be devised by searching for unlikely trajectories under the model. Our method achieves higher reward faster compared to baselines on a variety of tasks and environments in both the imitation learning and model-based reinforcement learning settings.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NSWS2GYB/Ke et al. - 2019 - LEARNING IMPROVED DYNAMICS MODEL IN RE- INFORCEMEN.pdf}
}

@article{keramatiAdaptiveIntegrationHabits2016,
  title = {Adaptive Integration of Habits into Depth-Limited Planning Defines a Habitual-Goal–Directed Spectrum},
  author = {Keramati, Mehdi and Smittenaar, Peter and Dolan, Raymond J. and Dayan, Peter},
  date = {2016-11-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {113},
  number = {45},
  eprint = {27791110},
  eprinttype = {pmid},
  pages = {12868--12873},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1609094113},
  url = {https://www.pnas.org/content/113/45/12868},
  urldate = {2022-01-17},
  abstract = {Behavioral and neural evidence reveal a prospective goal-directed decision process that relies on mental simulation of the environment, and a retrospective habitual process that caches returns previously garnered from available choices. Artificial systems combine the two by simulating the environment up to some depth and then exploiting habitual values as proxies for consequences that may arise in the further future. Using a three-step task, we provide evidence that human subjects use such a normative plan-until-habit strategy, implying a spectrum of approaches that interpolates between habitual and goal-directed responding. We found that increasing time pressure led to shallower goal-directed planning, suggesting that a speed-accuracy tradeoff controls the depth of planning with deeper search leading to more accurate evaluation, at the cost of slower decision-making. We conclude that subjects integrate habit-based cached values directly into goal-directed evaluations in a normative manner.},
  langid = {english},
  keywords = {habit,planning,reinforcement learning,speed/accuracy tradeoff,tree-based evaluation},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/U9FM3PV9/Keramati et al. - 2016 - Adaptive integration of habits into depth-limited .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ZTM8L3N4/12868.html}
}

@article{khamassiModelingAwakeHippocampal2020,
  title = {Modeling Awake Hippocampal Reactivations with Model-Based Bidirectional Search},
  author = {Khamassi, Mehdi and Girard, Benoît},
  date = {2020-04},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  volume = {114},
  number = {2},
  pages = {231--248},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-020-00817-x},
  url = {http://link.springer.com/10.1007/s00422-020-00817-x},
  urldate = {2021-12-07},
  abstract = {Hippocampal offline reactivations during reward-based learning, usually categorized as replay events, have been found to be important for performance improvement over time and for memory consolidation. Recent computational work has linked these phenomena to the need to transform reward information into stateaction values for decision-making and to propagate it to all relevant states of the environment. Nevertheless, it is still unclear whether an integrated reinforcement learning mechanism could account for the variety of awake hippocampal reactivations, including variety in order (forward and reverse reactivated trajectories) and variety in the location where they occur (reward site or decision-point). Here we present a model-based bidirectional search model which accounts for a variety of hippocampal reactivations. The model combines forward trajectory sampling from current position and backward sampling through prioritized sweeping from states associated with large reward prediction errors until the two trajectories connect. This is repeated until stabilization of state-action values (convergence), which could explain why hippocampal reactivations drastically diminish when the animal’s performance stabilizes. Simulations in a multiple T-maze task show that forward reactivations are prominently found at decision-points while backward reactivations are exclusively generated at reward sites. Finally, the model can generate imaginary trajectories that are not allowed to the agent during task performance. We raise some experimental predictions and implications for future studies of the role of the hippocampo-prefronto-striatal network in learning.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/H7HQJF5Z/Khamassi and Girard - 2020 - Modeling awake hippocampal reactivations with mode.pdf}
}

@article{khamassiModelingAwakeHippocampal2020a,
  title = {Modeling Awake Hippocampal Reactivations with Model-Based Bidirectional Search},
  author = {Khamassi, Mehdi and Girard, Benoît},
  date = {2020-04},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  volume = {114},
  number = {2},
  pages = {231--248},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-020-00817-x},
  url = {http://link.springer.com/10.1007/s00422-020-00817-x},
  urldate = {2021-12-07},
  abstract = {Hippocampal offline reactivations during reward-based learning, usually categorized as replay events, have been found to be important for performance improvement over time and for memory consolidation. Recent computational work has linked these phenomena to the need to transform reward information into stateaction values for decision-making and to propagate it to all relevant states of the environment. Nevertheless, it is still unclear whether an integrated reinforcement learning mechanism could account for the variety of awake hippocampal reactivations, including variety in order (forward and reverse reactivated trajectories) and variety in the location where they occur (reward site or decision-point). Here we present a model-based bidirectional search model which accounts for a variety of hippocampal reactivations. The model combines forward trajectory sampling from current position and backward sampling through prioritized sweeping from states associated with large reward prediction errors until the two trajectories connect. This is repeated until stabilization of state-action values (convergence), which could explain why hippocampal reactivations drastically diminish when the animal’s performance stabilizes. Simulations in a multiple T-maze task show that forward reactivations are prominently found at decision-points while backward reactivations are exclusively generated at reward sites. Finally, the model can generate imaginary trajectories that are not allowed to the agent during task performance. We raise some experimental predictions and implications for future studies of the role of the hippocampo-prefronto-striatal network in learning.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/H2425YB6/Khamassi and Girard - 2020 - Modeling awake hippocampal reactivations with mode.pdf}
}

@article{kimUnifiedFrameworkDopamine2020,
  title = {A {{Unified Framework}} for {{Dopamine Signals}} across {{Timescales}}},
  author = {Kim, HyungGoo R. and Malik, Athar N. and Mikhael, John G. and Bech, Pol and Tsutsui-Kimura, Iku and Sun, Fangmiao and Zhang, Yajun and Li, Yulong and Watabe-Uchida, Mitsuko and Gershman, Samuel J. and Uchida, Naoshige},
  date = {2020-12-10},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {183},
  number = {6},
  pages = {1600-1616.e25},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2020.11.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0092867420315300},
  urldate = {2022-02-18},
  abstract = {Rapid phasic activity of midbrain dopamine neurons is thought to signal reward prediction errors (RPEs), resembling temporal difference errors used in machine learning. However, recent studies describing slowly increasing dopamine signals have instead proposed that they represent state values and arise independent from somatic spiking activity. Here we developed experimental paradigms using virtual reality that disambiguate RPEs from values. We examined dopamine circuit activity at various stages, including somatic spiking, calcium signals at somata and axons, and striatal dopamine concentrations. Our results demonstrate that ramping dopamine signals are consistent with RPEs rather than value, and this ramping is observed at all stages examined. Ramping dopamine signals can be driven by a dynamic stimulus that indicates a gradual approach to a reward. We provide a unified computational understanding of rapid phasic and slowly ramping dopamine signals: dopamine neurons perform a derivative-like computation over values on a moment-by-moment basis.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UW9UWVZW/Kim et al. - 2020 - A Unified Framework for Dopamine Signals across Ti.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/C8ZNLQAA/S0092867420315300.html}
}

@article{kjaerbyMemoryenhancingPropertiesSleep2022,
  title = {Memory-Enhancing Properties of Sleep Depend on the Oscillatory Amplitude of Norepinephrine},
  author = {Kjaerby, Celia and Andersen, Mie and Hauglund, Natalie and Untiet, Verena and Dall, Camilla and Sigurdsson, Björn and Ding, Fengfei and Feng, Jiesi and Li, Yulong and Weikop, Pia and Hirase, Hajime and Nedergaard, Maiken},
  date = {2022-07-07},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  pages = {1--12},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01102-9},
  url = {https://www.nature.com/articles/s41593-022-01102-9},
  urldate = {2022-07-12},
  abstract = {Sleep has a complex micro-architecture, encompassing micro-arousals, sleep spindles and transitions between sleep stages. Fragmented sleep impairs memory consolidation, whereas spindle-rich and delta-rich non-rapid eye movement (NREM) sleep and rapid eye movement (REM) sleep promote it. However, the relationship between micro-arousals and memory-promoting aspects of sleep remains unclear. In this study, we used fiber photometry in mice to examine how release of the arousal mediator norepinephrine (NE) shapes sleep micro-architecture. Here we show that micro-arousals are generated in a periodic pattern during NREM sleep, riding on the peak of locus-coeruleus-generated infraslow oscillations of extracellular NE, whereas descending phases of NE oscillations drive spindles. The amplitude of NE oscillations is crucial for shaping sleep micro-architecture related to memory performance: prolonged descent of NE promotes spindle-enriched intermediate state and REM sleep but also associates with awakenings, whereas shorter NE descents uphold NREM sleep and micro-arousals. Thus, the NE oscillatory amplitude may be a target for improving sleep in sleep disorders.},
  langid = {english},
  keywords = {Consolidation,Locus coeruleus,Non-REM sleep,Wakefulness},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/D7KEM8DG/Kjaerby et al. - 2022 - Memory-enhancing properties of sleep depend on the.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/44KUBDCF/s41593-022-01102-9.html}
}

@article{knudsenHippocampalNeuronsConstruct2021,
  title = {Hippocampal Neurons Construct a Map of an Abstract Value Space},
  author = {Knudsen, Eric B. and Wallis, Joni D.},
  date = {2021-09},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {184},
  number = {18},
  pages = {4640-4650.e10},
  issn = {00928674},
  doi = {10.1016/j.cell.2021.07.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867421008369},
  urldate = {2021-12-07},
  abstract = {The hippocampus is thought to encode a ‘‘cognitive map,’’ a structural organization of knowledge about relationships in the world. Place cells, spatially selective hippocampal neurons that have been extensively studied in rodents, are one component of this map, describing the relative position of environmental features. However, whether this map extends to abstract, cognitive information remains unknown. Using the relative reward value of cues to define continuous ‘‘paths’’ through an abstract value space, we show that single neurons in primate hippocampus encode this space through value place fields, much like a rodent’s place neurons encode paths through physical space. Value place fields remapped when cues changed but also became increasingly correlated across contexts, allowing maps to become generalized. Our findings help explain the critical contribution of the hippocampus to value-based decision-making, providing a mechanism by which knowledge of relationships in the world can be incorporated into reward predictions for guiding decisions.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/83ELR8X5/Knudsen and Wallis - 2021 - Hippocampal neurons construct a map of an abstract.pdf}
}

@article{knudsenHippocampalNeuronsConstruct2021a,
  title = {Hippocampal Neurons Construct a Map of an Abstract Value Space},
  author = {Knudsen, Eric B. and Wallis, Joni D.},
  date = {2021-09-02},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {184},
  number = {18},
  pages = {4640-4650.e10},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2021.07.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0092867421008369},
  urldate = {2022-08-07},
  abstract = {The hippocampus is thought to encode a “cognitive map,” a structural organization of knowledge about relationships in the world. Place cells, spatially selective hippocampal neurons that have been extensively studied in rodents, are one component of this map, describing the relative position of environmental features. However, whether this map extends to abstract, cognitive information remains unknown. Using the relative reward value of cues to define continuous “paths” through an abstract value space, we show that single neurons in primate hippocampus encode this space through value place fields, much like a rodent’s place neurons encode paths through physical space. Value place fields remapped when cues changed but also became increasingly correlated across contexts, allowing maps to become generalized. Our findings help explain the critical contribution of the hippocampus to value-based decision-making, providing a mechanism by which knowledge of relationships in the world can be incorporated into reward predictions for guiding decisions.},
  langid = {english},
  keywords = {cognitive map,decision making,hippocampus,reward learning,value place cell},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/89FUHJ9G/Knudsen and Wallis - 2021 - Hippocampal neurons construct a map of an abstract.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/RBKLK589/S0092867421008369.html}
}

@article{knudsenTakingStockValue2022,
  title = {Taking Stock of Value in the Orbitofrontal Cortex},
  author = {Knudsen, Eric B. and Wallis, Joni D.},
  date = {2022-07},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {23},
  number = {7},
  pages = {428--438},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00589-2},
  url = {https://www.nature.com/articles/s41583-022-00589-2},
  urldate = {2022-08-07},
  abstract = {People with damage to the orbitofrontal cortex (OFC) have specific problems making decisions, whereas their other cognitive functions are spared. Neurophysiological studies have shown that OFC neurons fire in proportion to the value of anticipated outcomes. Thus, a central role of the OFC is to guide optimal decision-making by signalling values associated with different choices. Until recently, this view of OFC function dominated the field. New data, however, suggest that the OFC may have a much broader role in cognition by representing cognitive maps that can be used to guide behaviour and that value is just one of many variables that are important for behavioural control. In this Review, we critically evaluate these two alternative accounts of OFC function and examine how they might be reconciled.},
  issue = {7},
  langid = {english},
  keywords = {Decision,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/GYGRGIZX/s41583-022-00589-2.html}
}

@article{knudsenTakingStockValue2022a,
  title = {Taking Stock of Value in the Orbitofrontal Cortex},
  author = {Knudsen, Eric B. and Wallis, Joni D.},
  date = {2022-07},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {23},
  number = {7},
  pages = {428--438},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00589-2},
  url = {https://www.nature.com/articles/s41583-022-00589-2},
  urldate = {2022-08-07},
  abstract = {People with damage to the orbitofrontal cortex (OFC) have specific problems making decisions, whereas their other cognitive functions are spared. Neurophysiological studies have shown that OFC neurons fire in proportion to the value of anticipated outcomes. Thus, a central role of the OFC is to guide optimal decision-making by signalling values associated with different choices. Until recently, this view of OFC function dominated the field. New data, however, suggest that the OFC may have a much broader role in cognition by representing cognitive maps that can be used to guide behaviour and that value is just one of many variables that are important for behavioural control. In this Review, we critically evaluate these two alternative accounts of OFC function and examine how they might be reconciled.},
  issue = {7},
  langid = {english},
  keywords = {Decision,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LA4N2W7F/Knudsen and Wallis - 2022 - Taking stock of value in the orbitofrontal cortex.pdf}
}

@unpublished{kobayashiOptimisticReinforcementLearning2021,
  title = {Optimistic {{Reinforcement Learning}} by {{Forward Kullback-Leibler Divergence Optimization}}},
  author = {Kobayashi, Taisuke},
  date = {2021-05-27},
  eprint = {2105.12991},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.12991},
  urldate = {2021-12-07},
  abstract = {This paper addresses a new interpretation of reinforcement learning (RL) as reverse Kullback-Leibler (KL) divergence optimization, and derives a new optimization method using forward KL divergence. Although RL originally aims to maximize return indirectly through optimization of policy, the recent work by Levine has proposed a different derivation process with explicit consideration of optimality as stochastic variable. This paper follows this concept and formulates the traditional learning laws for both value function and policy as the optimization problems with reverse KL divergence including optimality. Focusing on the asymmetry of KL divergence, the new optimization problems with forward KL divergence are derived. Remarkably, such new optimization problems can be regarded as optimistic RL. That optimism is intuitively specified by a hyperparameter converted from an uncertainty parameter. In addition, it can be enhanced when it is integrated with prioritized experience replay and eligibility traces, both of which accelerate learning. The effects of this expected optimism was investigated through learning tendencies on numerical simulations using Pybullet. As a result, moderate optimism accelerated learning and yielded higher rewards. In a realistic robotic simulation, the proposed method with the moderate optimism outperformed one of the state-of-the-art RL method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6HZBKBQS/Kobayashi - 2021 - Optimistic Reinforcement Learning by Forward Kullb.pdf}
}

@article{krauseLargeMajorityAwake2022,
  title = {A Large Majority of Awake Hippocampal Sharp-Wave Ripples Feature Spatial Trajectories with Momentum},
  author = {Krause, Emma L. and Drugowitsch, Jan},
  date = {2022-02-16},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {110},
  number = {4},
  pages = {722-733.e8},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.11.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627321009521},
  urldate = {2022-04-26},
  abstract = {During periods of rest, hippocampal place cells feature bursts of activity called sharp-wave ripples (SWRs). Heuristic approaches have revealed that a small fraction of SWRs appear to “simulate” trajectories through the environment, called awake hippocampal replay. However, the functional role of a majority of these SWRs remains unclear. We find, using Bayesian model comparison of state-space models to characterize the spatiotemporal dynamics embedded in SWRs, that almost all SWRs of foraging rodents simulate such trajectories. Furthermore, these trajectories feature momentum, or inertia in their velocities, that mirrors the animals’ natural movement, in contrast to replay events during sleep, which lack such momentum. Last, we show that past analyses of replayed trajectories for navigational planning were biased by the heuristic SWR sub-selection. Our findings thus identify the dominant function of awake SWRs as simulating trajectories with momentum and provide a principled foundation for future work on their computational function.},
  langid = {english},
  keywords = {Bayesian model comparison,hippocampal replay,hippocampus,sharp-wave ripples,state-space models},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IU6QX7P4/Krause and Drugowitsch - 2022 - A large majority of awake hippocampal sharp-wave r.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/IFT8JU4H/S0896627321009521.html}
}

@article{krupicGridCellSymmetry2015,
  title = {Grid Cell Symmetry Is Shaped by Environmental Geometry},
  author = {Krupic, Julija and Bauza, Marius and Burton, Stephen and Barry, Caswell and O’Keefe, John},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  number = {7538},
  pages = {232--235},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14153},
  url = {https://www.nature.com/articles/nature14153},
  urldate = {2022-01-26},
  abstract = {Neuronal grid cells fire in a spatial grid pattern laid out across the surface of a familiar environment, however the role of environmental boundaries in the construction of this pattern is not well understood; this study shows that the grid pattern orients to the walls of polarized environments such as squares but not circles and that the hexagonal grid symmetry is permanently broken in highly polarized environments such as trapezoids.},
  issue = {7538},
  langid = {english},
  keywords = {Neural circuits},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Neural circuits Subject\_term\_id: neural-circuit},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QWY84UUE/Krupic et al. - 2015 - Grid cell symmetry is shaped by environmental geom.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/9VK89HQZ/nature14153.html}
}

@article{kubieHippocampalRemappingPhysiological2020,
  title = {Is Hippocampal Remapping the Physiological Basis for Context?},
  author = {Kubie, John L. and Levy, Eliott R. J. and Fenton, André A.},
  date = {2020},
  journaltitle = {Hippocampus},
  volume = {30},
  number = {8},
  pages = {851--864},
  issn = {1098-1063},
  doi = {10.1002/hipo.23160},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.23160},
  urldate = {2022-09-20},
  abstract = {In 1980, Nadel and Wilner extended Richard Hirsh's notion that the hippocampus creates environmental representations, called “contexts,” suggesting that the fundamental structure of context was the spatial representation proposed by O'Keefe and Nadel's landmark book, The Hippocampus as a Cognitive Map (1978). This book, in turn, derives from the discovery that individual hippocampal neurons act as place cells, with the complete set of place cells tiling an enclosure, forming a type of spatial map. It was found that unique environments had unique place cell representations. That is, if one takes the hippocampal map of a specific environment, this representation scrambles, or “remaps” when the animal is placed in a different environment. Several authors have speculated that “maps” and “remapping” form the physiological substrates for context and context shifting. One difficulty with this definition is that it is exclusively spatial; it can only be inferred when an animal locomotes in an enclosure. There are five aims for this article. The first is to give an historical overview of context as a variable that controls behavior. The second aim is to give an historical overview of concepts of place cell maps and remapping. The third aim is to propose an updated definition of a place cell map, based on temporal rather than spatial overlaps, which adds flexibility. The fourth aim is to address the issue of whether the biological phenomenon of hippocampal remapping, is, in fact, the substrate for shifts in the psychological phenomenon of context. The final aim is speculation of how contextual representations may contribute to effective behavior.},
  langid = {english},
  keywords = {context,hippocampal map,hippocampus,remapping},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.23160},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/62WZXMLL/Kubie et al. - 2020 - Is hippocampal remapping the physiological basis f.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/VY2SAFR8/hipo.html}
}

@article{kurth-nelsonFastSequencesNonspatial2016,
  title = {Fast {{Sequences}} of {{Non-spatial State Representations}} in {{Humans}}},
  author = {Kurth-Nelson, Zeb and Economides, Marcos and Dolan, Raymond J. and Dayan, Peter},
  date = {2016-07-06},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {91},
  number = {1},
  pages = {194--204},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.05.028},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627316302070},
  urldate = {2022-04-05},
  abstract = {Fast internally generated sequences of neural representations are suggested to support learning and online planning. However, these sequences have only been studied in the context of spatial tasks and never in humans. Here, we recorded magnetoencephalography (MEG) while human subjects performed a novel non-spatial reasoning task. The task required selecting paths through a set of six visual objects. We trained pattern classifiers on the MEG activity elicited by direct presentation of the visual objects alone and tested these classifiers on activity recorded during periods when no object was presented. During these object-free periods, the brain spontaneously visited representations of approximately four objects in fast sequences lasting on the order of 120~ms. These sequences followed backward trajectories along the permissible paths in the task. Thus, spontaneous fast sequential representation of states can be measured non-invasively in humans, and these sequences may be a fundamental feature of neural computation across tasks.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9F72EDYS/Kurth-Nelson et al. - 2016 - Fast Sequences of Non-spatial State Representation.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/Z4RRMS5Z/S0896627316302070.html}
}

@misc{kurth-nelsonReplayCompositionalComputation2022,
  title = {Replay and Compositional Computation},
  author = {Kurth-Nelson, Zeb and Behrens, Timothy and Wayne, Greg and Miller, Kevin and Luettgau, Lennart and Dolan, Ray and Liu, Yunzhe and Schwartenbeck, Philipp},
  date = {2022-09-15},
  number = {arXiv:2209.07453},
  eprint = {2209.07453},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.07453},
  url = {http://arxiv.org/abs/2209.07453},
  urldate = {2022-09-26},
  abstract = {It is still a mystery how the human brain radically generalizes from previous experiences to solve new problems with very little data, an ability that far surpasses deep learning. Here, we propose a speculative hypothesis: that 'replay' in the brain implements a form of compositional computation where entities are assembled into meaningful structures. We review recent advances in the neuroscience of replay, highlighting how the hippocampus flexibly binds objects to generalizable roles and how replay strings these role-bound objects into compound statements. We suggest new experiments to test this hypothesis. We end by noting the bidirectional interactions between replay and other brain systems and their relationship to AI methods that hybridize neural networks with compositional computation.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/L59GXATC/Kurth-Nelson et al. - 2022 - Replay and compositional computation.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/NYTZYS8K/2209.html}
}

@article{laiAsymptoticallyEfficientAdaptive1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T.L and Robbins, Herbert},
  date = {1985-03},
  journaltitle = {Advances in Applied Mathematics},
  shortjournal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {01968858},
  doi = {10.1016/0196-8858(85)90002-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0196885885900028},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NQT5V5S3/Lai and Robbins - 1985 - Asymptotically efficient adaptive allocation rules.pdf}
}

@article{lecunTutorialEnergyBasedLearning,
  title = {A {{Tutorial}} on {{Energy-Based Learning}}},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc’Aurelio and Huang, Fu Jie},
  pages = {59},
  abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/STEKHBZT/LeCun et al. - A Tutorial on Energy-Based Learning.pdf}
}

@article{leeBayesianOptimisticKullback2019,
  title = {Bayesian Optimistic {{Kullback}}–{{Leibler}} Exploration},
  author = {Lee, Kanghoon and Kim, Geon-Hyeong and Ortega, Pedro and Lee, Daniel D. and Kim, Kee-Eung},
  date = {2019-05},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {108},
  number = {5},
  pages = {765--783},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-018-5767-4},
  url = {http://link.springer.com/10.1007/s10994-018-5767-4},
  urldate = {2021-12-07},
  abstract = {We consider a Bayesian approach to model-based reinforcement learning, where the agent uses a distribution of environment models to find the action that optimally trades off exploration and exploitation. Unfortunately, it is intractable to find the Bayes-optimal solution to the problem except for restricted cases. In this paper, we present BOKLE, a simple algorithm that uses Kullback–Leibler divergence to constrain the set of plausible models for guiding the exploration. We provide a formal analysis that this algorithm is near Bayes-optimal with high probability. We also show an asymptotic relation between the solution pursued by BOKLE and a well-known algorithm called Bayesian exploration bonus. Finally, we show experimental results that clearly demonstrate the exploration efficiency of the algorithm.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DNTZVV84/Lee et al. - 2019 - Bayesian optimistic Kullback–Leibler exploration.pdf}
}

@article{leeDopamineFacilitatesAssociative2021,
  title = {Dopamine Facilitates Associative Memory Encoding in the Entorhinal Cortex},
  author = {Lee, Jason Y. and Jun, Heechul and Soma, Shogo and Nakazono, Tomoaki and Shiraiwa, Kaori and Dasgupta, Ananya and Nakagawa, Tatsuki and Xie, Jiayun L. and Chavez, Jasmine and Romo, Rodrigo and Yungblut, Sandra and Hagihara, Meiko and Murata, Koshi and Igarashi, Kei M.},
  date = {2021-10},
  journaltitle = {Nature},
  volume = {598},
  number = {7880},
  pages = {321--326},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03948-8},
  url = {https://www.nature.com/articles/s41586-021-03948-8},
  urldate = {2022-04-13},
  abstract = {Mounting evidence shows that dopamine in the striatum is critically involved in reward-based reinforcement learning1,2. However, it remains unclear how dopamine reward signals influence the entorhinal–hippocampal circuit, another brain network that is crucial for learning and memory3–5. Here, using cell-type-specific electrophysiological recording6, we show that dopamine signals from the ventral tegmental area and substantia nigra control the encoding of cue–reward association rules in layer 2a fan cells of the lateral entorhinal cortex (LEC). When mice learned novel olfactory cue–reward associations using a pre-learned association rule, spike representations of LEC fan cells grouped newly learned rewarded cues with a pre-learned rewarded cue, but separated them from a pre-learned unrewarded cue. Optogenetic inhibition of fan cells impaired the learning of new associations while sparing the retrieval of pre-learned memory. Using fibre photometry, we found that dopamine sends novelty-induced reward expectation signals to the LEC. Inhibition of LEC dopamine signals disrupted the associative encoding of fan cells and impaired learning performance. These results suggest that LEC fan cells represent a cognitive map of abstract task rules, and that LEC dopamine facilitates the incorporation of new memories into this map.},
  issue = {7880},
  langid = {english},
  keywords = {Cortex,Hippocampus,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QJLY7NX9/Lee et al. - 2021 - Dopamine facilitates associative memory encoding i.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/HKTZM4J3/s41586-021-03948-8.html}
}

@article{leeGradualTranslocationSpatial2006,
  title = {Gradual {{Translocation}} of {{Spatial Correlates}} of {{Neuronal Firing}} in the {{Hippocampus}} toward {{Prospective Reward Locations}}},
  author = {Lee, Inah and Griffin, Amy L. and Zilli, Eric A. and Eichenbaum, Howard and Hasselmo, Michael E.},
  date = {2006-09-07},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {51},
  number = {5},
  pages = {639--650},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2006.06.033},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627306005836},
  urldate = {2022-05-17},
  abstract = {In a continuous T-maze alternation task, CA1 complex-spike neurons in the hippocampus differentially fire as the rat traverses overlapping segments of the maze (i.e., the stem) repeatedly via alternate routes. The temporal dynamics of this phenomenon were further investigated in the current study. Rats learned the alternation task from the first day of acquisition and the differential firing pattern in the stem was observed accordingly. More importantly, we report a phenomenon in which spatial correlates of CA1 neuronal ensembles gradually changed from their original firing locations, shifting toward prospective goal locations in the continuous T-maze alternation task. The relative locations of simultaneously recorded firing fields, however, were preserved within the ensemble spatial representation during this shifting. The within-session shifts in~preferred firing locations in the absence of any changes in the environment suggest that certain cognitive factors can significantly alter the location-bound coding scheme of hippocampal neurons.},
  langid = {english},
  keywords = {SYSBIO,SYSNEURO,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6HH2252S/Lee et al. - 2006 - Gradual Translocation of Spatial Correlates of Neu.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/E7AFFWDT/S0896627306005836.html}
}

@article{lehnertSuccessorFeaturesCombine,
  title = {Successor {{Features Combine Elements}} of {{Model-Free}} and {{Model-based Reinforcement Learning}}},
  author = {Lehnert, Lucas and Littman, Michael L},
  pages = {53},
  abstract = {A key question in reinforcement learning is how an intelligent agent can generalize knowledge across different inputs. By generalizing across different inputs, information learned for one input can be immediately reused for improving predictions for another input. Reusing information allows an agent to compute an optimal decision-making strategy using less data. State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. This article analyzes properties of different latent state spaces, leading to new connections between modelbased and model-free reinforcement learning. Successor features, which predict frequencies of future observations, form a link between model-based and model-free learning: Learning to predict future expected reward outcomes, a key characteristic of model-based agents, is equivalent to learning successor features. Learning successor features is a form of temporal difference learning and is equivalent to learning to predict a single policy’s utility, which is a characteristic of model-free agents. Drawing on the connection between model-based reinforcement learning and successor features, we demonstrate that representations that are predictive of future reward outcomes generalize across variations in both transitions and rewards. This result extends previous work on successor features, which is constrained to fixed transitions and assumes re-learning of the transferred state representation.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3CMC78IY/Lehnert and Littman - Successor Features Combine Elements of Model-Free .pdf}
}

@article{lengyelComputationalTheoriesFunction2005,
  title = {Computational Theories on the Function of Theta Oscillations},
  author = {Lengyel, Máté and Huhn, Zsófia and Érdi, Péter},
  date = {2005-06},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  volume = {92},
  number = {6},
  pages = {393--408},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-005-0567-x},
  url = {http://link.springer.com/10.1007/s00422-005-0567-x},
  urldate = {2022-10-07},
  abstract = {Neural rhythms can be studied in terms of conditions for their generation, or in terms of their functional significance. The theta oscillation is a particularly prominent rhythm, reported to be present in many brain areas, and related to many important cognitive processes. The generating mechanisms of theta have extensively been studied and reviewed elsewhere; here we discuss ideas that have accumulated over the past decades on the computational roles it may subserve. Theories propose different aspects of theta oscillations as being relevant for their cognitive functions: limit cycle oscillations in neuronal firing rates, subthreshold membrane potential oscillations, periodic modulation of synaptic transmission and plasticity, and phase precession of hippocampal place cells. The relevant experimental data is briefly summarized in the light of these theories. Specific models proposing a function for theta in pattern recognition, memory, sequence learning and navigation are reviewed critically. Difficulties with testing and comparing alternative models are discussed, along with potentially important future research directions in the field.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/N9ADINSH/Lengyel et al. - 2005 - Computational theories on the function of theta os.pdf}
}

@inproceedings{lengyelHippocampalContributionsControl2007,
  title = {Hippocampal {{Contributions}} to {{Control}}: {{The Third Way}}},
  shorttitle = {Hippocampal {{Contributions}} to {{Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lengyel, Máté and Dayan, Peter},
  date = {2007},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2007/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html},
  urldate = {2022-04-12},
  abstract = {Recent experimental studies have focused on the specialization of different neural structures for different types of instrumental behavior. Recent theoretical work has provided normative accounts for why there should be more than one control system, and how the output of different controllers can be integrated. Two par- ticlar controllers have been identified, one associated with a forward model and the prefrontal cortex and a second associated with computationally simpler, habit- ual, actor-critic methods and part of the striatum. We argue here for the normative appropriateness of an additional, but so far marginalized control system, associ- ated with episodic memory, and involving the hippocampus and medial temporal cortices. We analyze in depth a class of simple environments to show that episodic control should be useful in a range of cases characterized by complexity and in- ferential noise, and most particularly at the very early stages of learning, long before habitization has set in. We interpret data on the transfer of control from the hippocampus to the striatum in the light of this hypothesis.},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CNVUBEZG/Lengyel and Dayan - 2007 - Hippocampal Contributions to Control The Third Wa.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/PUZV3DF6/proof_thirdway.pdf}
}

@article{lengyelUncertaintyPhaseOscillatory,
  title = {Uncertainty, Phase and Oscillatory Hippocampal Recall},
  author = {Lengyel, Máté and Dayan, Peter},
  pages = {8},
  abstract = {Many neural areas, notably, the hippocampus, show structured, dynamical, population behavior such as coordinated oscillations. It has long been observed that such oscillations provide a substrate for representing analog information in the firing phases of neurons relative to the underlying population rhythm. However, it has become increasingly clear that it is essential for neural populations to represent uncertainty about the information they capture, and the substantial recent work on neural codes for uncertainty has omitted any analysis of oscillatory systems. Here, we observe that, since neurons in an oscillatory network need not only fire once in each cycle (or even at all), uncertainty about the analog quantities each neuron represents by its firing phase might naturally be reported through the degree of concentration of the spikes that it fires. We apply this theory to memory in a model of oscillatory associative recall in hippocampal area CA3. Although it is not well treated in the literature, representing and manipulating uncertainty is fundamental to competent memory; our theory enables us to view CA3 as an effective uncertainty-aware, retrieval system.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/46KWPWI3/Lengyel and Dayan - Uncertainty, phase and oscillatory hippocampal rec.pdf}
}

@article{leutgebIndependentCodesSpatial2005,
  title = {Independent {{Codes}} for {{Spatial}} and {{Episodic Memory}} in {{Hippocampal Neuronal Ensembles}}},
  author = {Leutgeb, Stefan and Leutgeb, Jill K. and Barnes, Carol A. and Moser, Edvard I. and McNaughton, Bruce L. and Moser, May-Britt},
  date = {2005-07-22},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {309},
  number = {5734},
  pages = {619--623},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1114037},
  url = {https://www.science.org/doi/10.1126/science.1114037},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/K9EWR5JQ/Leutgeb et al. - 2005 - Independent Codes for Spatial and Episodic Memory .pdf}
}

@article{linSelfimprovingReactiveAgents1992,
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  author = {Lin, Long-Ji},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {293--321},
  issn = {1573-0565},
  doi = {10.1007/BF00992699},
  url = {https://doi.org/10.1007/BF00992699},
  urldate = {2022-05-09},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
  langid = {english},
  keywords = {connectionist networks,planning,Reinforcement learning,teaching},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/4VWRLFMT/Lin - 1992 - Self-improving reactive agents based on reinforcem.pdf}
}

@article{linSelfimprovingReactiveAgents1992a,
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  author = {Lin, Long-Ji},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {293--321},
  issn = {1573-0565},
  doi = {10.1007/BF00992699},
  url = {https://doi.org/10.1007/BF00992699},
  urldate = {2022-10-31},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
  langid = {english},
  keywords = {connectionist networks,planning,Reinforcement learning,teaching},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NMLAC83B/Lin - 1992 - Self-improving reactive agents based on reinforcem.pdf}
}

@article{littmanTutorialPartiallyObservable2009,
  title = {A Tutorial on Partially Observable {{Markov}} Decision Processes},
  author = {Littman, Michael L.},
  date = {2009-06},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {53},
  number = {3},
  pages = {119--125},
  issn = {00222496},
  doi = {10.1016/j.jmp.2009.01.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249609000042},
  urldate = {2021-12-07},
  abstract = {The partially observable Markov decision process (POMDP) model of environments was first explored in the engineering and operations research communities 40 years ago. More recently, the model has been embraced by researchers in artificial intelligence and machine learning, leading to a flurry of solution algorithms that can identify optimal or near-optimal behavior in many environments represented as POMDPs. The purpose of this article is to introduce the POMDP model to behavioral scientists who may wish to apply the framework to the problem of understanding normative behavior in experimental settings. The article includes concrete examples using a publicly-available POMDP solution package.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8Q3TU865/Littman - 2009 - A tutorial on partially observable Markov decision.pdf}
}

@article{liuDecodingCognitionSpontaneous2022,
  title = {Decoding Cognition from Spontaneous Neural Activity},
  author = {Liu, Yunzhe and Nour, Matthew M. and Schuck, Nicolas W. and Behrens, Timothy E. J. and Dolan, Raymond J.},
  date = {2022-04},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {23},
  number = {4},
  pages = {204--214},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00570-z},
  url = {https://www.nature.com/articles/s41583-022-00570-z},
  urldate = {2022-05-11},
  abstract = {In human neuroscience, studies of cognition are rarely grounded in non-task-evoked, ‘spontaneous’ neural activity. Indeed, studies of spontaneous activity tend to focus predominantly on intrinsic neural patterns (for example, resting-state networks). Taking a ‘representation-rich’ approach bridges the gap between cognition and resting-state communities: this approach relies on decoding task-related representations from spontaneous neural activity, allowing quantification of the representational content and rich dynamics of such activity. For example, if we know the neural representation of an episodic memory, we can decode its subsequent replay during rest. We argue that such an approach advances cognitive research beyond a focus on immediate task demand and provides insight into the functional relevance of the intrinsic neural pattern (for example, the default mode network). This in turn enables a greater integration between human and animal neuroscience, facilitating experimental testing of theoretical accounts of intrinsic activity, and opening new avenues of research in psychiatry.},
  issue = {4},
  langid = {english},
  keywords = {Decision,Human behaviour,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FFN9YN6N/Liu et al. - 2022 - Decoding cognition from spontaneous neural activit.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/A2SL95TH/s41583-022-00570-z.html}
}

@article{liuExperienceReplayAssociated2021a,
  title = {Experience Replay Is Associated with Efficient Nonlocal Learning},
  author = {Liu, Yunzhe and Mattar, Marcelo G. and Behrens, Timothy E. J. and Daw, Nathaniel D. and Dolan, Raymond J.},
  date = {2021-05-21},
  journaltitle = {Science},
  volume = {372},
  number = {6544},
  pages = {eabf1357},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.abf1357},
  url = {https://www.science.org/doi/full/10.1126/science.abf1357},
  urldate = {2022-05-06},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/SPSXWZU3/Liu et al. - 2021 - Experience replay is associated with efficient non.pdf}
}

@article{liuPrioritizedExperienceReplay2022,
  title = {Prioritized {{Experience Replay}} Based on {{Multi-armed Bandit}}},
  author = {Liu, Ximing and Zhu, Tianqing and Jiang, Cuiqing and Ye, Dayong and {Fuqing Zhao}},
  date = {2022-03-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {189},
  pages = {116023},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.116023},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417421013701},
  urldate = {2021-12-17},
  abstract = {Experience replay has been widely used in deep reinforcement learning. The learning algorithm allows online reinforcement learning agents to remember and reuse experiences from the past. In order to further improve the sampling efficiency for experience replay, the most useful experiences are expected to be sampled with higher frequency. Existing methods usually designed their sampling strategy according to a few criteria, but they tended to combine different criteria in a linear or fixed manner, where the strategy were static and independent of the agent learner. This ignores the dynamic attribute of the environment and thus can only lead to a suboptimal performance. In this work, we propose a dynamic experience replay strategy according to the interaction between the agent and environment, which is called Prioritized Experience Replay based on Multi-armed Bandit (PERMAB). PERMAB can adaptively combine multiple priority criteria to measure the importance of the experience. In particular, the weight of each assessing criterion can be adaptively adjusted from episode to episode according to their respective contribution to the agent performance, which guarantees useful criterion to be weighted more in its current state. The proposed replay strategy is able to take both sample informativeness and diversity into consideration, which could significantly boosts learning ability and speed of the game agent. Experimental results show that PERMAB accelerates the network learning and achieves a better performance compared to baseline algorithms on seven benchmark environments with various difficulties.},
  langid = {english},
  keywords = {Deep Q-network,Deep reinforcement learning,Experience replay,Multi-armed Bandit,Q-learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZSHQRXVZ/Liu et al. - 2022 - Prioritized Experience Replay based on Multi-armed.pdf}
}

@article{liuPrioritizedExperienceReplay2022a,
  title = {Prioritized {{Experience Replay}} Based on {{Multi-armed Bandit}}},
  author = {Liu, Ximing and Zhu, Tianqing and Jiang, Cuiqing and Ye, Dayong and Zhao, Fuqing},
  date = {2022-03-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {189},
  pages = {116023},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.116023},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417421013701},
  urldate = {2022-09-06},
  abstract = {Experience replay has been widely used in deep reinforcement learning. The learning algorithm allows online reinforcement learning agents to remember and reuse experiences from the past. In order to further improve the sampling efficiency for experience replay, the most useful experiences are expected to be sampled with higher frequency. Existing methods usually designed their sampling strategy according to a few criteria, but they tended to combine different criteria in a linear or fixed manner, where the strategy were static and independent of the agent learner. This ignores the dynamic attribute of the environment and thus can only lead to a suboptimal performance. In this work, we propose a dynamic experience replay strategy according to the interaction between the agent and environment, which is called Prioritized Experience Replay based on Multi-armed Bandit (PERMAB). PERMAB can adaptively combine multiple priority criteria to measure the importance of the experience. In particular, the weight of each assessing criterion can be adaptively adjusted from episode to episode according to their respective contribution to the agent performance, which guarantees useful criterion to be weighted more in its current state. The proposed replay strategy is able to take both sample informativeness and diversity into consideration, which could significantly boosts learning ability and speed of the game agent. Experimental results show that PERMAB accelerates the network learning and achieves a better performance compared to baseline algorithms on seven benchmark environments with various difficulties.},
  langid = {english},
  keywords = {Deep Q-network,Deep reinforcement learning,Experience replay,Multi-armed Bandit,Q-learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/74GCMMHS/Liu et al. - 2022 - Prioritized Experience Replay based on Multi-armed.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/37KFJUE2/S0957417421013701.html}
}

@article{lloydTampingRampingAlgorithmic2015,
  title = {Tamping {{Ramping}}: {{Algorithmic}}, {{Implementational}}, and {{Computational Explanations}} of {{Phasic Dopamine Signals}} in the {{Accumbens}}},
  shorttitle = {Tamping {{Ramping}}},
  author = {Lloyd, Kevin and Dayan, Peter},
  date = {2015-12-23},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {11},
  number = {12},
  pages = {e1004622},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004622},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004622},
  urldate = {2022-10-05},
  abstract = {Substantial evidence suggests that the phasic activity of dopamine neurons represents reinforcement learning’s temporal difference prediction error. However, recent reports of ramp-like increases in dopamine concentration in the striatum when animals are about to act, or are about to reach rewards, appear to pose a challenge to established thinking. This is because the implied activity is persistently predictable by preceding stimuli, and so cannot arise as this sort of prediction error. Here, we explore three possible accounts of such ramping signals: (a) the resolution of uncertainty about the timing of action; (b) the direct influence of dopamine over mechanisms associated with making choices; and (c) a new model of discounted vigour. Collectively, these suggest that dopamine ramps may be explained, with only minor disturbance, by standard theoretical ideas, though urgent questions remain regarding their proximal cause. We suggest experimental approaches to disentangling which of the proposed mechanisms are responsible for dopamine ramps.},
  langid = {english},
  keywords = {Animal communication,Behavior,Decision making,Dopamine,Dopaminergics,Neostriatum,Neurons,Signal processing},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/RBTJKGV7/Lloyd and Dayan - 2015 - Tamping Ramping Algorithmic, Implementational, an.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/YBB3JZ4K/article.html}
}

@article{lottemActivationSerotoninNeurons2018,
  title = {Activation of Serotonin Neurons Promotes Active Persistence in a Probabilistic Foraging Task},
  author = {Lottem, Eran and Banerjee, Dhruba and Vertechi, Pietro and Sarra, Dario and oude Lohuis, Matthijs and Mainen, Zachary F.},
  date = {2018-03-08},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {9},
  number = {1},
  pages = {1000},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-03438-y},
  url = {https://www.nature.com/articles/s41467-018-03438-y},
  urldate = {2022-04-04},
  abstract = {The neuromodulator serotonin (5-HT) has been implicated in a variety of functions that involve patience or impulse control. Many of these effects are consistent with a long-standing theory that 5-HT promotes behavioral inhibition, a motivational bias favoring passive over active behaviors. To further test this idea, we studied the impact of 5-HT in a probabilistic foraging task, in which mice must learn the statistics of the environment and infer when to leave a depleted foraging site for the next. Critically, mice were required to actively nose-poke in order to exploit a given site. We show that optogenetic activation of 5-HT neurons in the dorsal raphe nucleus increases the willingness of mice to actively attempt to exploit a reward site before giving up. These results indicate that behavioral inhibition is not an adequate description of 5-HT function and suggest that a unified account must be based on a higher-order function.},
  issue = {1},
  langid = {english},
  keywords = {Motivation,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JMVC8HLY/Lottem et al. - 2018 - Activation of serotonin neurons promotes active pe.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/VBC2J9DZ/s41467-018-03438-y.html}
}

@article{lowDynamicReversibleRemapping2021,
  title = {Dynamic and Reversible Remapping of Network Representations in an Unchanging Environment},
  author = {Low, Isabel I. C. and Williams, Alex H. and Campbell, Malcolm G. and Linderman, Scott W. and Giocomo, Lisa M.},
  date = {2021-09-15},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {109},
  number = {18},
  pages = {2967-2980.e11},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.07.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627321005043},
  urldate = {2022-04-29},
  abstract = {Neurons in the medial entorhinal cortex alter their firing properties in response to environmental changes. This flexibility in neural coding is hypothesized to support navigation and memory by dividing sensory experience into unique episodes. However, it is unknown how the entorhinal circuit as a whole transitions between different representations when sensory information is not delineated into discrete contexts. Here we describe rapid and reversible transitions between multiple spatial maps of an unchanging task and environment. These remapping events were synchronized across hundreds of neurons, differentially affected navigational cell types, and correlated with changes in running speed. Despite widespread changes in spatial coding, remapping comprised a translation along a single dimension in population-level activity space, enabling simple decoding strategies. These findings provoke reconsideration of how the medial entorhinal cortex dynamically represents space and suggest a remarkable capacity of cortical circuits to rapidly and substantially reorganize their neural representations.},
  langid = {english},
  keywords = {attractor manifolds,behavioral state,dynamic coding,medial entorhinal cortex,population coding},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/34WQFSSB/Low et al. - 2021 - Dynamic and reversible remapping of network repres.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/FHQF4L4E/S0896627321005043.html}
}

@article{luNeuralNetworkModel2022,
  title = {A Neural Network Model of When to Retrieve and Encode Episodic Memories},
  author = {Lu, Qihong and Hasson, Uri and Norman, Kenneth A},
  editor = {Badre, David},
  date = {2022-02-10},
  journaltitle = {eLife},
  volume = {11},
  pages = {e74445},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.74445},
  url = {https://doi.org/10.7554/eLife.74445},
  urldate = {2022-02-15},
  abstract = {Recent human behavioral and neuroimaging results suggest that people are selective in when they encode and retrieve episodic memories. To explain these findings, we trained a memory-augmented neural network to use its episodic memory to support prediction of upcoming states in an environment where past situations sometimes reoccur. We found that the network learned to retrieve selectively as a function of several factors, including its uncertainty about the upcoming state. Additionally, we found that selectively encoding episodic memories at the end of an event (but not mid-event) led to better subsequent prediction performance. In all of these cases, the benefits of selective retrieval and encoding can be explained in terms of reducing the risk of retrieving irrelevant memories. Overall, these modeling results provide a resource-rational account of why episodic retrieval and encoding should be selective and lead to several testable predictions.},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JZYARZI4/Lu et al. - 2022 - A neural network model of when to retrieve and enc.pdf}
}

@article{macdonaldHippocampalTimeCells2011,
  title = {Hippocampal “{{Time Cells}}” {{Bridge}} the {{Gap}} in {{Memory}} for {{Discontiguous Events}}},
  author = {MacDonald, Christopher~J. and Lepage, Kyle~Q. and Eden, Uri~T. and Eichenbaum, Howard},
  date = {2011-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {71},
  number = {4},
  pages = {737--749},
  issn = {08966273},
  doi = {10.1016/j.neuron.2011.07.012},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731100609X},
  urldate = {2021-12-07},
  abstract = {The hippocampus is critical to remembering the flow of events in distinct experiences and, in doing so, bridges temporal gaps between discontiguous events. Here, we report a robust hippocampal representation of sequence memories, highlighted by ‘‘time cells’’ that encode successive moments during an empty temporal gap between the key events, while also encoding location and ongoing behavior. Furthermore, just as most place cells ‘‘remap’’ when a salient spatial cue is altered, most time cells form qualitatively different representations (‘‘retime’’) when the main temporal parameter is altered. Hippocampal neurons also differentially encode the key events and disambiguate different event sequences to compose unique, temporally organized representations of specific experiences. These findings suggest that hippocampal neural ensembles segment temporally organized memories much the same as they represent locations of important events in spatially defined environments.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/J5PXJIZM/MacDonald et al. - 2011 - Hippocampal “Time Cells” Bridge the Gap in Memory .pdf}
}

@article{mackayInformationTheoryInference,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  author = {MacKay, David J C},
  pages = {640},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DQJPXIMG/MacKay - Information Theory, Inference, and Learning Algori.pdf}
}

@unpublished{madanFastSlowLearning2021,
  title = {Fast and {{Slow Learning}} of {{Recurrent Independent Mechanisms}}},
  author = {Madan, Kanika and Ke, Nan Rosemary and Goyal, Anirudh and Schölkopf, Bernhard and Bengio, Yoshua},
  date = {2021-05-18},
  eprint = {2105.08710},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.08710},
  urldate = {2021-12-07},
  abstract = {Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, metaparameters.We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/33EECCP5/Madan et al. - 2021 - Fast and Slow Learning of Recurrent Independent Me.pdf}
}

@article{madaraszBetterTransferLearning,
  title = {Better {{Transfer Learning}} with {{Inferred Successor Maps}}},
  author = {Madarasz, Tamas and Behrens, Tim},
  pages = {12},
  abstract = {Humans and animals show remarkable flexibility in adjusting their behaviour when their goals, or rewards in the environment change. While such flexibility is a hallmark of intelligent behaviour, these multi-task scenarios remain an important challenge for machine learning algorithms and neurobiological models alike. We investigated two approaches that could enable this flexibility: factorized representations, which abstract away general aspects of a task from those prone to change, and nonparametric, memory-based approaches, which can provide a principled way of using similarity to past experiences to guide current behaviour. In particular, we combine the successor representation (SR), that factors the value of actions into expected outcomes and corresponding rewards, with evaluating task similarity through clustering the space of rewards. The proposed algorithm inverts a generative model over tasks, and dynamically samples from a flexible number of distinct SR maps while accumulating evidence about the current task context through amortized inference. It improves SR’s transfer capabilities and outperforms competing algorithms and baselines in settings with both known and unsignalled rewards changes. Further, as a neurobiological model of spatial coding in the hippocampus, it explains important signatures of this representation, such as the "flickering" behaviour of hippocampal maps, and trajectory-dependent place cells (so-called splitter cells) and their dynamics. We thus provide a novel algorithmic approach for multi-task learning, as well as a common normative framework that links together these different characteristics of the brain’s spatial representation.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/35CUNGZ9/Madarasz and Behrens - Better Transfer Learning with Inferred Successor M.pdf}
}

@article{madaraszSupplementaryInformationBetter,
  title = {Supplementary {{Information}} for ’{{Better}} Transfer Learning with Inferred Successor Maps’},
  author = {Madarasz, Tamas J and Behrens, Timothy E},
  pages = {12},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IFULU8EM/Madarasz and Behrens - Supplementary Information for ’Better transfer lea.pdf}
}

@article{maingretHippocampocorticalCouplingMediates2016,
  title = {Hippocampo-Cortical Coupling Mediates Memory Consolidation during Sleep},
  author = {Maingret, Nicolas and Girardeau, Gabrielle and Todorova, Ralitsa and Goutierre, Marie and Zugaro, Michaël},
  date = {2016-07},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {7},
  pages = {959--964},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4304},
  url = {http://www.nature.com/articles/nn.4304},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9T3BI6A8/Maingret et al. - 2016 - Hippocampo-cortical coupling mediates memory conso.pdf}
}

@article{marinoPredictiveCodingVariational2021,
  title = {Predictive {{Coding}}, {{Variational Autoencoders}}, and {{Biological Connections}}},
  author = {Marino, Joseph},
  date = {2021-12-15},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {34},
  number = {1},
  pages = {1--44},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01458},
  url = {https://doi.org/10.1162/neco_a_01458},
  urldate = {2022-04-01},
  abstract = {We present a review of predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (nonlinear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field.},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/MJU4Y3N6/Marino - 2021 - Predictive Coding, Variational Autoencoders, and B.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/HK3AMW4B/Predictive-Coding-Variational-Autoencoders-and.html}
}

@article{mattarPlanningBrain2022,
  title = {Planning in the Brain},
  author = {Mattar, Marcelo G. and Lengyel, Máté},
  date = {2022-03-16},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {110},
  number = {6},
  pages = {914--934},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.12.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627321010357},
  urldate = {2022-09-26},
  abstract = {Recent breakthroughs in artificial intelligence (AI) have enabled machines to plan in tasks previously thought to be uniquely human. Meanwhile, the planning algorithms implemented by the brain itself remain largely unknown. Here, we review neural and behavioral data in sequential decision-making tasks that elucidate the ways in which the brain does—and does not—plan. To systematically review available biological data, we create a taxonomy of planning algorithms by summarizing the relevant design choices for such algorithms in AI. Across species, recording techniques, and task paradigms, we find converging evidence that the brain represents future states consistent with a class of planning algorithms within our taxonomy—focused, depth-limited, and serial. However, we argue that current data are insufficient for addressing more detailed algorithmic questions. We propose a new approach leveraging AI advances to drive experiments that can adjudicate between competing candidate algorithms.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UQRX9RVP/Mattar and Lengyel - 2022 - Planning in the brain.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/V9BA386J/S0896627321010357.html}
}

@article{mattarPrioritizedMemoryAccess2018,
  title = {Prioritized Memory Access Explains Planning and Hippocampal Replay},
  author = {Mattar, Marcelo G. and Daw, Nathaniel D.},
  date = {2018-11},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {21},
  number = {11},
  pages = {1609--1617},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0232-z},
  url = {https://www.nature.com/articles/s41593-018-0232-z},
  urldate = {2022-02-17},
  abstract = {To make decisions, animals must evaluate candidate choices by accessing memories of relevant experiences. Yet little is known about which experiences are considered or ignored during deliberation, which ultimately governs choice. We propose a normative theory predicting which memories should be accessed at each moment to optimize future decisions. Using nonlocal ‘replay’ of spatial locations in hippocampus as a window into memory access, we simulate a spatial navigation task in which an agent accesses memories of locations sequentially, ordered by utility: how much extra reward would be earned due to better choices. This prioritization balances two desiderata: the need to evaluate imminent choices versus the gain from propagating newly encountered information to preceding locations. Our theory offers a simple explanation for numerous findings about place cells; unifies seemingly disparate proposed functions of replay including planning, learning, and consolidation; and posits a mechanism whose dysfunction may underlie pathologies like rumination and craving.},
  issue = {11},
  langid = {english},
  keywords = {Decision,Hippocampus},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/SUHHX6AL/Mattar and Daw - 2018 - Prioritized memory access explains planning and hi.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/Y7BGZDPX/s41593-018-0232-z.html}
}

@article{mcfarlaneSurveyExplorationStrategies,
  title = {A {{Survey}} of {{Exploration Strategies}} in {{Reinforcement Learning}}},
  author = {McFarlane, Roger},
  pages = {10},
  abstract = {A fundamental issue in reinforcement learning algorithms is the balance between exploration of the environment and exploitation of information already obtained by the agent. This paper surveys exploration strategies used in reinforcement learning and summarizes the existing research with respect to their applicability and effectiveness.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NYMQI5EQ/McFarlane - A Survey of Exploration Strategies in Reinforcemen.pdf}
}

@article{mckenzieHippocampalRepresentationRelated2014,
  title = {Hippocampal {{Representation}} of {{Related}} and {{Opposing Memories Develop}} within {{Distinct}}, {{Hierarchically Organized Neural Schemas}}},
  author = {McKenzie, Sam and Frank, Andrea~J. and Kinsky, Nathaniel~R. and Porter, Blake and Rivière, Pamela~D. and Eichenbaum, Howard},
  date = {2014-07},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {83},
  number = {1},
  pages = {202--215},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.05.019},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731400405X},
  urldate = {2021-12-07},
  abstract = {Recent evidence suggests that the hippocampus may integrate overlapping memories into relational representations, or schemas, that link indirectly related events and support flexible memory expression. Here we explored the nature of hippocampal neural population representations for multiple features of events and the locations and contexts in which they occurred. Hippocampal networks developed hierarchical organizations of associated elements of related but separately acquired memories within a context, and distinct organizations for memories where the contexts differentiated objectreward associations. These findings reveal neural mechanisms for the development and organization of relational representations.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7WU99I5T/McKenzie et al. - 2014 - Hippocampal Representation of Related and Opposing.pdf}
}

@article{mckenziePreexistingHippocampalNetwork2021,
  title = {Preexisting Hippocampal Network Dynamics Constrain Optogenetically Induced Place Fields},
  author = {McKenzie, Sam and Huszár, Roman and English, Daniel F. and Kim, Kanghwan and Christensen, Fletcher and Yoon, Euisik and Buzsáki, György},
  date = {2021-03-17},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {109},
  number = {6},
  pages = {1040-1054.e7},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.01.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627321000337},
  urldate = {2022-09-26},
  abstract = {Memory models often emphasize the need to encode novel patterns of neural activity imposed by sensory drive. Prior learning and innate architecture likely restrict neural plasticity, however. Here, we test how the incorporation of synthetic hippocampal signals is constrained by preexisting circuit dynamics. We optogenetically stimulated small groups of CA1 neurons as mice traversed a chosen segment of a linear track, mimicking the emergence of place fields. Stimulation induced persistent place field remapping in stimulated and non-stimulated neurons. The emergence of place fields could be predicted from sporadic firing in the new place field location and the temporal relationship to peer neurons before the optogenetic perturbation. Circuit modification was reflected by altered spike transmission between connected pyramidal cells and inhibitory interneurons, which persisted during post-experience sleep. We hypothesize that optogenetic perturbation unmasked sub-threshold place fields. Plasticity in recurrent/lateral inhibition may drive learning through the rapid association of existing states.},
  langid = {english},
  keywords = {blank slate,consolidation,inhibition,learning,Memory,optogenetics,place cells,plasticity,preconfigured brain,sharp wave ripples},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FIK8YTBE/McKenzie et al. - 2021 - Preexisting hippocampal network dynamics constrain.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/EZGFPG9T/S0896627321000337.html}
}

@article{mcnamaraDopaminergicNeuronsPromote2014,
  title = {Dopaminergic Neurons Promote Hippocampal Reactivation and Spatial Memory Persistence},
  author = {McNamara, Colin G and Tejero-Cantero, Álvaro and Trouche, Stéphanie and Campo-Urriza, Natalia and Dupret, David},
  date = {2014-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {17},
  number = {12},
  pages = {1658--1660},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3843},
  url = {http://www.nature.com/articles/nn.3843},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BQGLTDLX/McNamara et al. - 2014 - Dopaminergic neurons promote hippocampal reactivat.pdf}
}

@article{mcnaughtonPathIntegrationNeural2006,
  title = {Path Integration and the Neural Basis of the 'Cognitive Map'},
  author = {McNaughton, Bruce L. and Battaglia, Francesco P. and Jensen, Ole and Moser, Edvard I. and Moser, May-Britt},
  date = {2006-08},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {7},
  number = {8},
  pages = {663--678},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn1932},
  url = {https://www.nature.com/articles/nrn1932},
  urldate = {2022-05-19},
  abstract = {Accumulating evidence indicates that the foundation of mammalian spatial orientation and learning is based on an internal network that can keep track of relative position and orientation (from an arbitrary starting point) on the basis of integration of self-motion cues derived from locomotion, vestibular activation and optic flow (path integration).Place cells in the hippocampal formation exhibit elevated activity at discrete spots in a given environment, and this spatial representation is determined primarily on the basis of which cells were active at the starting point and how far and in what direction the animal has moved since then. Environmental features become associatively bound to this intrinsic spatial framework and can serve to correct for cumulative error in the path integration process.Theoretical studies suggested that a path integration system could involve cooperative interactions (attractor dynamics) among a population of place coding neurons, the synaptic coupling of which defines a two-dimensional attractor map. These cells would communicate with an additional group of neurons, the activity of which depends on the conjunction of movement speed, location and orientation (head direction) information, allowing position on the attractor map to be updated by self-motion information.The attractor map hypothesis contains an inherent boundary problem: what happens when the animal's movements carry it beyond the boundary of the map? One solution to this problem is to make the boundaries of the map periodic by coupling neurons at each edge to those on the opposite edge, resulting in a toroidal synaptic matrix. This solution predicts that, in a sufficiently large space, place cells would exhibit a regularly spaced grid of place fields, something that has never been observed in the hippocampus proper.Recent discoveries in layer II of the medial entorhinal cortex (MEC), the main source of hippocampal afferents, indicate that these cells do have regularly spaced place fields (grid cells). In addition, cells in the deeper layers of this structure exhibit grid fields that are conjunctive for head orientation and movement speed. Pure head direction neurons are also found there. Therefore, all of the components of previous theoretical models for path integration appear in the MEC, suggesting that this network is the core of the path integration system.The scale of MEC spatial firing grids increases systematically from the dorsal to the ventral poles of this structure, in much the same way as is observed for hippocampal place cells, and we show how non-periodic hippocampal place fields could arise from the combination of inputs from entorhinal grid cells, if the inputs cover a range of spatial scales rather than a single scale. This phenomenon, in the spatial domain, is analogous to the low frequency 'beats' heard when two pure tones of slightly different frequencies are combined.The problem of how a two-dimensional synaptic matrix with periodic boundary conditions, postulated to underlie grid cell behaviour, could be self-organized in early development is addressed. Based on principles derived from Alan Turing's theory of spontaneous symmetry breaking in chemical systems, we suggest that topographically organized, grid-like patterns of neural activity might be present in the immature cortex, and that these activity patterns guide the development of the proposed periodic synaptic matrix through a mechanism involving competitive synaptic plasticity.},
  issue = {8},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BPZGVYF7/McNaughton et al. - 2006 - Path integration and the neural basis of the 'cogn.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/PGC4J3HJ/nrn1932.html}
}

@article{mehtaExperienceDependentAsymmetricShape2000,
  title = {Experience-{{Dependent Asymmetric Shape}} of {{Hippocampal Receptive Fields}}},
  author = {Mehta, Mayank R. and Quirk, Michael C. and Wilson, Matthew A.},
  date = {2000-03-01},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {25},
  number = {3},
  pages = {707--715},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(00)81072-7},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627300810727},
  urldate = {2022-01-25},
  abstract = {We propose a novel parameter, namely, the skewness, or asymmetry, of the shape of a receptive field to characterize two properties of hippocampal place fields. First, a majority of hippocampal receptive fields on linear tracks are negatively skewed, such that during a single pass the firing rate is low as the rat enters the field but high as it exits. Second, while the place fields are symmetric at the beginning of a session, they become highly asymmetric with experience. Further experiments suggest that these results are likely to arise due to synaptic plasticity during behavior. Using a purely feed forward neural network model, we show that following repeated directional activation, NMDA-dependent long-term potentiation/long-term depotentiation (LTP/LTD) could result in an experience-dependent asymmetrization of receptive fields.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AR67V6AB/Mehta et al. - 2000 - Experience-Dependent Asymmetric Shape of Hippocamp.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/SEZ8Y8FB/S0896627300810727.html}
}

@article{meshulamCollectiveBehaviorPlace2017,
  title = {Collective {{Behavior}} of {{Place}} and {{Non-place Neurons}} in the {{Hippocampal Network}}},
  author = {Meshulam, Leenoy and Gauthier, Jeffrey L. and Brody, Carlos D. and Tank, David W. and Bialek, William},
  date = {2017-12-06},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {96},
  number = {5},
  pages = {1178-1191.e4},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.10.027},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627317309960},
  urldate = {2022-09-21},
  abstract = {Discussions of the hippocampus often focus on place cells, but many neurons are not place cells in any given environment. Here we describe the collective activity in such mixed populations, treating place and non-place cells on the same footing. We start with optical imaging experiments on CA1 in mice as they run along a virtual linear track and use maximum entropy methods to approximate the distribution of patterns of activity in the population, matching the correlations between pairs of cells but otherwise assuming as little structure as possible. We find that these simple models accurately predict the activity of each neuron from the state of all the other neurons in the network, regardless of how well that neuron codes for position. Our results suggest that understanding the neural activity may require not only knowledge of the external variables modulating it but also of the internal network state.},
  langid = {english},
  keywords = {collective phenomena,hippocampus,maximum entropy,pairwise correlations,place cells},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/J69LQMIU/Meshulam et al. - 2017 - Collective Behavior of Place and Non-place Neurons.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/M4RGQG8M/S0896627317309960.html}
}

@inproceedings{metelliPropagatingUncertaintyReinforcement2019,
  title = {Propagating {{Uncertainty}} in {{Reinforcement Learning}} via {{Wasserstein Barycenters}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Metelli, Alberto Maria and Likmeta, Amarildo and Restelli, Marcello},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2019/hash/f83630579d055dc5843ae693e7cdafe0-Abstract.html},
  urldate = {2022-10-11},
  abstract = {How does the uncertainty of the value function propagate when performing temporal difference learning? In this paper, we address this question by proposing a Bayesian framework in which we employ approximate posterior distributions to model the uncertainty of the value function and Wasserstein barycenters to propagate it across state-action pairs. Leveraging on these tools, we present an algorithm, Wasserstein Q-Learning (WQL), starting in the tabular case and then, we show how it can be extended to deal with continuous domains. Furthermore, we prove that, under mild assumptions, a slight variation of WQL enjoys desirable theoretical properties in the tabular setting. Finally, we present an experimental campaign to show the effectiveness of WQL on finite problems, compared to several RL algorithms, some of which are specifically designed for exploration, along with some preliminary results on Atari games.},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TMK57CZL/Metelli et al. - 2019 - Propagating Uncertainty in Reinforcement Learning .pdf}
}

@article{meuleauExplorationMultiStateEnvironments,
  title = {Exploration of {{Multi-State Environments}}: {{Local Measures}} and {{Back-Propagation}} of {{Uncertainty}}},
  author = {Meuleau, Nicolas},
  pages = {38},
  abstract = {This paper presents an action selection technique for reinforcement learning in stationary Markovian environments. This technique may be used in direct algorithms such as Q-learning, or in indirect algorithms such as adaptive dynamic programming. It is based on two principles. The first is to define a local measure of the uncertainty using the theory of bandit problems. We show that such a measure suffers from several drawbacks. In particular, a direct application of it leads to algorithms of low quality that can be easily misled by particular configurations of the environment. The second basic principle was introduced to eliminate this drawback. It consists of assimilating the local measures of uncertainty to rewards, and back-propagating them with the dynamic programming or temporal difference mechanisms. This allows reproducing global-scale reasoning about the uncertainty, using only local measures of it. Numerical simulations clearly show the efficiency of these propositions.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZFN4B7DP/Meuleau - Exploration of Multi-State Environments Local Mea.pdf}
}

@article{michonPostlearningHippocampalReplay2019,
  title = {Post-Learning {{Hippocampal Replay Selectively Reinforces Spatial Memory}} for {{Highly Rewarded Locations}}},
  author = {Michon, Frédéric and Sun, Jyh-Jang and Kim, Chae Young and Ciliberti, Davide and Kloosterman, Fabian},
  date = {2019-05-06},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {29},
  number = {9},
  pages = {1436-1444.e5},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2019.03.048},
  url = {https://www.sciencedirect.com/science/article/pii/S096098221930346X},
  urldate = {2022-08-12},
  abstract = {Offline replay of hippocampal neural patterns supports the acquisition of new tasks in novel contexts, but its contribution to consolidation of salient experiences in a familiar context is unknown. Here, we show that in a highly familiar spatial memory task, large rewards selectively enhanced performance for demanding task configurations. The reward-related enhancement was sensitive to ripple-specific disruption, and the proportion of replay events positively correlated with reward size and task demands. Hippocampal replay thus selectively enhances memory of highly rewarded locations in a familiar context.},
  langid = {english},
  keywords = {behavior,consolidation,hippocampus,memory,rats,replay,reward,sharp wave ripples},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CEBXIAY8/Michon et al. - 2019 - Post-learning Hippocampal Replay Selectively Reinf.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/3XNZKVNX/S096098221930346X.html}
}

@article{michonSingletrialDynamicsHippocampal2021,
  title = {Single-Trial Dynamics of Hippocampal Spatial Representations Are Modulated by Reward Value},
  author = {Michon, Frédéric and Krul, Esther and Sun, Jyh-Jang and Kloosterman, Fabian},
  date = {2021-10-25},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {31},
  number = {20},
  pages = {4423-4435.e5},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2021.07.058},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982221010484},
  urldate = {2022-05-18},
  abstract = {Reward value is known to modulate learning speed in spatial memory tasks, but little is known about its influence on the dynamical changes in hippocampal spatial representations. Here, we monitored the trial-to-trial changes in hippocampal place cell activity during the acquisition of place-reward associations with varying reward size. We show a faster reorganization and stabilization of the hippocampal place map when a goal location is associated with a large reward. The reorganization is driven by both rate changes and the appearance and disappearance of place fields. The occurrence of hippocampal replay activity largely followed the dynamics of changes in spatial representations. Replay patterns became more selectively tuned toward behaviorally relevant experiences over the course of learning via the refined contributions of specific cell subpopulations. These results suggest that high reward value enhances memory retention by accelerating the formation and stabilization of the hippocampal cognitive map and selectively enhancing its reactivation during learning.},
  langid = {english},
  keywords = {hippocampus,learning,place cells,replay,reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/K2MQAJP3/Michon2021.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/F4ZBMS7G/S0960982221010484.html}
}

@article{mikhaelRoleStateUncertainty2022,
  title = {The Role of State Uncertainty in the Dynamics of Dopamine},
  author = {Mikhael, John G. and Kim, HyungGoo R. and Uchida, Naoshige and Gershman, Samuel J.},
  date = {2022-02-02},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2022.01.025},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982222000367},
  urldate = {2022-02-15},
  abstract = {Reinforcement learning models of the basal ganglia map the phasic dopamine signal to reward prediction errors (RPEs). Conventional models assert that, when a stimulus predicts a reward with fixed delay, dopamine activity during the delay should converge to baseline through learning. However, recent studies have found that dopamine ramps up before reward in certain conditions even after learning, thus challenging the conventional models. In this work, we show that sensory feedback causes an unbiased learner to produce RPE ramps. Our model predicts that when feedback gradually decreases during a trial, dopamine activity should resemble a “bump,” whose ramp-up phase should, furthermore, be greater than that of conditions where the feedback stays high. We trained mice on a virtual navigation task with varying brightness, and both predictions were empirically observed. In sum, our theoretical and experimental results reconcile the seemingly conflicting data on dopamine behaviors under the RPE hypothesis.},
  langid = {english},
  keywords = {bumps,dopamine,ramps,reinforcement learning,reward prediction error,sensory feedback,state uncertainty,state value,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AHDVGBBJ/Mikhael et al. - 2022 - The role of state uncertainty in the dynamics of d.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/CYZ3CJ8B/S0960982222000367.html}
}

@article{millerDorsalHippocampusContributes2017,
  title = {Dorsal Hippocampus Contributes to Model-Based Planning},
  author = {Miller, Kevin J and Botvinick, Matthew M and Brody, Carlos D},
  date = {2017-09-01},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {20},
  number = {9},
  pages = {1269--1276},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4613},
  url = {http://www.nature.com/articles/nn.4613},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/36Z7AYMZ/Miller et al. - 2017 - Dorsal hippocampus contributes to model-based plan.pdf}
}

@article{miyazakiRewardProbabilityTiming2018,
  title = {Reward Probability and Timing Uncertainty Alter the Effect of Dorsal Raphe Serotonin Neurons on Patience},
  author = {Miyazaki, Katsuhiko and Miyazaki, Kayoko W. and Yamanaka, Akihiro and Tokuda, Tomoki and Tanaka, Kenji F. and Doya, Kenji},
  date = {2018-06-01},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {9},
  number = {1},
  pages = {2048},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04496-y},
  url = {https://www.nature.com/articles/s41467-018-04496-y},
  urldate = {2022-04-05},
  abstract = {Recent experiments have shown that optogenetic activation of serotonin neurons in the dorsal raphe nucleus (DRN) in mice enhances patience in waiting for future rewards. Here, we show that serotonin effect in promoting waiting is maximized by both high probability and high timing uncertainty of reward. Optogenetic activation of serotonergic neurons prolongs waiting time in no-reward trials in a task with 75\% food reward probability, but not with 50 or 25\% reward probabilities. Serotonin effect in promoting waiting increases when the timing of reward presentation becomes unpredictable. To coherently explain the experimental data, we propose a Bayesian decision model of waiting that assumes that serotonin neuron activation increases the prior probability or subjective confidence of reward delivery. The present data and modeling point to the possibility of a generalized role of serotonin in resolving trade-offs, not only between immediate and delayed rewards, but also between sensory evidence and subjective confidence.},
  issue = {1},
  langid = {english},
  keywords = {Decision,Motivation,Operant learning,Reward,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3G786TRE/Miyazaki et al. - 2018 - Reward probability and timing uncertainty alter th.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/Z6WU8R7Q/s41467-018-04496-y.html}
}

@article{mobbsForagingFoundationsDecision2018,
  title = {Foraging for Foundations in Decision Neuroscience: Insights from Ethology},
  shorttitle = {Foraging for Foundations in Decision Neuroscience},
  author = {Mobbs, Dean and Trimmer, Pete C. and Blumstein, Daniel T. and Dayan, Peter},
  date = {2018-07},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {19},
  number = {7},
  pages = {419--427},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-018-0010-7},
  url = {https://www.nature.com/articles/s41583-018-0010-7},
  urldate = {2022-02-21},
  abstract = {Modern decision neuroscience offers a powerful and broad account of human behaviour using computational techniques that link psychological and neuroscientific approaches to the ways that individuals can generate near-optimal choices in complex controlled environments. However, until recently, relatively little attention has been paid to the extent to which the structure of experimental environments relates to natural scenarios, and the survival problems that individuals have evolved to solve. This situation not only risks leaving decision-theoretic accounts ungrounded but also makes various aspects of the solutions, such as hard-wired or Pavlovian policies, difficult to interpret in the natural world. Here, we suggest importing concepts, paradigms and approaches from the fields of ethology and behavioural ecology, which concentrate on the contextual and functional correlates of decisions made about foraging and escape and address these lacunae.},
  issue = {7},
  langid = {english},
  keywords = {Animal behaviour,Behavioural ecology,Decision,Psychology,Striatum,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5EMQZZM8/Mobbs et al. - 2018 - Foraging for foundations in decision neuroscience.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/IMGGTJF6/s41583-018-0010-7.html}
}

@unpublished{moerlandModelbasedReinforcementLearning2021,
  title = {Model-Based {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Model-Based {{Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Jonker, Catholijn M.},
  date = {2021-02-25},
  eprint = {2006.16712},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.16712},
  urldate = {2022-03-07},
  abstract = {Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a key challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two section, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL, like enhanced data efficiency, targeted exploration, and improved stability. The survey also draws connection to several related RL fields, like hierarchical RL and transfer. Altogether, the survey presents a broad conceptual overview of planning-learning combinations for MDP optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DJN638A7/Moerland et al. - 2021 - Model-based Reinforcement Learning A Survey.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/W8GA273Q/2006.html}
}

@article{momennejadOfflineReplaySupports2018,
  title = {Offline Replay Supports Planning in Human Reinforcement Learning},
  author = {Momennejad, Ida and Otto, A Ross and Daw, Nathaniel D and Norman, Kenneth A},
  editor = {Badre, David and Frank, Michael J},
  date = {2018-12-14},
  journaltitle = {eLife},
  volume = {7},
  pages = {e32548},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.32548},
  url = {https://doi.org/10.7554/eLife.32548},
  urldate = {2022-05-05},
  abstract = {Making decisions in sequentially structured tasks requires integrating distally acquired information. The extensive computational cost of such integration challenges planning methods that integrate online, at decision time. Furthermore, it remains unclear whether ‘offline’ integration during replay supports planning, and if so which memories should be replayed. Inspired by machine learning, we propose that (a) offline replay of trajectories facilitates integrating representations that guide decisions, and (b) unsigned prediction errors (uncertainty) trigger such integrative replay. We designed a 2-step revaluation task for fMRI, whereby participants needed to integrate changes in rewards with past knowledge to optimally replan decisions. As predicted, we found that (a) multi-voxel pattern evidence for off-task replay predicts subsequent replanning; (b) neural sensitivity to uncertainty predicts subsequent replay and replanning; (c) off-task hippocampus and anterior cingulate activity increase when revaluation is required. These findings elucidate how the brain leverages offline mechanisms in planning and goal-directed behavior under uncertainty.},
  keywords = {cognitive computational neuroscience,decision-making,Dyna,fMRI,hippocampus,learning and memory,memory,model-based learning,offline memory processes,planning,prediction error,prioritized replay,reinforcement learning,replay,representation learning,reward revaluation,To read,uncertainty},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IZK7KGSZ/Momennejad et al. - 2018 - Offline replay supports planning in human reinforc.pdf}
}

@report{momennejadPredictingFutureMultiscale2018,
  type = {preprint},
  title = {Predicting the {{Future}} with {{Multi-scale Successor Representations}}},
  author = {Momennejad, Ida and Howard, Marc W.},
  date = {2018-10-22},
  institution = {{Neuroscience}},
  doi = {10.1101/449470},
  url = {http://biorxiv.org/lookup/doi/10.1101/449470},
  urldate = {2021-12-07},
  abstract = {The successor representation (SR) is a candidate principle for generalization in reinforcement learning, computational accounts of memory, and the structure of neural representations in the hippocampus. Given a sequence of states, the SR learns a predictive representation for every given state that encodes how often, on average, each upcoming state is expected to be visited, even if it is multiple steps ahead. A discount or scale parameter determines how many steps into the future SR’s generalizations reach, enabling rapid value computation, subgoal discovery, and flexible decision-making in large trees. However, SR with a single scale could discard information for predicting both the sequential order of and the distance between states, which are common problems in navigation for animals and artificial agents. Here we propose a solution: an ensemble of SRs with multiple scales. We show that the derivative of multi-scale SR can reconstruct both the sequence of expected future states and estimate distance to goal. This derivative can be computed linearly: we show that a multi-scale SR ensemble is the Laplace transform of future states, and the inverse of this Laplace transform is a biologically plausible linear estimation of the derivative. Multi-scale SR and its derivative could lead to a common principle for how the medial temporal lobe supports both map-based and vector-based navigation.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BYWAU4RS/Momennejad and Howard - 2018 - Predicting the Future with Multi-scale Successor R.pdf}
}

@article{momennejadPredictingFutureMultiscale2018a,
  title = {Predicting the {{Future}} with {{Multi-scale Successor Representations}}},
  author = {Momennejad, Ida and Howard, Marc W.},
  date = {2018-10-22},
  pages = {449470},
  publisher = {{bioRxiv}},
  doi = {10.1101/449470},
  url = {https://www.biorxiv.org/content/10.1101/449470v1},
  urldate = {2022-02-18},
  abstract = {The successor representation (SR) is a candidate principle for generalization in reinforcement learning, computational accounts of memory, and the structure of neural representations in the hippocampus. Given a sequence of states, the SR learns a predictive representation for every given state that encodes how often, on average, each upcoming state is expected to be visited, even if it is multiple steps ahead. A discount or scale parameter determines how many steps into the future SR’s generalizations reach, enabling rapid value computation, subgoal discovery, and flexible decision-making in large trees. However, SR with a single scale could discard information for predicting both the sequential order of and the distance between states, which are common problems in navigation for animals and artificial agents. Here we propose a solution: an ensemble of SRs with multiple scales. We show that the derivative of multi-scale SR can reconstruct both the sequence of expected future states and estimate distance to goal. This derivative can be computed linearly: we show that a multi-scale SR ensemble is the Laplace transform of future states, and the inverse of this Laplace transform is a biologically plausible linear estimation of the derivative. Multi-scale SR and its derivative could lead to a common principle for how the medial temporal lobe supports both map-based and vector-based navigation.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/PPQNXTIV/Momennejad and Howard - 2018 - Predicting the Future with Multi-scale Successor R.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/UBWX8RX9/449470v1.html}
}

@article{moorePrioritizedSweepingReinforcement1993,
  title = {Prioritized Sweeping: {{Reinforcement}} Learning with Less Data and Less Time},
  shorttitle = {Prioritized Sweeping},
  author = {Moore, Andrew W. and Atkeson, Christopher G.},
  date = {1993-10},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {13},
  number = {1},
  pages = {103--130},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993104},
  url = {http://link.springer.com/10.1007/BF00993104},
  urldate = {2021-12-07},
  abstract = {We present a new algorithm,prioritized sweeping, for efficientprediction and control of stochastic Markovsystems.Incrementallearningmethodssuchas temporaldifferencingand Q-learninghavereal-timeperformance. Classicalmethodsare slower,but moreaccurate,becausetheymakefull use of the observations.Prioritized sweepingaims for the best of both worlds. It uses all previous experiencesboth to prioritize important dynamicprogrammingsweepsand to guidethe explorationof state-space. Wecompareprioritizedsweepingwith other reinforcementlearningschemesfor a numberofdifferentstochasticoptimalcontrolproblems.It successfully solves large state-space real-time problems with which other methods have difficulty.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3XSA2SVI/Moore and Atkeson - 1993 - Prioritized sweeping Reinforcement learning with .pdf}
}

@article{moorePrioritizedSweepingReinforcement1993a,
  title = {Prioritized Sweeping: {{Reinforcement}} Learning with Less Data and Less Time},
  shorttitle = {Prioritized Sweeping},
  author = {Moore, Andrew W. and Atkeson, Christopher G.},
  date = {1993-10},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {13},
  number = {1},
  pages = {103--130},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993104},
  url = {http://link.springer.com/10.1007/BF00993104},
  urldate = {2021-12-07},
  abstract = {We present a new algorithm,prioritized sweeping, for efficientprediction and control of stochastic Markovsystems.Incrementallearningmethodssuchas temporaldifferencingand Q-learninghavereal-timeperformance. Classicalmethodsare slower,but moreaccurate,becausetheymakefull use of the observations.Prioritized sweepingaims for the best of both worlds. It uses all previous experiencesboth to prioritize important dynamicprogrammingsweepsand to guidethe explorationof state-space. Wecompareprioritizedsweepingwith other reinforcementlearningschemesfor a numberofdifferentstochasticoptimalcontrolproblems.It successfully solves large state-space real-time problems with which other methods have difficulty.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/44RFQE5Y/Moore and Atkeson - 1993 - Prioritized sweeping Reinforcement learning with .pdf}
}

@inproceedings{morimuraNonparametricReturnDistribution2019,
  title = {Nonparametric {{Return Distribution Approximation}} for {{Reinforcement Learning}}},
  author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
  date = {2019-07-16},
  url = {https://openreview.net/forum?id=HybsNibObS},
  urldate = {2022-07-26},
  abstract = {Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected...},
  eventtitle = {{{ICML}}},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/93GIZRR4/Morimura et al. - 2019 - Nonparametric Return Distribution Approximation fo.pdf}
}

@unpublished{moskovitzFirstOccupancyRepresentationReinforcement2021,
  title = {A {{First-Occupancy Representation}} for {{Reinforcement Learning}}},
  author = {Moskovitz, Ted and Wilson, Spencer R. and Sahani, Maneesh},
  date = {2021-11-06},
  eprint = {2109.13863},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2109.13863},
  urldate = {2022-02-18},
  abstract = {Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states. The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity. However, in the real world, rewards may move or only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span. To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed. We demonstrate that the FR facilitates exploration, the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7C3YNXA2/Moskovitz et al. - 2021 - A First-Occupancy Representation for Reinforcement.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/DZ4UN8VS/2109.html}
}

@unpublished{moskovitzTacticalOptimismPessimism2021,
  title = {Tactical {{Optimism}} and {{Pessimism}} for {{Deep Reinforcement Learning}}},
  author = {Moskovitz, Ted and Parker-Holder, Jack and Pacchiano, Aldo and Arbel, Michael and Jordan, Michael I.},
  date = {2021-05-31},
  eprint = {2102.03765},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.03765},
  urldate = {2021-12-07},
  abstract = {In recent years, deep off-policy actor-critic algorithms have become a dominant approach to reinforcement learning for continuous control. This comes after a series of breakthroughs to address function approximation errors, which previously led to poor performance. These insights encourage the use of pessimistic value updates. However, this discourages exploration and runs counter to theoretical support for the efficacy of optimism in the face of uncertainty. So which approach is best? In this work, we show that the optimal degree of optimism can vary both across tasks and over the course of learning. Inspired by this insight, we introduce a novel deep actor-critic algorithm, Dynamic Optimistic and Pessimistic Estimation (DOPE) to switch between optimistic and pessimistic value learning online by formulating the selection as a multi-arm bandit problem. We show in a series of challenging continuous control tasks that DOPE outperforms existing state-ofthe-art methods, which rely on a fixed degree of optimism. Since our changes are simple to implement, we believe these insights can be extended to a number of off-policy algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XTHYU6AL/Moskovitz et al. - 2021 - Tactical Optimism and Pessimism for Deep Reinforce.pdf}
}

@article{motiwalaEfficientCodingCognitive2022,
  title = {Efficient Coding of Cognitive Variables Underlies Dopamine Response and Choice Behavior},
  author = {Motiwala, Asma and Soares, Sofia and Atallah, Bassam V. and Paton, Joseph J. and Machens, Christian K.},
  date = {2022-06},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {25},
  number = {6},
  pages = {738--748},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01085-7},
  url = {https://www.nature.com/articles/s41593-022-01085-7},
  urldate = {2022-07-12},
  abstract = {Reward expectations based on internal knowledge of the external environment are a core component of adaptive behavior. However, internal knowledge may be inaccurate or incomplete due to errors in sensory measurements. Some features of the environment may also be encoded inaccurately to minimize representational costs associated with their processing. In this study, we investigated how reward expectations are affected by features of internal representations by studying behavior and dopaminergic activity while mice make time-based decisions. We show that several possible representations allow a reinforcement learning agent to model animals’ overall performance during the task. However, only a small subset of highly compressed representations simultaneously reproduced the co-variability in animals’ choice behavior and dopaminergic activity. Strikingly, these representations predict an unusual distribution of response times that closely match animals’ behavior. These results inform how constraints of representational efficiency may be expressed in encoding representations of dynamic cognitive variables used for reward-based computations.},
  issue = {6},
  langid = {english},
  keywords = {Computational neuroscience,Learning algorithms,Neural encoding,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZWIBND5E/Motiwala et al. - 2022 - Efficient coding of cognitive variables underlies .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/2J62TJH6/s41593-022-01085-7.html}
}

@article{muessigCoordinatedEmergenceHippocampal2019,
  title = {Coordinated {{Emergence}} of {{Hippocampal Replay}} and {{Theta Sequences}} during {{Post-natal Development}}},
  author = {Muessig, Laurenz and Lasek, Michal and Varsavsky, Isabella and Cacucci, Francesca and Wills, Thomas Joseph},
  date = {2019-03-04},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {29},
  number = {5},
  pages = {834-840.e4},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2019.01.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982219300065},
  urldate = {2022-08-12},
  abstract = {Hippocampal place cells encode an animal’s current position in space during exploration [1]. During sleep, hippocampal network activity recapitulates patterns observed during recent experience: place cells with overlapping spatial fields show a greater tendency to co-fire (“reactivation”) [2], and temporally ordered and compressed sequences of place cell firing observed during wakefulness are reinstated (“replay”) [3, 4, 5]. Reactivation and replay may underlie memory consolidation [6, 7, 8, 9, 10]. Compressed sequences of place cell firing also occur during exploration: during each cycle of the theta oscillation, the set of active place cells shifts from those signaling positions behind to those signaling positions ahead of an animal’s current location [11, 12]. These “theta sequences” have been linked to spatial planning [13]. Here, we demonstrate that, before weaning (post-natal day [P]21), offline place cell activity associated with sharp-wave ripples (SWRs) reflects predominantly stationary locations in~recently visited environments. By contrast, sequential place cell firing, describing extended trajectories through space during exploration (theta sequences) and subsequent rest (replay), emerge gradually after~weaning in a coordinated fashion, possibly due~to a progressive decrease in the threshold for experience-driven plasticity. Hippocampus-dependent learning and memory emerge late in altricial mammals [14, 15, 16, 17], appearing around weaning in rats and slowly maturing thereafter [14, 15]. In contrast, spatially localized firing is observed 1~week earlier (with reduced spatial tuning and stability) [18, 19, 20, 21]. By examining the development of hippocampal reactivation, replay, and theta sequences, we show that the coordinated maturation of offline consolidation and online sequence generation parallels the late emergence of hippocampal memory in the rat.},
  langid = {english},
  keywords = {consolidation,development,hippocampus,memory,place cell,reactivation,replay,sleep,theta,theta sequence},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3WSP5N87/Muessig et al. - 2019 - Coordinated Emergence of Hippocampal Replay and Th.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/HZ7ZGTKP/S0960982219300065.html}
}

@report{muldersStructuredScaffoldUnderlies2021,
  type = {preprint},
  title = {A Structured Scaffold Underlies Activity in the Hippocampus},
  author = {Mulders, Dounia and Yim, Man Yi and Lee, Jae Sung and Lee, Albert K. and Taillefumier, Thibaud and Fiete, Ila R.},
  date = {2021-11-22},
  institution = {{Neuroscience}},
  doi = {10.1101/2021.11.20.469406},
  url = {http://biorxiv.org/lookup/doi/10.1101/2021.11.20.469406},
  urldate = {2021-12-07},
  abstract = {Place cells are believed to organize memory across space and time, inspiring the idea of the cognitive map. Yet unlike the structured activity in the associated grid and head-direction cells, they remain an enigma: their responses have been difficult to predict and are complex enough to be statistically well-described by a random process. Here we report one step toward the ultimate goal of understanding place cells well enough to predict their fields. Within a theoretical framework in which place fields are derived as a conjunction of external cues with internal grid cell inputs, we predict that even apparently random place cell responses should reflect the structure of their grid inputs and that this structure can be unmasked if probed in sufficiently large neural populations and large environments. To test the theory, we design experiments in long, locally featureless spaces to demonstrate that structured scaffolds undergird place cell responses. Our findings, together with other theoretical and experimental results, suggest that place cells build memories of external inputs by attaching them to a largely prespecified grid scaffold.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/A3Z78SEE/Mulders et al. - 2021 - A structured scaffold underlies activity in the hi.pdf}
}

@article{muldersStructuredScaffoldUnderlies2021a,
  title = {A Structured Scaffold Underlies Activity in the Hippocampus},
  author = {Mulders, Dounia and Yim, Man Yi and Lee, Jae Sung and Lee, Albert K. and Taillefumier, Thibaud and Fiete, Ila R.},
  date = {2021-11-22},
  pages = {2021.11.20.469406},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.11.20.469406},
  url = {https://www.biorxiv.org/content/10.1101/2021.11.20.469406v1},
  urldate = {2022-01-27},
  abstract = {Place cells are believed to organize memory across space and time, inspiring the idea of the cognitive map. Yet unlike the structured activity in the associated grid and head-direction cells, they remain an enigma: their responses have been difficult to predict and are complex enough to be statistically well-described by a random process. Here we report one step toward the ultimate goal of understanding place cells well enough to predict their fields. Within a theoretical framework in which place fields are derived as a conjunction of external cues with internal grid cell inputs, we predict that even apparently random place cell responses should reflect the structure of their grid inputs and that this structure can be unmasked if probed in sufficiently large neural populations and large environments. To test the theory, we design experiments in long, locally featureless spaces to demonstrate that structured scaffolds undergird place cell responses. Our findings, together with other theoretical and experimental results, suggest that place cells build memories of external inputs by attaching them to a largely prespecified grid scaffold.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LYNVNYXX/Mulders et al. - 2021 - A structured scaffold underlies activity in the hi.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/X5PRGRV5/2021.11.20.469406v1.html}
}

@article{mullerHippocampusCognitiveGraph1996,
  title = {The Hippocampus as a Cognitive Graph.},
  author = {Muller, R U and Stead, M and Pach, J},
  date = {1996-06-01},
  journaltitle = {Journal of General Physiology},
  shortjournal = {Journal of General Physiology},
  volume = {107},
  number = {6},
  pages = {663--694},
  issn = {0022-1295},
  doi = {10.1085/jgp.107.6.663},
  url = {https://doi.org/10.1085/jgp.107.6.663},
  urldate = {2022-10-12},
  abstract = {A theory of cognitive mapping is developed that depends only on accepted properties of hippocampal function, namely, long-term potentiation, the place cell phenomenon, and the associative or recurrent connections made among CA3 pyramidal cells. It is proposed that the distance between the firing fields of connected pairs of CA3 place cells is encoded as synaptic resistance (reciprocal synaptic strength). The encoding occurs because pairs of cells with coincident or overlapping fields will tend to fire together in time, thereby causing a decrease in synaptic resistance via long-term potentiation; in contrast, cells with widely separated fields will tend never to fire together, causing no change or perhaps (via long-term depression) an increase in synaptic resistance. A network whose connection pattern mimics that of CA3 and whose connection weights are proportional to synaptic resistance can be formally treated as a weighted, directed graph. In such a graph, a "node" is assigned to each CA3 cell and two nodes are connected by a "directed edge" if and only if the two corresponding cells are connected by a synapse. Weighted, directed graphs can be searched for an optimal path between any pair of nodes with standard algorithms. Here, we are interested in finding the path along which the sum of the synaptic resistances from one cell to another is minimal. Since each cell is a place cell, such a path also corresponds to a path in two-dimensional space. Our basic finding is that minimizing the sum of the synaptic resistances along a path in neural space yields the shortest (optimal) path in unobstructed two-dimensional space, so long as the connectivity of the network is great enough. In addition to being able to find geodesics in unobstructed space, the same network enables solutions to the "detour" and "shortcut" problems, in which it is necessary to find an optimal path around a newly introduced barrier and to take a shorter path through a hole opened up in a preexisting barrier, respectively. We argue that the ability to solve such problems qualifies the proposed hippocampal object as a cognitive map. Graph theory thus provides a sort of existence proof demonstrating that the hippocampus contains the necessary information to function as a map, in the sense postulated by others (O'Keefe, J., and L. Nadel. 1978. The Hippocampus as a Cognitive Map. Clarendon Press, Oxford, UK). It is also possible that the cognitive mapping functions of the hippocampus are carried out by parallel graph searching algorithms implemented as neural processes. This possibility has the great attraction that the hippocampus could then operate in much the same way to find paths in general problem space; it would only be necessary for pyramidal cells to exhibit a strong nonpositional firing correlate.},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/YWZFIYLZ/Muller et al. - 1996 - The hippocampus as a cognitive graph..pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/IIHV7PRM/The-hippocampus-as-a-cognitive-graph.html}
}

@article{mullerQuarterCenturyPlace1996,
  title = {A {{Quarter}} of a {{Century}} of {{Place Cells}}},
  author = {Muller, Robert},
  date = {1996-11},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {17},
  number = {5},
  pages = {813--822},
  issn = {08966273},
  doi = {10.1016/S0896-6273(00)80214-7},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627300802147},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AHIUC9CL/Muller - 1996 - A Quarter of a Century of Place Cells.pdf}
}

@online{NavigatingCognitionSpatial2018,
  title = {Navigating Cognition: {{Spatial}} Codes for Human Thinking},
  shorttitle = {Navigating Cognition},
  date = {2018},
  doi = {10.1126/science.aat6766},
  url = {https://www.science.org/doi/10.1126/science.aat6766},
  urldate = {2022-01-13},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/PZE5M6K2/Navigating cognition Spatial codes for human thin.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/4PC8M62F/science.html}
}

@article{neuUnifyingViewOptimism,
  title = {A {{Unifying View}} of {{Optimism}} in {{Episodic Reinforcement Learning}}},
  author = {Neu, Gergely and Pike-Burke, Ciara},
  pages = {12},
  abstract = {The principle of “optimism in the face of uncertainty” underpins many theoretically successful reinforcement learning algorithms. In this paper we provide a general framework for designing, analyzing and implementing such algorithms in the episodic reinforcement learning problem. This framework is built upon Lagrangian duality, and demonstrates that every model-optimistic algorithm that constructs an optimistic MDP has an equivalent representation as a value-optimistic dynamic programming algorithm. Typically, it was thought that these two classes of algorithms were distinct, with model-optimistic algorithms benefiting from a cleaner probabilistic analysis while value-optimistic algorithms are easier to implement and thus more practical. With the framework developed in this paper, we show that it is possible to get the best of both worlds by providing a class of algorithms which have a computationally efficient dynamic-programming implementation and also a simple probabilistic analysis. Besides being able to capture many existing algorithms in the tabular setting, our framework can also address large-scale problems under realizable function approximation, where it enables a simple model-based analysis of some recently proposed methods.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZUKBXHMX/Neu and Pike-Burke - A Unifying View of Optimism in Episodic Reinforcem.pdf}
}

@article{niehGeometryAbstractLearned2021,
  title = {Geometry of Abstract Learned Knowledge in the Hippocampus},
  author = {Nieh, Edward H. and Schottdorf, Manuel and Freeman, Nicolas W. and Low, Ryan J. and Lewallen, Sam and Koay, Sue Ann and Pinto, Lucas and Gauthier, Jeffrey L. and Brody, Carlos D. and Tank, David W.},
  date = {2021-07-01},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {595},
  number = {7865},
  pages = {80--84},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03652-7},
  url = {http://www.nature.com/articles/s41586-021-03652-7},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3FLVNFN3/Nieh et al. - 2021 - Geometry of abstract learned knowledge in the hipp.pdf}
}

@online{nilsnybergSpatialGoalCoding2021,
  title = {Spatial Goal Coding in the Hippocampal Formation | {{Elsevier Enhanced Reader}}},
  author = {{Nils Nyberg} and {Eleonore Duvelle} and {Caswell Barry} and {Hugo Spiers}},
  date = {2021},
  doi = {10.1016/j.neuron.2021.12.012},
  url = {https://reader.elsevier.com/reader/sd/pii/S0896627321010291?token=F9228A98802CB6CAC4C1484EAADC213C7023BC75B522C87C4A6BD22E50A1A5C7A3B2FF9F6636228F56A976CCB2A4D6FA&originRegion=eu-west-1&originCreation=20220118154545},
  urldate = {2022-01-18},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UCMHQ9JK/Spatial goal coding in the hippocampal formation .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/PEC3MPG9/S0896627321010291.html}
}

@article{nivTonicDopamineOpportunity2007,
  title = {Tonic Dopamine: Opportunity Costs and the Control of Response Vigor},
  shorttitle = {Tonic Dopamine},
  author = {Niv, Yael and Daw, Nathaniel D. and Joel, Daphna and Dayan, Peter},
  date = {2007-04-01},
  journaltitle = {Psychopharmacology},
  shortjournal = {Psychopharmacology},
  volume = {191},
  number = {3},
  pages = {507--520},
  issn = {1432-2072},
  doi = {10.1007/s00213-006-0502-4},
  url = {https://doi.org/10.1007/s00213-006-0502-4},
  urldate = {2022-02-03},
  abstract = {Dopamine neurotransmission has long been known to exert a powerful influence over the vigor, strength, or rate of responding. However, there exists no clear understanding of the computational foundation for this effect; predominant accounts of dopamine’s computational function focus on a role for phasic dopamine in controlling the discrete selection between different actions and have nothing to say about response vigor or indeed the free-operant tasks in which it is typically measured.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/F9CMSI4U/Niv et al. - 2007 - Tonic dopamine opportunity costs and the control .pdf}
}

@unpublished{odonoghueMakingSenseReinforcement2020,
  title = {Making {{Sense}} of {{Reinforcement Learning}} and {{Probabilistic Inference}}},
  author = {O'Donoghue, Brendan and Osband, Ian and Ionescu, Catalin},
  date = {2020-11-04},
  eprint = {2001.00805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2001.00805},
  urldate = {2021-12-07},
  abstract = {Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts ‘RL as inference’ and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular ‘RL as inference’ approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7REIDXK5/O'Donoghue et al. - 2020 - Making Sense of Reinforcement Learning and Probabi.pdf}
}

@article{odonoghueUncertaintyBellmanEquation,
  title = {The {{Uncertainty Bellman Equation}} and {{Exploration}}},
  author = {O’Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
  pages = {10},
  abstract = {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for -greedy improves DQN performance on 51 out of 57 games in the Atari suite.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7RE5VZDT/O’Donoghue et al. - The Uncertainty Bellman Equation and Exploration.pdf}
}

@book{okeefeHippocampusCognitiveMap1978,
  title = {The Hippocampus as a Cognitive Map},
  author = {O'Keefe, John and Nadel, Lynn},
  date = {1978},
  publisher = {{Clarendon Press ; Oxford University Press}},
  location = {{Oxford : New York}},
  isbn = {978-0-19-857206-0},
  langid = {english},
  pagetotal = {570},
  keywords = {Cognition,Hippocampus (Brain),Memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/4XKJNZDL/O'Keefe and Nadel - 1978 - The hippocampus as a cognitive map.pdf}
}

@article{okeefeHippocampusSpatialMap1971,
  title = {The Hippocampus as a Spatial Map. {{Preliminary}} Evidence from Unit Activity in the Freely-Moving Rat},
  author = {O'Keefe, J. and Dostrovsky, J.},
  date = {1971-11},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  volume = {34},
  number = {1},
  pages = {171--175},
  issn = {00068993},
  doi = {10.1016/0006-8993(71)90358-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0006899371903581},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZA8UJ7YU/O'Keefe and Dostrovsky - 1971 - The hippocampus as a spatial map. Preliminary evid.pdf}
}

@article{olafsdottirHippocampalPlaceCells2015,
  title = {Hippocampal Place Cells Construct Reward Related Sequences through Unexplored Space},
  author = {Ólafsdóttir, H Freyja and Barry, Caswell and Saleem, Aman B and Hassabis, Demis and Spiers, Hugo J},
  date = {2015-06-26},
  journaltitle = {eLife},
  volume = {4},
  pages = {e06063},
  issn = {2050-084X},
  doi = {10.7554/eLife.06063},
  url = {https://elifesciences.org/articles/06063},
  urldate = {2021-12-07},
  abstract = {Dominant theories of hippocampal function propose that place cell representations are formed during an animal's first encounter with a novel environment and are subsequently replayed during off-line states to support consolidation and future behaviour. Here we report that viewing the delivery of food to an unvisited portion of an environment leads to off-line pre-activation of place cells sequences corresponding to that space. Such ‘preplay’ was not observed for an unrewarded but otherwise similar portion of the environment. These results suggest that a hippocampal representation of a visible, yet unexplored environment can be formed if the environment is of motivational relevance to the animal. We hypothesise such goal-biased preplay may support preparation for future experiences in novel environments.           ,              As an animal explores an area, part of the brain called the hippocampus creates a mental map of the space. When the animal is in one location, a few neurons called ‘place cells’ will fire. If the animal moves to a new spot, other place cells fire instead. Each time the animal returns to that spot, the same place cells will fire. Thus, as the animal moves, a place-specific pattern of firing emerges that scientists can view by recording the cells' activity and which can be used to reconstruct the animal's position.             After exploring a space, the hippocampus may replay the new place-specific pattern of activity during sleep. By doing so, the brain consolidates the memory of the space for return visits. Recent evidence now suggests that these mental rehearsals—or internal simulations of the space—may begin even before a new space has been explored.             Now, Ólafsdóttir, Barry et al. report that whether an animal's brain simulates a first visit to a new space depends on whether the animal anticipates a reward. In the experiments, rats were allowed to run up to the junction in a T-shaped track. The animals could see into each of the arms, but not enter them. Food was then placed in one of the inaccessible arms. Ólafsdóttir, Barry et al. recorded the firing of place cells in the brain of the animals when they were on the track and during a rest period afterwards. The rats were then allowed onto the inaccessible arms, and again their brain activity was recorded.             In the rest period after the rats first viewed the inaccessible arms, the place cell pattern that would later form the mental map of a journey to and from the food-containing arm was pre-activated. However, the place cell pattern that would become the mental map of the other inaccessible arm was not activated before the rat explored that area. Therefore, Ólafsdóttir, Barry et al. suggest that the perception of reward influences which place cell pattern is simulated during rest. An implication of these findings is that the brain preferentially simulates past or future experiences that are deemed to be functionally significant, such as those associated with reward. A future challenge will be to determine whether this goal-related simulation of unvisited spaces predicts and is needed for behaviour such as successful navigation to a goal.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KW39L6NQ/Ólafsdóttir et al. - 2015 - Hippocampal place cells construct reward related s.pdf}
}

@article{oneillSuperficialLayersMedial2017,
  title = {Superficial Layers of the Medial Entorhinal Cortex Replay Independently of the Hippocampus},
  author = {O’Neill, J. and Boccara, C. N. and Stella, F. and Schoenenberger, P. and Csicsvari, J.},
  date = {2017-01-13},
  journaltitle = {Science},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aag2787},
  url = {https://www.science.org/doi/abs/10.1126/science.aag2787},
  urldate = {2022-01-11},
  abstract = {The medial entorhinal cortex can replay waking experiences independently of the hippocampus.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/Y9XFR4K9/O’Neill et al. - 2017 - Superficial layers of the medial entorhinal cortex.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/7W3D9IS4/science.html}
}

@unpublished{osbandDeepExplorationRandomized2019,
  title = {Deep {{Exploration}} via {{Randomized Value Functions}}},
  author = {Osband, Ian and Van Roy, Benjamin and Russo, Daniel and Wen, Zheng},
  date = {2019-09-23},
  eprint = {1703.07608},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.07608},
  urldate = {2021-12-07},
  abstract = {We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FLS79E3U/Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf}
}

@unpublished{osbandMoreEfficientReinforcement2013,
  title = {({{More}}) {{Efficient Reinforcement Learning}} via {{Posterior Sampling}}},
  author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  date = {2013-12-26},
  eprint = {1306.0940},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1306.0940},
  urldate = {2021-12-07},
  abstract = {Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an \$\textbackslash tilde\{O\}(\textbackslash tau S \textbackslash sqrt\{AT\})\$ bound on the expected regret, where \$T\$ is time, \$\textbackslash tau\$ is the episode length and \$S\$ and \$A\$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KQMYZAUP/Osband et al. - 2013 - (More) Efficient Reinforcement Learning via Poster.pdf}
}

@article{padoa-schioppaNeuronsOrbitofrontalCortex2006,
  title = {Neurons in the Orbitofrontal Cortex Encode Economic Value},
  author = {Padoa-Schioppa, Camillo and Assad, John A.},
  date = {2006-05},
  journaltitle = {Nature},
  volume = {441},
  number = {7090},
  pages = {223--226},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature04676},
  url = {https://www.nature.com/articles/nature04676},
  urldate = {2022-08-07},
  abstract = {In the study of animal behaviour, an ‘economic choice’ involves an individual animal's selection between many options based on a subjective estimation of the benefits. It has long been known that neurons in different parts of the brain respond to separate attributes, such as quantity, colour and taste. And now part of the brain, the orbitofrontal cortex (OFC) has been linked to the value judgements involved in economic choice. This was established in tests on macaque monkeys choosing whether to drink water or various types of juice. Neurons in the OFC changed their firing rate in a way that reflected the monkey's valuation of the drink. In humans, lesions in the orbitofrontal cortex are known to result in eating disorders, compulsive gambling and other conditions involving ‘choice deficit’; there is also a link to drug abuse, arguably another aspect of choice.},
  issue = {7090},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XZVZ6WXD/Padoa-Schioppa and Assad - 2006 - Neurons in the orbitofrontal cortex encode economi.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/3GUYEB9F/nature04676.html}
}

@online{panahiGenerativeModelsBrain,
  title = {Generative {{Models}} of {{Brain Dynamics}} -- {{A}} Review},
  author = {Panahi, Mahta and Abrevaya, German and Gagnon-Audet, Jean-Christophe and Voleti, Vikram and Rish, Irina and Dumas, Guillaume},
  url = {https://arxiv.org/abs/2112.12147},
  urldate = {2022-01-25},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BAJARTVN/Panahi et al. - Generative Models of Brain Dynamics -- A review.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/36XDQUSK/2112.html}
}

@article{panzeriStructuresFunctionsCorrelations2022,
  title = {The Structures and Functions of Correlations in Neural Population Codes},
  author = {Panzeri, Stefano and Moroni, Monica and Safaai, Houman and Harvey, Christopher D.},
  date = {2022-06-22},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  pages = {1--17},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00606-4},
  url = {https://www.nature.com/articles/s41583-022-00606-4},
  urldate = {2022-06-30},
  abstract = {The collective activity of a population of neurons, beyond the properties of individual cells, is crucial for many brain functions. A fundamental question is how activity correlations between neurons affect how neural populations process information. Over the past 30 years, major progress has been made on how the levels and structures of correlations shape the encoding of information in population codes. Correlations influence population coding through the organization of pairwise-activity correlations with respect to the similarity of tuning of individual neurons, by their stimulus modulation and by the presence of higher-order correlations. Recent work has shown that correlations also profoundly shape other important functions performed by neural populations, including generating codes across multiple timescales and facilitating information transmission to, and readout by, downstream brain areas to guide behaviour. Here, we review this recent work and discuss how the structures of correlations can have opposite effects on the different functions of neural populations, thus creating trade-offs and constraints for the structure–function relationships of population codes. Further, we present ideas on how to combine large-scale simultaneous recordings of neural populations, computational models, analyses of behaviour, optogenetics and anatomy to unravel how the structures of correlations might be optimized to serve multiple functions.},
  langid = {english},
  keywords = {Neural decoding,Sensory processing},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LFW9EBCD/Panzeri et al. - 2022 - The structures and functions of correlations in ne.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/DHUN5N7N/s41583-022-00606-4.html}
}

@article{pavlidesInfluencesHippocampalPlace1989,
  title = {Influences of Hippocampal Place Cell Firing in the Awake State on the Activity of These Cells during Subsequent Sleep Episodes},
  author = {Pavlides, C and Winson, J},
  date = {1989-08-01},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {9},
  number = {8},
  pages = {2907--2918},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.09-08-02907.1989},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.09-08-02907.1989},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/87UDNKFI/Pavlides and Winson - 1989 - Influences of hippocampal place cell firing in the.pdf}
}

@article{pettitHippocampalPlaceCodes2022,
  title = {Hippocampal Place Codes Are Gated by Behavioral Engagement},
  author = {Pettit, Noah L. and Yuan, Xintong C. and Harvey, Christopher D.},
  date = {2022-05},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {25},
  number = {5},
  pages = {561--566},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01050-4},
  url = {https://www.nature.com/articles/s41593-022-01050-4},
  urldate = {2022-09-05},
  abstract = {As animals explore an environment, the hippocampus is thought to automatically form and maintain a place code by combining sensory and self-motion signals. Instead, we observed an extensive degradation of the place code when mice voluntarily disengaged from a virtual navigation task, remarkably even as they continued to traverse the identical environment. Internal states, therefore, can strongly gate spatial maps and reorganize hippocampal activity even without sensory and self-motion changes.},
  issue = {5},
  langid = {english},
  keywords = {Hippocampus,Navigation},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TSHY3JD8/Pettit et al. - 2022 - Hippocampal place codes are gated by behavioral en.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/MMAMJ9M5/s41593-022-01050-4.html}
}

@misc{peyracheQueryingHippocampalReplay2022,
  title = {Querying Hippocampal Replay with Subcortical Inputs},
  author = {Peyrache, Adrien},
  date = {2022-05-05},
  number = {arXiv:2205.02665},
  eprint = {2205.02665},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.02665},
  url = {http://arxiv.org/abs/2205.02665},
  urldate = {2022-09-23},
  abstract = {During sleep, the hippocampus recapitulates neuronal patterns corresponding to behavioral trajectories during previous experiences. This hippocampal replay supports the formation of long-term memories. Yet, whether replay originates within the hippocampal circuitry or is initiated by extrahippocampal inputs is unknown. Here, I review recent findings regarding the organization of neuronal activity upstream to the hippocampus, in the head-direction (HD) and grid cell networks. I argue that hippocampal activity is under the influence of primary spatial signals, which originate from subcortical structures and set the stage for memory replay. In turn, hippocampal replay resets the HD network activity to select a new direction for the next replay event. This reciprocal interaction between the HD network and the hippocampus may be essential in providing meaning to hippocampal activity, specifically by training decoders of hippocampal sequences. Neuronal dynamics in thalamo-hippocampal loops may thus be instrumental for memory processes during sleep.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IKHNQ7FQ/Peyrache - 2022 - Querying hippocampal replay with subcortical input.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/SK3PG995/2205.html}
}

@article{pfeifferAutoassociativeDynamicsGeneration2015,
  title = {Autoassociative Dynamics in the Generation of Sequences of Hippocampal Place Cells},
  author = {Pfeiffer, Brad E. and Foster, David J.},
  date = {2015-07-10},
  journaltitle = {Science},
  volume = {349},
  number = {6244},
  pages = {180--183},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aaa9633},
  url = {https://www.science.org/doi/full/10.1126/science.aaa9633},
  urldate = {2022-04-12},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/2WMIWSLC/Pfeiffer and Foster - 2015 - Autoassociative dynamics in the generation of sequ.pdf}
}

@article{pfeifferHippocampalPlacecellSequences2013,
  title = {Hippocampal Place-Cell Sequences Depict Future Paths to Remembered Goals},
  author = {Pfeiffer, Brad E. and Foster, David J.},
  date = {2013-05},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {497},
  number = {7447},
  pages = {74--79},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12112},
  url = {http://www.nature.com/articles/nature12112},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/Q4ZQZHUY/Pfeiffer and Foster - 2013 - Hippocampal place-cell sequences depict future pat.pdf}
}

@article{pfeifferSpatialLearningDrives2022,
  title = {Spatial {{Learning Drives Rapid Goal Representation}} in {{Hippocampal Ripples}} without {{Place Field Accumulation}} or {{Goal-Oriented Theta Sequences}}},
  author = {Pfeiffer, Brad E.},
  date = {2022-05-11},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {42},
  number = {19},
  eprint = {35396328},
  eprinttype = {pmid},
  pages = {3975--3988},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2479-21.2022},
  url = {https://www.jneurosci.org/content/42/19/3975},
  urldate = {2022-05-12},
  abstract = {The hippocampus is critical for rapid acquisition of many forms of memory, although the circuit-level mechanisms through which the hippocampus rapidly consolidates novel information are unknown. Here, the activity of large ensembles of hippocampal neurons in adult male Long-Evans rats was monitored across a period of rapid spatial learning to assess how the network changes during the initial phases of memory formation and retrieval. In contrast to several reports, the hippocampal network did not display enhanced representation of the goal location via accumulation of place fields or elevated firing rates at the goal. Rather, population activity rates increased globally as a function of experience. These alterations in activity were mirrored in the power of the theta oscillation and in the quality of theta sequences, without preferential encoding of paths to the learned goal location. In contrast, during brief “offline” pauses in movement, representation of a novel goal location emerged rapidly in ripples, preceding other changes in network activity. These data demonstrate that the hippocampal network can facilitate active navigation without enhanced goal representation during periods of active movement, and further indicate that goal representation in hippocampal ripples before movement onset supports subsequent navigation, possibly through activation of downstream cortical networks. SIGNIFICANCE STATEMENT Understanding the mechanisms through which the networks of the brain rapidly assimilate information and use previously learned knowledge are fundamental areas of focus in neuroscience. In particular, the hippocampal circuit is a critical region for rapid formation and use of spatial memory. In this study, several circuit-level features of hippocampal function were quantified while rats performed a spatial navigation task requiring rapid memory formation and use. During periods of active navigation, a general increase in overall network activity is observed during memory acquisition, which plateaus during memory retrieval periods, without specific enhanced representation of the goal location. During pauses in navigation, rapid representation of the distant goal well emerges before either behavioral improvement or changes in online activity.},
  langid = {english},
  keywords = {hippocampus,place cell,replay,sharp-wave/ripple,spatial memory,theta oscillation,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/GIIHJDGZ/Pfeiffer - 2022 - Spatial Learning Drives Rapid Goal Representation .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/W5NZGF4T/3975.html}
}

@article{pirayLinearReinforcementLearning2021,
  title = {Linear Reinforcement Learning in Planning, Grid Fields, and Cognitive Control},
  author = {Piray, Payam and Daw, Nathaniel D.},
  date = {2021-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {4942},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25123-3},
  url = {https://www.nature.com/articles/s41467-021-25123-3},
  urldate = {2021-12-07},
  abstract = {Abstract             It is thought that the brain’s judicious reuse of previous computation underlies our ability to plan flexibly, but also that inappropriate reuse gives rise to inflexibilities like habits and compulsion. Yet we lack a complete, realistic account of either. Building on control engineering, here we introduce a model for decision making in the brain that reuses a temporally abstracted map of future events to enable biologically-realistic, flexible choice at the expense of specific, quantifiable biases. It replaces the classic nonlinear, model-based optimization with a linear approximation that softly maximizes around (and is weakly biased toward) a default policy. This solution demonstrates connections between seemingly disparate phenomena across behavioral neuroscience, notably flexible replanning with biases and cognitive control. It also provides insight into how the brain can represent maps of long-distance contingencies stably and componentially, as in entorhinal response fields, and exploit them to guide choice even under changing goals.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/MQ6LC5K6/Piray and Daw - 2021 - Linear reinforcement learning in planning, grid fi.pdf}
}

@article{plittExperiencedependentContextualCodes2021,
  title = {Experience-Dependent Contextual Codes in the Hippocampus},
  author = {Plitt, Mark H. and Giocomo, Lisa M.},
  date = {2021-05},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {24},
  number = {5},
  pages = {705--714},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-021-00816-6},
  url = {http://www.nature.com/articles/s41593-021-00816-6},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6J39DX2W/Plitt and Giocomo - 2021 - Experience-dependent contextual codes in the hippo.pdf}
}

@article{poldrackCanCognitiveProcesses2006,
  title = {Can Cognitive Processes Be Inferred from Neuroimaging Data?},
  author = {Poldrack, Russell A.},
  date = {2006-02-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {2},
  pages = {59--63},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2005.12.004},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661305003360},
  urldate = {2022-06-30},
  abstract = {There is much interest currently in using functional neuroimaging techniques to understand better the nature of cognition. One particular practice that has become common is ‘reverse inference’, by which the engagement of a particular cognitive process is inferred from the activation of a particular brain region. Such inferences are not deductively valid, but can still provide some information. Using a Bayesian analysis of the BrainMap neuroimaging database, I characterize the amount of additional evidence in favor of the engagement of a cognitive process that can be offered by a reverse inference. Its usefulness is particularly limited by the selectivity of activation in the region of interest. I argue that cognitive neuroscientists should be circumspect in the use of reverse inference, particularly when selectivity of the region in question cannot be established or is known to be weak.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/RB8R8SI6/Poldrack - 2006 - Can cognitive processes be inferred from neuroimag.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/F4XNQLF6/S1364661305003360.html}
}

@inproceedings{poupartAnalyticSolutionDiscrete2006,
  title = {An Analytic Solution to Discrete {{Bayesian}} Reinforcement Learning},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Poupart, Pascal and Vlassis, Nikos and Hoey, Jesse and Regan, Kevin},
  date = {2006},
  pages = {697--704},
  publisher = {{ACM Press}},
  location = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143932},
  url = {http://portal.acm.org/citation.cfm?doid=1143844.1143932},
  urldate = {2021-12-07},
  abstract = {Reinforcement learning (RL) was originally proposed as a framework to allow agents to learn in an online fashion as they interact with their environment. Existing RL algorithms come short of achieving this goal because the amount of exploration required is often too costly and/or too time consuming for online learning. As a result, RL is mostly used for offline learning in simulated environments. We propose a new algorithm, called BEETLE, for effective online learning that is computationally efficient while minimizing the amount of exploration. We take a Bayesian model-based approach, framing RL as a partially observable Markov decision process. Our two main contributions are the analytical derivation that the optimal value function is the upper envelope of a set of multivariate polynomials, and an efficient pointbased value iteration algorithm that exploits this simple parameterization.},
  eventtitle = {The 23rd International Conference},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EY2MQPD4/Poupart et al. - 2006 - An analytic solution to discrete Bayesian reinforc.pdf}
}

@book{ProbabilisticMachineLearning,
  title = {Probabilistic {{Machine Learning}}: {{An Introduction}}},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JPRIMG6Q/Probabilistic Machine Learning An Introduction.pdf}
}

@article{redishReplayIntroductionSpecial2020,
  title = {Beyond Replay: {{Introduction}} to the Special Issue on Hippocampal Replay},
  shorttitle = {Beyond Replay},
  author = {Redish, A. David},
  date = {2020},
  journaltitle = {Hippocampus},
  volume = {30},
  number = {1},
  pages = {3--5},
  issn = {1098-1063},
  doi = {10.1002/hipo.23184},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.23184},
  urldate = {2022-09-23},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.23184},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3LT3AT7I/Redish - 2020 - Beyond replay Introduction to the special issue o.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/4YJATCRW/hipo.html}
}

@article{ricebergHippocampalSignalsModify2022a,
  title = {Hippocampal Signals Modify Orbitofrontal Representations to Learn New Paths},
  author = {Riceberg, Justin S. and Srinivasan, Aditya and Guise, Kevin G. and Shapiro, Matthew L.},
  date = {2022-06-27},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2022.06.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982222009228},
  urldate = {2022-07-04},
  abstract = {We often remember the consequences of past choices to adapt to changing circumstances. Recalling past events requires the hippocampus (HPC), and using stimuli to anticipate outcome values requires the orbitofrontal cortex (OFC).1, 2, 3 Spatial reversal tasks require both structures to navigate newly rewarded paths.4,5 Both HPC place6 and OFC value cells7,8 fire in phase with theta (4–12~Hz) oscillations. Both structures are described as cognitive maps: HPC maps space9 and OFC maps task states.10 These similarities imply that OFC-HPC interactions are crucial for using memory to predict outcomes when circumstances change, but the mechanisms remain largely unknown. To investigate possible interactions, we simultaneously recorded ensembles in OFC and CA1 as rats learned spatial reversals in a plus maze. Striking interactions occurred only while rats learned their first reversal: CA1 population vectors predicted changes in OFC activity but not vice versa, OFC spikes phase locked to hippocampal theta oscillations, mixed pairs of CA1 and OFC neurons fired together within single theta cycles, and CA1 led OFC spikes by ∼30~ms. After the new contingency became familiar, CA1 ensembles stably represented distinct spatial paths, whereas OFC ensembles developed more generalized goal arm representations in different paths to identical rewards. These frontotemporal interactions, engaged selectively when new task features inform decision-making, suggest a mechanism for linking novel episodes with expected outcomes, when HPC signals trigger “cognitive remapping” by OFC.11},
  langid = {english},
  keywords = {decision-making,expected outcome,frontotemporal,hippocampus,learning,memory,orbitofrontal,synchrony,theta},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UFF8LM3I/Riceberg et al. - 2022 - Hippocampal signals modify orbitofrontal represent.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/8KTPWQ9R/S0960982222009228.html}
}

@article{richLargeEnvironmentsReveal2014,
  title = {Large Environments Reveal the Statistical Structure Governing Hippocampal Representations},
  author = {Rich, P. Dylan and Liaw, Hua-Peng and Lee, Albert K.},
  date = {2014-08-15},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {345},
  number = {6198},
  pages = {814--817},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255635},
  url = {https://www.science.org/doi/10.1126/science.1255635},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/WH6K9UEH/Rich et al. - 2014 - Large environments reveal the statistical structur.pdf}
}

@article{riggallRelationshipWorkingMemory2012,
  title = {The {{Relationship}} between {{Working Memory Storage}} and {{Elevated Activity}} as {{Measured}} with {{Functional Magnetic Resonance Imaging}}},
  author = {Riggall, Adam C. and Postle, Bradley R.},
  date = {2012-09-19},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {32},
  number = {38},
  eprint = {22993416},
  eprinttype = {pmid},
  pages = {12990--12998},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1892-12.2012},
  url = {https://www.jneurosci.org/content/32/38/12990},
  urldate = {2022-05-04},
  abstract = {Does the sustained, elevated neural activity observed during working memory tasks reflect the short-term retention of information? Functional magnetic resonance imaging (fMRI) data of delayed recognition of visual motion in human participants were analyzed with two methods: a general linear model (GLM) and multivoxel pattern analysis. Although the GLM identified sustained, elevated delay-period activity in superior and lateral frontal cortex and in intraparietal sulcus, pattern classifiers were unable to recover trial-specific stimulus information from these delay-active regions. The converse—no sustained, elevated delay-period activity but successful classification of trial-specific stimulus information—was true of posterior visual regions, including area MT+ (which contains both middle temporal area and medial superior temporal area) and calcarine and pericalcarine cortex. In contrast to stimulus information, pattern classifiers were able to extract trial-specific task instruction-related information from frontal and parietal areas showing elevated delay-period activity. Thus, the elevated delay-period activity that is measured with fMRI may reflect processes other than the storage, per se, of trial-specific stimulus information. It may be that the short-term storage of stimulus information is represented in patterns of (statistically) “subthreshold” activity distributed across regions of low-level sensory cortex that univariate methods cannot detect.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UN8TWLL5/Riggall and Postle - 2012 - The Relationship between Working Memory Storage an.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ABI2MSWD/12990.html}
}

@article{robbeAlterationThetaTimescale2009,
  title = {Alteration of {{Theta Timescale Dynamics}} of {{Hippocampal Place Cells}} by a {{Cannabinoid Is Associated}} with {{Memory Impairment}}},
  author = {Robbe, David and Buzsáki, György},
  date = {2009-10-07},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {29},
  number = {40},
  eprint = {19812334},
  eprinttype = {pmid},
  pages = {12597--12605},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2407-09.2009},
  url = {https://www.jneurosci.org/content/29/40/12597},
  urldate = {2022-01-11},
  abstract = {The integrity of the hippocampus is critical for both spatial navigation and episodic memory, but how its neuronal firing patterns underlie those functions is not well understood. In particular, the modality by which hippocampal place cells contribute to spatial memory is debated. We found that administration of the cannabinoid receptor agonist CP55940 (2-[(1S,2R,5S)-5-hydroxy-2-(3-hydroxypropyl)cyclohexyl]-5-(2-methyloctan-2-yl)phenol) induced a profound and reversible behavioral deficit in the hippocampus-dependent delayed spatial alternation task. On the one hand, despite severe memory impairment, the location-dependent firing of CA1 hippocampal place cells remained mostly intact. On the other hand, both spike-timing coordination between place cells at the theta timescale and theta phase precession of spikes were reversibly reduced. These results raise the possibility that cannabinoids impair memory primarily by altering short-term temporal dynamics of hippocampal neurons. We hypothesize that precise temporal coordination of hippocampal neurons is necessary for guiding behavior in spatial memory tasks.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/494WEBX6/Robbe and Buzsáki - 2009 - Alteration of Theta Timescale Dynamics of Hippocam.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/2BYNILKW/12597.html}
}

@article{robinsonTargetedActivationHippocampal2020,
  title = {Targeted {{Activation}} of {{Hippocampal Place Cells Drives Memory-Guided Spatial Behavior}}},
  author = {Robinson, Nick T. M. and Descamps, Lucie A. L. and Russell, Lloyd E. and Buchholz, Moritz O. and Bicknell, Brendan A. and Antonov, Georgy K. and Lau, Joanna Y. N. and Nutbrown, Rebecca and Schmidt-Hieber, Christoph and Häusser, Michael},
  date = {2020-12-10},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {183},
  number = {6},
  pages = {1586-1599.e10},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2020.09.061},
  url = {https://www.sciencedirect.com/science/article/pii/S0092867420313027},
  urldate = {2022-03-10},
  abstract = {The hippocampus is crucial for spatial navigation and episodic memory formation. Hippocampal place cells exhibit spatially selective activity within an environment and have been proposed to form the neural basis of a cognitive map of space that supports these mnemonic functions. However, the direct influence of place cell activity on spatial navigation behavior has not yet been demonstrated. Using an ‘all-optical’ combination of simultaneous two-photon calcium imaging and two-photon optogenetics, we identified and selectively activated place cells that encoded behaviorally relevant locations in a virtual reality environment. Targeted stimulation of a small number of place cells was sufficient to bias the behavior of animals during a spatial memory task, providing causal evidence that hippocampal place cells actively support spatial navigation and memory.},
  langid = {english},
  keywords = {all-optical interrogation,behavior,hippocampus,inhibition,memory,place cell,spatial navigation,two-photon calcium imaging,two-photon optogenetics,virtual reality},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9QIEAKS8/Robinson et al. - 2020 - Targeted Activation of Hippocampal Place Cells Dri.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/HR2FKF4S/S0092867420313027.html}
}

@article{robsonDynamicalSystemsView2022,
  title = {A Dynamical Systems View of Neuroethology: {{Uncovering}} Stateful Computation in Natural Behaviors},
  shorttitle = {A Dynamical Systems View of Neuroethology},
  author = {Robson, Drew N. and Li, Jennifer M.},
  date = {2022-04-01},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {73},
  pages = {102517},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2022.01.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0959438822000022},
  urldate = {2022-05-04},
  abstract = {State-dependent computation is key to cognition in both biological and artificial systems. Alan Turing recognized the power of stateful computation when he created the Turing machine with theoretically infinite computational capacity in 1936. Independently, by 1950, ethologists such as Tinbergen and Lorenz also began to implicitly embed rudimentary forms of state-dependent computation to create qualitative models of internal drives and naturally occurring animal behaviors. Here, we reformulate core ethological concepts in explicitly dynamical systems terms for stateful computation. We examine, based on a wealth of recent neural data collected during complex innate behaviors across species, the neural dynamics that determine the temporal structure of internal states. We will also discuss the degree to which the brain can be hierarchically partitioned into nested dynamical systems and the need for a multi-dimensional state-space model of the neuromodulatory system that underlies motivational and affective states.},
  langid = {english},
  keywords = {Dynamical systems,Innate behavior,Internal state,Neural dynamics,Neuroethology,Neuromodulation},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/W9DGKLDG/Robson and Li - 2022 - A dynamical systems view of neuroethology Uncover.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/QFGKEEMZ/S0959438822000022.html}
}

@article{roelofsFreezingRevisitedCoordinated2022,
  title = {Freezing Revisited: Coordinated Autonomic and Central Optimization of Threat Coping},
  shorttitle = {Freezing Revisited},
  author = {Roelofs, Karin and Dayan, Peter},
  date = {2022-06-27},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  pages = {1--13},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00608-2},
  url = {https://www.nature.com/articles/s41583-022-00608-2},
  urldate = {2022-06-30},
  abstract = {Animals have sophisticated mechanisms for coping with danger. Freezing is a unique state that, upon threat detection, allows evidence to be gathered, response possibilities to be previsioned and preparations to be made for worst-case fight or flight. We propose that — rather than reflecting a passive fear state — the particular somatic and cognitive characteristics of freezing help to conceal overt responses, while optimizing sensory processing and action preparation. Critical for these functions are the neurotransmitters noradrenaline and acetylcholine, which modulate neural information processing and also control the sympathetic and parasympathetic branches of the autonomic nervous system. However, the interactions between autonomic systems and the brain during freezing, and the way in which they jointly coordinate responses, remain incompletely explored. We review the joint actions of these systems and offer a novel computational framework to describe their temporally harmonized integration. This reconceptualization of freezing has implications for its role in decision-making under threat and for psychopathology.},
  langid = {english},
  keywords = {Amygdala,Autonomic nervous system,Human behaviour,Sensorimotor processing},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IR3J8Y7T/Roelofs and Dayan - 2022 - Freezing revisited coordinated autonomic and cent.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/R4X6G49C/s41583-022-00608-2.html}
}

@article{rosenbergMiceLabyrinthShow2021,
  title = {Mice in a Labyrinth Show Rapid Learning, Sudden Insight, and Efficient Exploration},
  author = {Rosenberg, Matthew and Zhang, Tony and Perona, Pietro and Meister, Markus},
  editor = {Mathis, Mackenzie W and Dulac, Catherine and Berman, Gordon J},
  date = {2021-07-01},
  journaltitle = {eLife},
  volume = {10},
  pages = {e66175},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.66175},
  url = {https://doi.org/10.7554/eLife.66175},
  urldate = {2022-05-16},
  abstract = {Animals learn certain complex tasks remarkably fast, sometimes after a single experience. What behavioral algorithms support this efficiency? Many contemporary studies based on two-alternative-forced-choice (2AFC) tasks observe only slow or incomplete learning. As an alternative, we study the unconstrained behavior of mice in a complex labyrinth and measure the dynamics of learning and the behaviors that enable it. A mouse in the labyrinth makes \textasciitilde 2000 navigation decisions per hour. The animal explores the maze, quickly discovers the location of a reward, and executes correct 10-bit choices after only 10 reward experiences — a learning rate 1000-fold higher than in 2AFC experiments. Many mice improve discontinuously from one minute to the next, suggesting moments of sudden insight about the structure of the labyrinth. The underlying search algorithm does not require a global memory of places visited and is largely explained by purely local turning rules.},
  keywords = {behavior,cognitive map,decision-making,few-shot learning,navigation,predictive models},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UH79FKFB/Rosenberg et al. - 2021 - Mice in a labyrinth show rapid learning, sudden in.pdf}
}

@article{rubinRevealingNeuralCorrelates2019,
  title = {Revealing Neural Correlates of Behavior without Behavioral Measurements},
  author = {Rubin, Alon and Sheintuch, Liron and Brande-Eilat, Noa and Pinchasof, Or and Rechavi, Yoav and Geva, Nitzan and Ziv, Yaniv},
  date = {2019-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {4745},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12724-2},
  url = {http://www.nature.com/articles/s41467-019-12724-2},
  urldate = {2021-12-07},
  abstract = {Abstract             Measuring neuronal tuning curves has been instrumental for many discoveries in neuroscience but requires a priori assumptions regarding the identity of the encoded variables. We applied unsupervised learning to large-scale neuronal recordings in behaving mice from circuits involved in spatial cognition and uncovered a highly-organized internal structure of ensemble activity patterns. This emergent structure allowed defining for each neuron an~‘internal tuning-curve’~that characterizes its activity relative to the network activity, rather than relative to any predefined external variable, revealing place-tuning and head-direction tuning without relying on measurements of place or head-direction. Similar investigation in prefrontal cortex revealed schematic representations of distances and actions, and exposed a previously unknown variable, the ‘trajectory-phase’. The internal structure was conserved across mice, allowing using one animal’s data to decode another animal’s behavior. Thus, the internal structure of neuronal activity itself enables reconstructing internal representations and discovering new behavioral variables hidden within a neural code.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TQ8M9Y28/Rubin et al. - 2019 - Revealing neural correlates of behavior without be.pdf}
}

@article{rueckemannGridCodeOrdered2021,
  title = {The Grid Code for Ordered Experience},
  author = {Rueckemann, Jon W. and Sosa, Marielena and Giocomo, Lisa M. and Buffalo, Elizabeth A.},
  date = {2021-10},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {22},
  number = {10},
  pages = {637--649},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-021-00499-9},
  url = {https://www.nature.com/articles/s41583-021-00499-9},
  urldate = {2022-01-13},
  abstract = {Entorhinal cortical grid cells fire in a periodic pattern that tiles space, which is suggestive of a spatial coordinate system. However, irregularities in the grid pattern as well as responses of grid cells in contexts other than spatial navigation have presented a challenge to existing models of entorhinal function. In this Perspective, we propose that hippocampal input provides a key informative drive to the grid network in both spatial and non-spatial circumstances, particularly around salient events. We build on previous models in which neural activity propagates through the entorhinal–hippocampal network in time. This temporal contiguity in network activity points to temporal order as a necessary characteristic of representations generated by the hippocampal formation. We advocate that interactions in the entorhinal–hippocampal loop build a topological representation that is rooted in the temporal order of experience. In this way, the structure of grid cell firing supports a learned topology rather than a rigid coordinate frame that is bound to measurements of the physical world.},
  issue = {10},
  langid = {english},
  keywords = {Cognitive neuroscience,Hippocampus,Spatial memory,To read},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Cognitive neuroscience;Hippocampus;Spatial memory Subject\_term\_id: cognitive-neuroscience;hippocampus;spatial-memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NJ3EF2V6/Rueckemann et al. - 2021 - The grid code for ordered experience.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/3U99ZTL4/s41583-021-00499-9.html}
}

@article{russekPredictiveRepresentationsCan2017,
  title = {Predictive Representations Can Link Model-Based Reinforcement Learning to Model-Free Mechanisms},
  author = {Russek, Evan M. and Momennejad, Ida and Botvinick, Matthew M. and Gershman, Samuel J. and Daw, Nathaniel D.},
  date = {2017-09-25},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {13},
  number = {9},
  pages = {e1005768},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005768},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005768},
  urldate = {2022-01-25},
  abstract = {Humans and animals are capable of evaluating actions by considering their long-run future rewards through a process described using model-based reinforcement learning (RL) algorithms. The mechanisms by which neural circuits perform the computations prescribed by model-based RL remain largely unknown; however, multiple lines of evidence suggest that neural circuits supporting model-based behavior are structurally homologous to and overlapping with those thought to carry out model-free temporal difference (TD) learning. Here, we lay out a family of approaches by which model-based computation may be built upon a core of TD learning. The foundation of this framework is the successor representation, a predictive state representation that, when combined with TD learning of value predictions, can produce a subset of the behaviors associated with model-based learning, while requiring less decision-time computation than dynamic programming. Using simulations, we delineate the precise behavioral capabilities enabled by evaluating actions using this approach, and compare them to those demonstrated by biological organisms. We then introduce two new algorithms that build upon the successor representation while progressively mitigating its limitations. Because this framework can account for the full range of observed putatively model-based behaviors while still utilizing a core TD framework, we suggest that it represents a neurally plausible family of mechanisms for model-based evaluation.},
  langid = {english},
  keywords = {Algorithms,Animal behavior,Behavior,Dopamine,Hippocampus,Human learning,Learning,Neostriatum},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9Q283GBL/Russek et al. - 2017 - Predictive representations can link model-based re.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/8W588H37/article.html}
}

@article{russoInformationTheoreticAnalysisThompson,
  title = {An {{Information-Theoretic Analysis}} of {{Thompson Sampling}}},
  author = {Russo, Daniel},
  pages = {30},
  abstract = {We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HPQWMCTX/Russo - An Information-Theoretic Analysis of Thompson Samp.pdf}
}

@article{russoTutorialThompsonSampling,
  title = {A {{Tutorial}} on {{Thompson Sampling}}},
  author = {Russo, Daniel J and Roy, Benjamin Van and Kazerouni, Abbas and Wen, Zheng},
  pages = {96},
  abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9B7ZR4Z2/Russo et al. - A Tutorial on Thompson Sampling.pdf}
}

@article{sargoliniConjunctiveRepresentationPosition2006,
  title = {Conjunctive {{Representation}} of {{Position}}, {{Direction}}, and {{Velocity}} in {{Entorhinal Cortex}}},
  author = {Sargolini, Francesca and Fyhn, Marianne and Hafting, Torkel and McNaughton, Bruce L. and Witter, Menno P. and Moser, May-Britt and Moser, Edvard I.},
  date = {2006-05-05},
  journaltitle = {Science},
  volume = {312},
  number = {5774},
  pages = {758--762},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1125572},
  url = {https://www.science.org/doi/abs/10.1126/science.1125572},
  urldate = {2021-12-21},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/69X39Q5T/Sargolini et al. - 2006 - Conjunctive Representation of Position, Direction,.pdf}
}

@inproceedings{saxeHierarchyCompositionMultitask2017,
  title = {Hierarchy {{Through Composition}} with {{Multitask LMDPs}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Saxe, Andrew M. and Earle, Adam C. and Rosman, Benjamin},
  date = {2017-07-17},
  pages = {3017--3026},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/saxe17a.html},
  urldate = {2022-03-15},
  abstract = {Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme exploits the guaranteed concurrent compositionality provided by the linearly solvable Markov decision process (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/C47LZCVH/Saxe et al. - 2017 - Hierarchy Through Composition with Multitask LMDPs.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/FY47N9GX/Saxe et al. - 2017 - Hierarchy Through Composition with Multitask LMDPs.pdf}
}

@article{schalkFacephenesRainbowsCausal2017,
  title = {Facephenes and Rainbows: {{Causal}} Evidence for Functional and Anatomical Specificity of Face and Color Processing in the Human Brain},
  shorttitle = {Facephenes and Rainbows},
  author = {Schalk, Gerwin and Kapeller, Christoph and Guger, Christoph and Ogawa, Hiroshi and Hiroshima, Satoru and Lafer-Sousa, Rosa and Saygin, Zeynep M. and Kamada, Kyousuke and Kanwisher, Nancy},
  date = {2017-11-14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {46},
  pages = {12285--12290},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1713447114},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1713447114},
  urldate = {2022-10-14},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/2D6C2FBT/Schalk et al. - 2017 - Facephenes and rainbows Causal evidence for funct.pdf}
}

@article{schapiroStatisticalLearningTemporal2016,
  title = {Statistical Learning of Temporal Community Structure in the Hippocampus},
  author = {Schapiro, Anna C. and Turk-Browne, Nicholas B. and Norman, Kenneth A. and Botvinick, Matthew M.},
  date = {2016},
  journaltitle = {Hippocampus},
  volume = {26},
  number = {1},
  pages = {3--8},
  issn = {1098-1063},
  doi = {10.1002/hipo.22523},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.22523},
  urldate = {2022-01-27},
  abstract = {The hippocampus is involved in the learning and representation of temporal statistics, but little is understood about the kinds of statistics it can uncover. Prior studies have tested various forms of structure that can be learned by tracking the strength of transition probabilities between adjacent items in a sequence. We test whether the hippocampus can learn higher-order structure using sequences that have no variance in transition probability and instead exhibit temporal community structure. We find that the hippocampus is indeed sensitive to this form of structure, as revealed by its representations, activity dynamics, and connectivity with other regions. These findings suggest that the hippocampus is a sophisticated learner of environmental regularities, able to uncover higher-order structure that requires sensitivity to overlapping associations. © 2015 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {background connectivity,event representation,fMRI,pattern analysis,transition probability},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.22523},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EWJ5UUDQ/Schapiro et al. - 2016 - Statistical learning of temporal community structu.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/YP376MKT/hipo.html}
}

@misc{schaulPrioritizedExperienceReplay2016,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date = {2016-02-25},
  number = {arXiv:1511.05952},
  eprint = {1511.05952},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.05952},
  url = {http://arxiv.org/abs/1511.05952},
  urldate = {2022-10-31},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5X8WT734/Schaul et al. - 2016 - Prioritized Experience Replay.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/F5593SXL/1511.html}
}

@article{schlingloffMechanismsSharpWave2014,
  title = {Mechanisms of {{Sharp Wave Initiation}} and {{Ripple Generation}}},
  author = {Schlingloff, Dániel and Káli, Szabolcs and Freund, Tamás F. and Hájos, Norbert and Gulyás, Attila I.},
  date = {2014-08-20},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {34},
  number = {34},
  eprint = {25143618},
  eprinttype = {pmid},
  pages = {11385--11398},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0867-14.2014},
  url = {https://www.jneurosci.org/content/34/34/11385},
  urldate = {2022-09-23},
  abstract = {Replay of neuronal activity during hippocampal sharp wave-ripples (SWRs) is essential in memory formation. To understand the mechanisms underlying the initiation of irregularly occurring SWRs and the generation of periodic ripples, we selectively manipulated different components of the CA3 network in mouse hippocampal slices. We recorded EPSCs and IPSCs to examine the buildup of neuronal activity preceding SWRs and analyzed the distribution of time intervals between subsequent SWR events. Our results suggest that SWRs are initiated through a combined refractory and stochastic mechanism. SWRs initiate when firing in a set of spontaneously active pyramidal cells triggers a gradual, exponential buildup of activity in the recurrent CA3 network. We showed that this tonic excitatory envelope drives reciprocally connected parvalbumin-positive basket cells, which start ripple-frequency spiking that is phase-locked through reciprocal inhibition. The synchronized GABAA receptor-mediated currents give rise to a major component of the ripple-frequency oscillation in the local field potential and organize the phase-locked spiking of pyramidal cells. Optogenetic stimulation of parvalbumin-positive cells evoked full SWRs and EPSC sequences in pyramidal cells. Even with excitation blocked, tonic driving of parvalbumin-positive cells evoked ripple oscillations. Conversely, optogenetic silencing of parvalbumin-positive cells interrupted the SWRs or inhibited their occurrence. Local drug applications and modeling experiments confirmed that the activity of parvalbumin-positive perisomatic inhibitory neurons is both necessary and sufficient for ripple-frequency current and rhythm generation. These interneurons are thus essential in organizing pyramidal cell activity not only during gamma oscillation, but, in a different configuration, during SWRs.},
  langid = {english},
  keywords = {basket cell,current generator,hippocampus,inhibition,oscillation,rhythm},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DFKMYI8D/Schlingloff et al. - 2014 - Mechanisms of Sharp Wave Initiation and Ripple Gen.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/L7SFQZ72/11385.html}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, Jürgen},
  date = {2015-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {61},
  pages = {85--117},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2014.09.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
  urldate = {2022-04-11},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  langid = {english},
  keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,To read,Unsupervised learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/I367WXJL/Schmidhuber2015.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/T4EYG4KJ/Schmidhuber - 2015 - Deep learning in neural networks An overview.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/TVG5W4EY/S0893608014002135.html}
}

@incollection{scholkopfLogarithmicOnlineRegret2007,
  title = {Logarithmic {{Online Regret Bounds}} for {{Undiscounted Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  editor = {Schölkopf, Bernhard and Platt, John and Hofmann, Thomas},
  date = {2007},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/7503.003.0011},
  url = {https://direct.mit.edu/books/book/3168/chapter/87374/logarithmic-online-regret-bounds-for-undiscounted},
  urldate = {2021-12-07},
  abstract = {We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some finite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper confidence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy.},
  isbn = {978-0-262-25691-9},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9GMHKMU7/Schölkopf et al. - 2007 - Logarithmic Online Regret Bounds for Undiscounted .pdf}
}

@article{schrittwieserMasteringAtariGo2020,
  title = {Mastering {{Atari}}, {{Go}}, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2020-12-24},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  url = {http://www.nature.com/articles/s41586-020-03051-4},
  urldate = {2021-12-12},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XLJYAAGJ/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf}
}

@article{schuckHumanOrbitofrontalCortex2016,
  title = {Human {{Orbitofrontal Cortex Represents}} a {{Cognitive Map}} of {{State Space}}},
  author = {Schuck, Nicolas W. and Cai, Ming Bo and Wilson, Robert C. and Niv, Yael},
  date = {2016-09-21},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {91},
  number = {6},
  pages = {1402--1412},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.08.019},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627316305116},
  urldate = {2022-02-10},
  abstract = {Although the orbitofrontal cortex (OFC) has been studied intensely for decades, its precise functions have remained elusive. We recently hypothesized that the OFC contains a “cognitive map” of task space in which the current state of the task is represented, and this representation is especially critical for behavior when states are unobservable from sensory input. To test this idea, we apply pattern-classification techniques to neuroimaging data from humans performing a decision-making task with 16 states. We show that unobservable task states can be decoded from activity in OFC, and decoding accuracy is related to task performance and the occurrence of individual behavioral errors. Moreover, similarity between the neural representations of consecutive states correlates with behavioral accuracy in corresponding state transitions. These results support the idea that OFC represents a cognitive map of task space and establish the feasibility of decoding state representations in humans using non-invasive neuroimaging.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QU4I78N7/Schuck et al. - 2016 - Human Orbitofrontal Cortex Represents a Cognitive .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/3S3GX7FL/S0896627316305116.html}
}

@article{schuckSequentialReplayNonspatial2019,
  title = {Sequential Replay of Nonspatial Task States in the Human Hippocampus},
  author = {Schuck, Nicolas W. and Niv, Yael},
  date = {2019-06-28},
  journaltitle = {Science},
  volume = {364},
  number = {6447},
  pages = {eaaw5181},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aaw5181},
  url = {https://www.science.org/doi/full/10.1126/science.aaw5181},
  urldate = {2022-04-05},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/N9KFWSYA/Schuck and Niv - 2019 - Sequential replay of nonspatial task states in the.pdf}
}

@article{schuetteLongTermCharacterizationHippocampal2020,
  title = {Long-{{Term Characterization}} of {{Hippocampal Remapping}} during {{Contextual Fear Acquisition}} and {{Extinction}}},
  author = {Schuette, Peter J. and Reis, Fernando M. C. V. and Maesta-Pereira, Sandra and Chakerian, Meghmik and Torossian, Anita and Blair, Garrett J. and Wang, Weisheng and Blair, Hugh T. and Fanselow, Michael S. and Kao, Jonathan C. and Adhikari, Avishek},
  date = {2020-10-21},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {40},
  number = {43},
  eprint = {32958567},
  eprinttype = {pmid},
  pages = {8329--8342},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1022-20.2020},
  url = {https://www.jneurosci.org/content/40/43/8329},
  urldate = {2022-09-20},
  abstract = {Hippocampal CA1 place cell spatial maps are known to alter their firing properties in response to contextual fear conditioning, a process called “remapping.” In the present study, we use chronic calcium imaging to examine remapping during fear retrieval and extinction of an inhibitory avoidance task in mice of both sexes over an extended period of time and with thousands of neurons. We demonstrate that hippocampal ensembles encode space at a finer scale following fear memory acquisition. This effect is strongest near the shock grid. We also characterize the long-term effects of shock on place cell ensemble stability, demonstrating that shock delivery induces several days of high fear and low between-session place field stability, followed by a new, stable spatial representation that appears after fear extinction. Finally, we identify a novel group of CA1 neurons that robustly encode freeze behavior independently from spatial location. Thus, following fear acquisition, hippocampal CA1 place cells sharpen their spatial tuning and dynamically change spatial encoding stability throughout fear learning and extinction. SIGNIFICANCE STATEMENT The hippocampus contains place cells that encode an animal's location. This spatial code updates, or remaps, in response to environmental change. It is known that contextual fear can induce such remapping; in the present study, we use chronic calcium imaging to examine inhibitory avoidance-induced remapping over an extended period of time and with thousands of neurons and demonstrate that hippocampal ensembles encode space at a finer scale following electric shock, an effect which is enhanced by threat proximity. We also identify a novel group of freeze behavior-activated neurons. These results suggest that, more than merely shuffling their spatial code following threat exposure, place cells enhance their spatial coding with the possible benefit of improved threat localization.},
  langid = {english},
  keywords = {calcium imaging,contextual fear conditioning,hippocampus,miniaturized microscope,place cell,remapping},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IX4SWT37/Schuette et al. - 2020 - Long-Term Characterization of Hippocampal Remappin.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/C8VA8AUI/8329.html}
}

@article{schuetteLongTermCharacterizationHippocampal2020a,
  title = {Long-{{Term Characterization}} of {{Hippocampal Remapping}} during {{Contextual Fear Acquisition}} and {{Extinction}}},
  author = {Schuette, Peter J. and Reis, Fernando M. C. V. and Maesta-Pereira, Sandra and Chakerian, Meghmik and Torossian, Anita and Blair, Garrett J. and Wang, Weisheng and Blair, Hugh T. and Fanselow, Michael S. and Kao, Jonathan C. and Adhikari, Avishek},
  date = {2020-10-21},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {40},
  number = {43},
  pages = {8329--8342},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1022-20.2020},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1022-20.2020},
  urldate = {2022-09-20},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HTLN8BMT/Schuette et al. - 2020 - Long-Term Characterization of Hippocampal Remappin.pdf}
}

@article{schultzNeuralSubstratePrediction,
  title = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P Read},
  pages = {8},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/2GWNWF3M/Schultz et al. - A Neural Substrate of Prediction and Reward.pdf}
}

@article{schulzAlgorithmicArchitectureExploration2019,
  title = {The Algorithmic Architecture of Exploration in the Human Brain},
  author = {Schulz, Eric and Gershman, Samuel J.},
  date = {2019-04-01},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Machine {{Learning}}, {{Big Data}}, and {{Neuroscience}}},
  volume = {55},
  pages = {7--14},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2018.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0959438818300904},
  urldate = {2022-02-09},
  abstract = {Balancing exploration and exploitation is one of the central problems in reinforcement learning. We review recent studies that have identified multiple algorithmic strategies underlying exploration. In particular, humans use a combination of random and uncertainty-directed exploration strategies, which rely on different brain systems, have different developmental trajectories, and are sensitive to different task manipulations. Humans are also able to exploit sophisticated structural knowledge to aid their exploration, such as information about correlations between options. New computational models, drawing inspiration from machine learning, have begun to formalize these ideas and offer new ways to understand the neural basis of reinforcement learning.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8YAWB8PU/Schulz and Gershman - 2019 - The algorithmic architecture of exploration in the.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/T34J684G/S0959438818300904.html}
}

@article{schwartenbeckGenerativeReplayCompositional2021,
  title = {Generative Replay for Compositional Visual Understanding in the Prefrontal-Hippocampal Circuit},
  author = {Schwartenbeck, Philipp and Baram, Alon and Liu, Yunzhe and Mark, Shirley and Muller, Timothy and Dolan, Raymond and Botvinick, Matthew and Kurth-Nelson, Zeb and Behrens, Timothy},
  date = {2021-06-06},
  pages = {2021.06.06.447249},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.06.06.447249},
  url = {https://www.biorxiv.org/content/10.1101/2021.06.06.447249v1},
  urldate = {2022-05-09},
  abstract = {Understanding the visual world is a constructive process. Whilst a frontal-hippocampal circuit is known to be essential for this task, little is known about the associated neuronal computations. Visual understanding appears superficially distinct from other known functions of this circuit, such as spatial reasoning and model-based planning, but recent models suggest deeper computational similarities. Here, using fMRI, we show that representations of a simple visual scene in these brain regions are relational and compositional – key computational properties theorised to support rapid construction of hippocampal maps. Using MEG, we show that rapid sequences of representations, akin to replay in spatial navigation and planning problems, are also engaged in visual construction. Whilst these sequences have previously been proposed as mechanisms to plan possible futures or learn from the past, here they are used to understand the present. Replay sequences form constructive hypotheses about possible scene configurations. These hypotheses play out in an optimal order for relational inference, progressing from predictable to uncertain scene elements, gradually constraining possible configurations, and converging on the correct scene configuration. Together, these results suggest a computational bridge between apparently distinct functions of hippocampal-prefrontal circuitry, and a role for generative replay in constructive inference and hypothesis testing.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/V5ZDUPXQ/Schwartenbeck et al. - 2021 - Generative replay for compositional visual underst.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/7URABBH2/2021.06.06.447249v1.html}
}

@unpublished{sezenerComputingValueComputation2018,
  title = {Computing the {{Value}} of {{Computation}} for {{Planning}}},
  author = {Sezener, Can Eren},
  date = {2018-11-07},
  eprint = {1811.03035},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.03035},
  urldate = {2022-03-07},
  abstract = {An intelligent agent performs actions in order to achieve its goals. Such actions can either be externally directed, such as opening a door, or internally directed, such as writing data to a memory location or strengthening a synaptic connection. Some internal actions, to which we refer as computations, potentially help the agent choose better actions. Considering that (external) actions and computations might draw upon the same resources, such as time and energy, deciding when to act or compute, as well as what to compute, are detrimental to the performance of an agent. In an environment that provides rewards depending on an agent's behavior, an action's value is typically defined as the sum of expected long-term rewards succeeding the action (itself a complex quantity that depends on what the agent goes on to do after the action in question). However, defining the value of a computation is not as straightforward, as computations are only valuable in a higher order way, through the alteration of actions. This thesis offers a principled way of computing the value of a computation in a planning setting formalized as a Markov decision process. We present two different definitions of computation values: static and dynamic. They address two extreme cases of the computation budget: affording calculation of zero or infinitely many steps in the future. We show that these values have desirable properties, such as temporal consistency and asymptotic convergence. Furthermore, we propose methods for efficiently computing and approximating the static and dynamic computation values. We describe a sense in which the policies that greedily maximize these values can be optimal. We utilize these principles to construct Monte Carlo tree search algorithms that outperform most of the state-of-the-art in terms of finding higher quality actions given the same simulation resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FGHG6UN5/Sezener - 2018 - Computing the Value of Computation for Planning.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/INUYGBNJ/1811.html}
}

@article{sezenerOptimizingDepthDirection2019,
  title = {Optimizing the Depth and the Direction of Prospective Planning Using Information Values},
  author = {Sezener, Can Eren and Dezfouli, Amir and Keramati, Mehdi},
  date = {2019-03-12},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {15},
  number = {3},
  pages = {e1006827},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006827},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006827},
  urldate = {2022-03-02},
  abstract = {Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are “in which directions the search tree should be expanded?”, and “when should the expansion stop?”. Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.},
  langid = {english},
  keywords = {Algorithms,Animal behavior,Decision making,Decision trees,Monte Carlo method,Normal distribution,To read,Trees,Working memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KB7JJHSH/Sezener et al. - 2019 - Optimizing the depth and the direction of prospect.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/Q8KJB4SB/article.html}
}

@article{sezenerStaticDynamicValues2020,
  title = {Static and {{Dynamic Values}} of {{Computation}} in {{MCTS}}},
  author = {Sezener, Eren and Dayan, Peter},
  date = {2020},
  pages = {10},
  abstract = {Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations (i.e., simulations) to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the myopic limitations of existing computation-value-based methods in two senses: (I) we are able to account for the impact of non-immediate (ie, future) computations (II) on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/K3U9BJ8D/Sezener and Dayan - Static and Dynamic Values of Computation in MCTS.pdf}
}

@inproceedings{sezenerStaticDynamicValues2020a,
  title = {Static and {{Dynamic Values}} of {{Computation}} in {{MCTS}}},
  booktitle = {Proceedings of the 36th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  author = {Sezener, Eren and Dayan, Peter},
  date = {2020-08-27},
  pages = {31--40},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v124/sezener20a.html},
  urldate = {2022-01-14},
  abstract = {Monte-Carlo Tree Search (MCTS) is one of the most-widely used methodsfor planning, and has powered many recent advances in artificialintelligence. In MCTS, one typically performs computations(i.e., simulations) to collect statistics about the possible futureconsequences of actions, and then chooses accordingly. Manypopular MCTS methods such as UCT and its variants decide whichcomputations to perform by trading-off exploration and exploitation. Inthis work, we take a more direct approach, and explicitly quantify thevalue of a computation based on its expected impact on the quality ofthe action eventually chosen. Our approach goes beyond the \textbackslash emph\{myopic\}limitations of existing computation-value-based methods in two senses:(I) we are able to account for the impact of non-immediate (ie, future)computations (II) on non-immediate actions. We show that policies thatgreedily optimize computation values are optimal under certainassumptions and obtain results that are competitive with thestate-of-the-art.},
  eventtitle = {Conference on {{Uncertainty}} in {{Artificial Intelligence}}},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AKMNW2KE/Sezener and Dayan - 2020 - Static and Dynamic Values of Computation in MCTS.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/N2QIP2DN/Sezener and Dayan - 2020 - Static and Dynamic Values of Computation in MCTS.pdf}
}

@unpublished{sharmaMapInductionCompositional2021,
  title = {Map {{Induction}}: {{Compositional}} Spatial Submap Learning for Efficient Exploration in Novel Environments},
  shorttitle = {Map {{Induction}}},
  author = {Sharma, Sugandha and Curtis, Aidan and Kryven, Marta and Tenenbaum, Josh and Fiete, Ila},
  date = {2021-10-23},
  eprint = {2110.12301},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.12301},
  urldate = {2022-03-20},
  abstract = {Humans are expert explorers. Understanding the computational cognitive mechanisms that support this efficiency can advance the study of the human mind and enable more efficient exploration algorithms. We hypothesize that humans explore new environments efficiently by inferring the structure of unobserved spaces using spatial information collected from previously explored spaces. This cognitive process can be modeled computationally using program induction in a Hierarchical Bayesian framework that explicitly reasons about uncertainty with strong spatial priors. Using a new behavioral Map Induction Task, we demonstrate that this computational framework explains human exploration behavior better than non-inductive models and outperforms state-of-the-art planning algorithms when applied to a realistic spatial navigation domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZZYI7PUH/Sharma et al. - 2021 - Map Induction Compositional spatial submap learni.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/3NLA46DT/2110.html}
}

@article{sharpeIntegratedModelAction2019,
  title = {An {{Integrated Model}} of {{Action Selection}}: {{Distinct Modes}} of {{Cortical Control}} of {{Striatal Decision Making}}},
  shorttitle = {An {{Integrated Model}} of {{Action Selection}}},
  author = {Sharpe, Melissa J. and Stalnaker, Thomas and Schuck, Nicolas W. and Killcross, Simon and Schoenbaum, Geoffrey and Niv, Yael},
  date = {2019-01-04},
  journaltitle = {Annual Review of Psychology},
  shortjournal = {Annu. Rev. Psychol.},
  volume = {70},
  number = {1},
  pages = {53--76},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-010418-102824},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-010418-102824},
  urldate = {2022-07-19},
  abstract = {Making decisions in environments with few choice options is easy. We select the action that results in the most valued outcome. Making decisions in more complex environments, where the same action can produce different outcomes in different conditions, is much harder. In such circumstances, we propose that accurate action selection relies on top-down control from the prelimbic and orbitofrontal cortices over striatal activity through distinct thalamostriatal circuits. We suggest that the prelimbic cortex exerts direct influence over medium spiny neurons in the dorsomedial striatum to represent the state space relevant to the current environment. Conversely, the orbitofrontal cortex is argued to track a subject's position within that state space, likely through modulation of cholinergic interneurons.},
  langid = {english},
  keywords = {OFC,PFC,Striatum},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8YXS54BX/Sharpe et al. - 2019 - An Integrated Model of Action Selection Distinct .pdf}
}

@article{shevlinHighvalueDecisionsAre2022,
  title = {High-Value Decisions Are Fast and Accurate, Inconsistent with Diminishing Value Sensitivity},
  author = {Shevlin, Blair R. K. and Smith, Stephanie M. and Hausfeld, Jan and Krajbich, Ian},
  date = {2022-02-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {119},
  number = {6},
  eprint = {35105801},
  eprinttype = {pmid},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2101508119},
  url = {https://www.pnas.org/content/119/6/e2101508119},
  urldate = {2022-02-15},
  abstract = {It is a widely held belief that people’s choices are less sensitive to changes in value as value increases. For example, the subjective difference between \$11 and \$12 is believed to be smaller than between \$1 and \$2. This idea is consistent with applications of the Weber-Fechner Law and divisive normalization to value-based choice and with psychological interpretations of diminishing marginal utility. According to random utility theory in economics, smaller subjective differences predict less accurate choices. Meanwhile, in the context of sequential sampling models in psychology, smaller subjective differences also predict longer response times. Based on these models, we would predict decisions between high-value options to be slower and less accurate. In contrast, some have argued on normative grounds that choices between high-value options should be made with less caution, leading to faster and less accurate choices. Here, we model the dynamics of the choice process across three different choice domains, accounting for both discriminability and response caution. Contrary to predictions, we mostly observe faster and more accurate decisions (i.e., higher drift rates) between high-value options. We also observe that when participants are alerted about incoming high-value decisions, they exert more caution and not less. We rule out several explanations for these results, using tasks with both subjective and objective values. These results cast doubt on the notion that increasing value reduces discriminability.},
  langid = {english},
  keywords = {decision-making,drift diffusion model,neuroeconomics,overall value,response time,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/L97QDZ7V/Shevlin et al. - 2022 - High-value decisions are fast and accurate, incons.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/B6CGYDRE/e2101508119.html}
}

@article{shinDynamicsAwakeHippocampalPrefrontal2019,
  title = {Dynamics of {{Awake Hippocampal-Prefrontal Replay}} for {{Spatial Learning}} and {{Memory-Guided Decision Making}}},
  author = {Shin, Justin D. and Tang, Wenbo and Jadhav, Shantanu P.},
  date = {2019-12-18},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {104},
  number = {6},
  pages = {1110-1125.e7},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.09.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627319307858},
  urldate = {2022-01-11},
  abstract = {Spatial learning requires remembering and choosing paths to goals. Hippocampal place cells replay spatial paths during immobility in reverse and forward order, offering a potential mechanism. However, how replay supports both goal-directed learning and memory-guided decision making is unclear. We therefore continuously tracked awake replay in the same hippocampal-prefrontal ensembles throughout learning of a spatial alternation task. We found that, during pauses between behavioral trajectories, reverse and forward hippocampal replay supports an internal cognitive search of available past and future possibilities and exhibits opposing learning gradients for prediction of past and future behavioral paths, respectively. Coordinated hippocampal-prefrontal replay distinguished correct past and future paths from alternative choices, suggesting a role in recall of past paths to guide planning of future decisions for spatial working memory. Our findings reveal a learning shift from hippocampal reverse-replay-based retrospective evaluation to forward-replay-based prospective planning, with prefrontal readout of memory-guided paths for learning and decision making.},
  langid = {english},
  keywords = {decision making,hippocampus,planning,prefrontal cortex,prospection,replay,retrospection,sharp-wave ripple,spatial learning,working memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9VPJGZYX/Shin et al. - 2019 - Dynamics of Awake Hippocampal-Prefrontal Replay fo.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/WHM8SITW/S0896627319307858.html}
}

@unpublished{shlensTutorialPrincipalComponent2014,
  title = {A {{Tutorial}} on {{Principal Component Analysis}}},
  author = {Shlens, Jonathon},
  date = {2014-04-03},
  eprint = {1404.1100},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1404.1100},
  urldate = {2021-12-11},
  abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/SZTKP2GH/Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf}
}

@unpublished{shyamModelBasedActiveExploration2019,
  title = {Model-{{Based Active Exploration}}},
  author = {Shyam, Pranav and Jaśkowski, Wojciech and Gomez, Faustino},
  date = {2019-06-13},
  eprint = {1810.12162},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1810.12162},
  urldate = {2021-12-07},
  abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to highdimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NZL8SFB3/Shyam et al. - 2019 - Model-Based Active Exploration.pdf}
}

@article{silvaTrajectoryEventsHippocampal2015,
  title = {Trajectory Events across Hippocampal Place Cells Require Previous Experience},
  author = {Silva, Delia and Feng, Ting and Foster, David J.},
  date = {2015-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {18},
  number = {12},
  pages = {1772--1779},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4151},
  url = {https://www.nature.com/articles/nn.4151},
  urldate = {2022-09-27},
  abstract = {Hippocampal place cells are active offline in ‘replay’ sequences reflecting speeded-up depictions of behavioral trajectories, suggesting a model of memory. The authors show that encoding of replay sequences requires behavioral experience and the activation of molecular mechanisms of synaptic plasticity, while retrieval does not.},
  issue = {12},
  langid = {english},
  keywords = {Hippocampus,Navigation,Neuroscience},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CRS47NMC/Silva et al. - 2015 - Trajectory events across hippocampal place cells r.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/NVCDEJLN/nn.html}
}

@article{silverDeterministicPolicyGradient,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  pages = {9},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6WK4RI6W/Silver et al. - Deterministic Policy Gradient Algorithms.pdf}
}

@inproceedings{silverMonteCarloPlanningLarge2010,
  title = {Monte-{{Carlo Planning}} in {{Large POMDPs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Silver, David and Veness, Joel},
  date = {2010},
  volume = {23},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2010/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html},
  urldate = {2022-02-09},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/UXW99EKY/Silver and Veness - 2010 - Monte-Carlo Planning in Large POMDPs.pdf}
}

@article{singerHippocampalSWRActivity2013,
  title = {Hippocampal {{SWR Activity Predicts Correct Decisions}} during the {{Initial Learning}} of an {{Alternation Task}}},
  author = {Singer, Annabelle~C. and Carr, Margaret~F. and Karlsson, Mattias~P. and Frank, Loren~M.},
  date = {2013-03},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {77},
  number = {6},
  pages = {1163--1173},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.01.027},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627313000937},
  urldate = {2021-12-07},
  abstract = {The hippocampus frequently replays memories of past experiences during sharp-wave ripple (SWR) events. These events can represent spatial trajectories extending from the animal’s current location to distant locations, suggesting a role in the evaluation of upcoming choices. While SWRs have been linked to learning and memory, the specific role of awake replay remains unclear. Here we show that there is greater coordinated neural activity during SWRs preceding correct, as compared to incorrect, trials in a spatial alternation task. As a result, the proportion of cell pairs coactive during SWRs was predictive of subsequent correct or incorrect responses on a trial-by-trial basis. This effect was seen specifically during early learning, when the hippocampus is essential for task performance. SWR activity preceding correct trials represented multiple trajectories that included both correct and incorrect options. These results suggest that reactivation during awake SWRs contributes to the evaluation of possible choices during memory-guided decision making.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EKTWZT55/Singer et al. - 2013 - Hippocampal SWR Activity Predicts Correct Decision.pdf}
}

@article{singerRewardedOutcomesEnhance2009,
  title = {Rewarded {{Outcomes Enhance Reactivation}} of {{Experience}} in the {{Hippocampus}}},
  author = {Singer, Annabelle C. and Frank, Loren M.},
  date = {2009-12},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {64},
  number = {6},
  pages = {910--921},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.11.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089662730900899X},
  urldate = {2021-12-07},
  abstract = {Remembering experiences that lead to reward is essential for survival. The hippocampus is required for forming and storing memories of events and places, but the mechanisms that associate specific experiences with rewarding outcomes are not understood. Event memory storage is thought to depend on the reactivation of previous experiences during hippocampal sharp wave ripples (SWRs). We used a sequence switching task that allowed us to examine the interaction between SWRs and reward. We compared SWR activity after animals traversed spatial trajectories and either received or did not receive a reward. Here, we show that rat hippocampal CA3 principal cells are significantly more active during SWRs following receipt of reward. This SWR activity was further enhanced during learning and reactivated coherent elements of the paths associated with the reward location. This enhanced reactivation in response to reward could be a mechanism to bind rewarding outcomes to the experiences that precede them.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TDNE42BS/Singer and Frank - 2009 - Rewarded Outcomes Enhance Reactivation of Experien.pdf}
}

@article{singhModelAutonomousInteractions2022,
  title = {A Model of Autonomous Interactions between Hippocampus and Neocortex Driving Sleep-Dependent Memory Consolidation},
  author = {Singh, Dhairyya and Norman, Kenneth A. and Schapiro, Anna C.},
  date = {2022-02-01},
  pages = {2022.01.31.478475},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.01.31.478475},
  url = {https://www.biorxiv.org/content/10.1101/2022.01.31.478475v1},
  urldate = {2022-05-05},
  abstract = {How do we build up our knowledge of the world over time? Many theories of memory formation and consolidation have posited that the hippocampus stores new information, then “teaches” this information to neocortex over time, especially during sleep. But it is unclear, mechanistically, how this actually works — how are these systems able to interact during periods with virtually no environmental input to accomplish useful learning and shifts in representation? We provide a framework for thinking about this question, with neural network model simulations serving as demonstrations. The model contains hippocampus and neocortical areas, which replay memories and interact with one another completely autonomously during simulated sleep. Oscillations are leveraged to support error-driven learning that leads to useful changes in memory representation and behavior. The model has a non-Rapid Eye Movement (NREM) sleep stage, where dynamics between hippocampus and neocortex are tightly coupled, with hippocampus helping neocortex to reinstate high-fidelity versions of new attractors, and a REM sleep stage, where neocortex is able to more freely explore existing attractors. We find that alternating between NREM and REM sleep stages, which alternately focuses the model’s replay on recent and remote information, facilitates graceful continual learning. We thus provide an account of how the hippocampus and neocortex can interact without any external input during sleep to drive useful new cortical learning and to protect old knowledge as new information is integrated.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZJUWHMIL/Singh et al. - 2022 - A model of autonomous interactions between hippoca.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/L2HI6IUC/2022.01.31.478475v1.html}
}

@article{sirotaCommunicationNeocortexHippocampus2003,
  title = {Communication between Neocortex and Hippocampus during Sleep in Rodents},
  author = {Sirota, A. and Csicsvari, J. and Buhl, D. and Buzsaki, G.},
  date = {2003-02-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proceedings of the National Academy of Sciences},
  volume = {100},
  number = {4},
  pages = {2065--2069},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0437938100},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0437938100},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/VFSMQVPP/Sirota et al. - 2003 - Communication between neocortex and hippocampus du.pdf}
}

@unpublished{slivkinsIntroductionMultiArmedBandits2021,
  title = {Introduction to {{Multi-Armed Bandits}}},
  author = {Slivkins, Aleksandrs},
  date = {2021-06-26},
  eprint = {1904.07272},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.07272},
  urldate = {2021-12-10},
  abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TC47GJVY/Slivkins - 2021 - Introduction to Multi-Armed Bandits.pdf}
}

@article{soltaniAdaptiveLearningExpected2019,
  title = {Adaptive Learning under Expected and Unexpected Uncertainty},
  author = {Soltani, Alireza and Izquierdo, Alicia},
  date = {2019-10},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {20},
  number = {10},
  pages = {635--644},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-019-0180-y},
  url = {https://www.nature.com/articles/s41583-019-0180-y},
  urldate = {2022-08-07},
  abstract = {The outcome of a decision is often uncertain, and outcomes can vary over repeated decisions. Whether decision outcomes should substantially affect behaviour and learning depends on whether they are representative of a typically experienced range of outcomes or signal a change in the reward environment. Successful learning and decision-making therefore require the ability to estimate expected uncertainty (related to the variability of outcomes) and unexpected uncertainty (related to the variability of the environment). Understanding the bases and effects of these two types of uncertainty and the interactions between them — at the computational and the neural level — is crucial for understanding adaptive learning. Here, we examine computational models and experimental findings to distil computational principles and neural mechanisms for adaptive learning under uncertainty.},
  issue = {10},
  langid = {english},
  keywords = {Amygdala,Computational neuroscience,Learning and memory,Prefrontal cortex},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/K5RARGC9/Soltani and Izquierdo - 2019 - Adaptive learning under expected and unexpected un.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/6RTSXW9N/s41583-019-0180-y.html}
}

@article{sosaNavigatingReward2021,
  title = {Navigating for Reward},
  author = {Sosa, Marielena and Giocomo, Lisa M.},
  date = {2021-08},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {22},
  number = {8},
  pages = {472--487},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-021-00479-z},
  url = {http://www.nature.com/articles/s41583-021-00479-z},
  urldate = {2021-12-07},
  abstract = {An organism’s survival can depend on its ability to recall and navigate to spatial locations associated with rewards, such as food or a home. Accumulating research has revealed that computations of reward and its prediction occur on multiple levels across a complex set of interacting brain regions, including those that support memory and navigation. However, how the brain coordinates the encoding, recall and use of reward information to guide navigation remains incompletely understood. In this Review, we propose that the brain’s classical navigation centres — the hippocampus and the entorhinal cortex — are ideally suited to coordinate this larger network by representing both physical and mental space as a series of states. These states may be linked to reward via neuromodulatory inputs to the hippocampus–entorhinal cortex system. Hippocampal outputs can then broadcast sequences of states to the rest of the brain to store reward associations or to facilitate decision-m aking, potentially engaging additional value signals downstream. This proposal is supported by recent advances in both experimental and theoretical neuroscience. By discussing the neural systems traditionally tied to navigation and reward at their intersection, we aim to offer an integrated framework for understanding navigation to reward as a fundamental feature of many cognitive processes.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/SV36NQBB/Sosa and Giocomo - 2021 - Navigating for reward.pdf}
}

@article{sridharGeometryDecisionmakingIndividuals2021,
  title = {The Geometry of Decision-Making in Individuals and Collectives},
  author = {Sridhar, Vivek H. and Li, Liang and Gorbonos, Dan and Nagy, Máté and Schell, Bianca R. and Sorochkin, Timothy and Gov, Nir S. and Couzin, Iain D.},
  date = {2021-12-14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {118},
  number = {50},
  eprint = {34880130},
  eprinttype = {pmid},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2102157118},
  url = {https://www.pnas.org/content/118/50/e2102157118},
  urldate = {2021-12-21},
  abstract = {Choosing among spatially distributed options is a central challenge for animals, from deciding among alternative potential food sources or refuges to choosing with whom to associate. Using an integrated theoretical and experimental approach (employing immersive virtual reality), we consider the interplay between movement and vectorial integration during decision-making regarding two, or more, options in space. In computational models of this process, we reveal the occurrence of spontaneous and abrupt “critical” transitions (associated with specific geometrical relationships) whereby organisms spontaneously switch from averaging vectorial information among, to suddenly excluding one among, the remaining options. This bifurcation process repeats until only one option—the one ultimately selected—remains. Thus, we predict that the brain repeatedly breaks multichoice decisions into a series of binary decisions in space–time. Experiments with fruit flies, desert locusts, and larval zebrafish reveal that they exhibit these same bifurcations, demonstrating that across taxa and ecological contexts, there exist fundamental geometric principles that are essential to explain how, and why, animals move the way they do.},
  langid = {english},
  keywords = {collective behavior,embodied choice,movement ecology,navigation,ring attractor},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HFKWQ7SQ/Sridhar et al. - 2021 - The geometry of decision-making in individuals and.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/QQHEN48I/e2102157118.html}
}

@article{stachenfeldHippocampusPredictiveMap2017,
  title = {The Hippocampus as a Predictive Map},
  author = {Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman, Samuel J},
  date = {2017-11-01},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {20},
  number = {11},
  pages = {1643--1653},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4650},
  url = {http://www.nature.com/articles/nn.4650},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/85KYGYY3/Stachenfeld et al. - 2017 - The hippocampus as a predictive map.pdf}
}

@article{starkweatherDopamineRewardPrediction2017,
  title = {Dopamine Reward Prediction Errors Reflect Hidden-State Inference across Time},
  author = {Starkweather, Clara Kwon and Babayan, Benedicte M. and Uchida, Naoshige and Gershman, Samuel J.},
  date = {2017-04},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {20},
  number = {4},
  pages = {581--589},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4520},
  url = {https://www.nature.com/articles/nn.4520},
  urldate = {2022-04-25},
  abstract = {A long-standing idea in modern neuroscience is that the brain computes inferences about the outside world rather than passively observing its environment. The authors record from midbrain dopamine neurons during tasks with different reward contingencies and show that responses are consistent with a learning rule that harnesses hidden-state inference.},
  issue = {4},
  langid = {english},
  keywords = {Learning algorithms,Reward},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XYFDFNII/Starkweather et al. - 2017 - Dopamine reward prediction errors reflect hidden-s.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/D8KI6EZY/nn.html}
}

@article{stoianovHippocampalFormationHierarchical2022,
  title = {The Hippocampal Formation as a Hierarchical Generative Model Supporting Generative Replay and Continual Learning},
  author = {Stoianov, Ivilin and Maisto, Domenico and Pezzulo, Giovanni},
  date = {2022-10-01},
  journaltitle = {Progress in Neurobiology},
  shortjournal = {Progress in Neurobiology},
  volume = {217},
  pages = {102329},
  issn = {0301-0082},
  doi = {10.1016/j.pneurobio.2022.102329},
  url = {https://www.sciencedirect.com/science/article/pii/S0301008222001150},
  urldate = {2022-08-10},
  abstract = {We advance a novel computational theory of the hippocampal formation as a hierarchical generative model that organizes sequential experiences, such as rodent trajectories during spatial navigation, into coherent spatiotemporal contexts. We propose that the hippocampal generative model is endowed with inductive biases to identify individual items of experience (first hierarchical layer), organize them into sequences (second layer) and cluster them into maps (third layer). This theory entails a novel characterization of hippocampal reactivations as generative replay: the offline resampling of fictive sequences from the generative model, which supports the continual learning of multiple sequential experiences. We show that the model learns and efficiently retains multiple spatial navigation trajectories, by organizing them into spatial maps. Furthermore, the model reproduces flexible and prospective aspects of hippocampal dynamics that are challenging to explain within existing frameworks. This theory reconciles multiple roles of the hippocampal formation in map-based navigation, episodic memory and imagination.},
  langid = {english},
  keywords = {Cognitive map,Generative model,Generative replay,Hippocampus,Sequence generation; continual learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/MXUR8U77/Stoianov et al. - 2022 - The hippocampal formation as a hierarchical genera.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/VJJLDSGG/S0301008222001150.html}
}

@article{strauchPupillometryIntegratedReadout2022,
  title = {Pupillometry as an Integrated Readout of Distinct Attentional Networks},
  author = {Strauch, Christoph and Wang, Chin-An and Einhäuser, Wolfgang and Van der Stigchel, Stefan and Naber, Marnix},
  date = {2022-06-01},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2022.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0166223622000972},
  urldate = {2022-06-07},
  abstract = {The course of pupillary constriction and dilation provides an easy-to-access, inexpensive, and noninvasive readout of brain activity. We propose a new taxonomy of factors affecting the pupil and link these to associated neural underpinnings in an ascending hierarchy. In addition to two well-established low-level factors (light level and focal distance), we suggest two further intermediate-level factors, alerting and orienting, and a higher-level factor, executive functioning. Alerting, orienting, and executive functioning – including their respective underlying neural circuitries – overlap with the three principal attentional networks, making pupil size an integrated readout of distinct states of attention. As a now widespread technique, pupillometry is ready to provide meaningful applications and constitutes a viable part of the psychophysiological toolbox.},
  langid = {english},
  keywords = {alerting,executive function,locus coeruleus,orienting,superior colliculus},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/YICQLP8E/S0166223622000972.html}
}

@article{strauchPupillometryIntegratedReadout2022a,
  title = {Pupillometry as an Integrated Readout of Distinct Attentional Networks},
  author = {Strauch, Christoph and Wang, Chin-An and Einhäuser, Wolfgang and Van der Stigchel, Stefan and Naber, Marnix},
  date = {2022-06},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  pages = {S0166223622000972},
  issn = {01662236},
  doi = {10.1016/j.tins.2022.05.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223622000972},
  urldate = {2022-06-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/M4LUNYG7/Strauch et al. - 2022 - Pupillometry as an integrated readout of distinct .pdf}
}

@article{stringerHighdimensionalGeometryPopulation2019,
  title = {High-Dimensional Geometry of Population Responses in Visual Cortex},
  author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
  date = {2019-07},
  journaltitle = {Nature},
  volume = {571},
  number = {7765},
  pages = {361--365},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1346-5},
  url = {https://www.nature.com/articles/s41586-019-1346-5},
  urldate = {2022-01-13},
  abstract = {A neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncorrelated, and most robustly when they are lower-dimensional and correlated. Here we analysed the dimensionality of the encoding of natural images by large populations of neurons in the visual cortex of awake mice. The evoked population activity was high-dimensional, and correlations obeyed an unexpected power law: the nth principal component variance scaled as 1/n. This scaling was not inherited from the power law spectrum of natural images, because it persisted after stimulus whitening. We proved mathematically that if the variance spectrum was to decay more slowly then the population code could not be smooth, allowing small changes in input to dominate population activity. The theory also predicts larger power-law exponents for lower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness may represent a fundamental constraint that determines correlations in neural population codes.},
  issue = {7765},
  langid = {english},
  keywords = {Neural encoding,Striate cortex,To read},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Neural encoding;Striate cortex Subject\_term\_id: neural-encoding;striate-cortex},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/45LPH3DC/Stringer et al. - 2019 - High-dimensional geometry of population responses .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ZH7V8YB7/s41586-019-1346-5.html}
}

@article{stringerSpontaneousBehaviorsDrive2019,
  title = {Spontaneous Behaviors Drive Multidimensional, Brainwide Activity},
  author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Reddy, Charu Bai and Carandini, Matteo and Harris, Kenneth D.},
  date = {2019-04-19},
  journaltitle = {Science},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aav7893},
  url = {https://www.science.org/doi/abs/10.1126/science.aav7893},
  urldate = {2022-01-13},
  abstract = {Neurons in the primary visual cortex encode both visual information and motor activity.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IHWFD6H7/Stringer et al. - 2019 - Spontaneous behaviors drive multidimensional, brai.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/YT4V6GBL/science.html}
}

@article{sucevicNeuralNetworkModel2022,
  title = {A Neural Network Model of Hippocampal Contributions to Category Learning},
  author = {Sučević, Jelena and Schapiro, Anna C.},
  date = {2022-01-13},
  pages = {2022.01.12.476051},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.01.12.476051},
  url = {https://www.biorxiv.org/content/10.1101/2022.01.12.476051v1},
  urldate = {2022-05-04},
  abstract = {In addition to its critical role in encoding individual episodes, the hippocampus is capable of extracting regularities across experiences. This ability is central to category learning, and a growing literature indicates that the hippocampus indeed makes important contributions to this kind of learning. Using a neural network model that mirrors the anatomy of the hippocampus, we investigated the mechanisms by which the hippocampus may support novel category learning. We simulated three category learning paradigms and evaluated the network’s ability to categorize and to recognize specific exemplars in each. We found that the trisynaptic pathway within the hippocampus—connecting entorhinal cortex to dentate gyrus, CA3, and CA1—was critical for remembering individual exemplars, reflecting the rapid binding and pattern separation functions of this circuit. The monosynaptic pathway from entorhinal cortex to CA1, in contrast, was responsible for detecting the regularities that define category structure, made possible by the use of distributed representations and a slower learning rate. Together, the simulations provide an account of how the hippocampus and its constituent pathways support novel category learning.},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HNXQDNGU/Sučević and Schapiro - 2022 - A neural network model of hippocampal contribution.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/ZYP6T2RV/2022.01.12.html}
}

@article{sunHippocampalNeuronsRepresent2020,
  title = {Hippocampal Neurons Represent Events as Transferable Units of Experience},
  author = {Sun, Chen and Yang, Wannan and Martin, Jared and Tonegawa, Susumu},
  date = {2020-05},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {23},
  number = {5},
  pages = {651--663},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-020-0614-x},
  url = {http://www.nature.com/articles/s41593-020-0614-x},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/Q92A9VME/Sun et al. - 2020 - Hippocampal neurons represent events as transferab.pdf}
}

@article{suttonDynaIntegratedArchitecture1991,
  title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
  author = {Sutton, Richard S.},
  date = {1991-07},
  journaltitle = {ACM SIGART Bulletin},
  shortjournal = {SIGART Bull.},
  volume = {2},
  number = {4},
  pages = {160--163},
  issn = {0163-5719},
  doi = {10.1145/122344.122377},
  url = {https://dl.acm.org/doi/10.1145/122344.122377},
  urldate = {2021-12-07},
  abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples--these are among the basic building blocks making up the architecture--yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9IDGAIW3/Sutton - 1991 - Dyna, an integrated architecture for learning, pla.pdf}
}

@unpublished{suttonEmphaticApproachProblem2015,
  title = {An {{Emphatic Approach}} to the {{Problem}} of {{Off-policy Temporal-Difference Learning}}},
  author = {Sutton, Richard S. and Mahmood, A. Rupam and White, Martha},
  date = {2015-04-20},
  eprint = {1503.04269},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1503.04269},
  urldate = {2022-05-02},
  abstract = {In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD(\$\textbackslash lambda\$)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD(\$\textbackslash lambda\$), and GQ(\$\textbackslash lambda\$). Compared to these methods, our \_emphatic TD(\$\textbackslash lambda\$)\_ is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TE5LU6C8/Sutton et al. - 2015 - An Emphatic Approach to the Problem of Off-policy .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/I893U6UY/1503.html}
}

@incollection{suttonIntegratedArchitecturesLearning1990,
  title = {Integrated {{Architectures}} for {{Learning}}, {{Planning}}, and {{Reacting Based}} on {{Approximating Dynamic Programming}}},
  booktitle = {Machine {{Learning Proceedings}} 1990},
  author = {Sutton, Richard S.},
  date = {1990},
  pages = {216--224},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-141-3.50030-4},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558601413500304},
  urldate = {2021-12-07},
  abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins?s Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.},
  isbn = {978-1-55860-141-3},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LDM5E86P/Sutton - 1990 - Integrated Architectures for Learning, Planning, a.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  keywords = {Reinforcement learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8PK78IH6/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@unpublished{swiechowskiMonteCarloTree2021,
  title = {Monte {{Carlo Tree Search}}: {{A Review}} of {{Recent Modifications}} and {{Applications}}},
  shorttitle = {Monte {{Carlo Tree Search}}},
  author = {Świechowski, Maciej and Godlewski, Konrad and Sawicki, Bartosz and Mańdziuk, Jacek},
  date = {2021-03-09},
  eprint = {2103.04931},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.04931},
  urldate = {2022-03-07},
  abstract = {Monte Carlo Tree Search (MCTS) is a powerful approach to designing game-playing bots or solving sequential decision problems. The method relies on intelligent tree search that balances exploration and exploitation. MCTS performs random sampling in the form of simulations and stores statistics of actions to make more educated choices in each subsequent iteration. The method has become a state-of-the-art technique for combinatorial games, however, in more complex games (e.g. those with high branching factor or real-time ones), as well as in various practical domains (e.g. transportation, scheduling or security) an efficient MCTS application often requires its problem-dependent modification or integration with other techniques. Such domain-specific modifications and hybrid approaches are the main focus of this survey. The last major MCTS survey has been published in 2012. Contributions that appeared since its release are of particular interest for this review.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5KI3AZSF/Świechowski et al. - 2021 - Monte Carlo Tree Search A Review of Recent Modifi.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/C5J6E6TT/2103.html}
}

@article{syedActionInitiationShapes2016,
  title = {Action Initiation Shapes Mesolimbic Dopamine Encoding of Future Rewards},
  author = {Syed, Emilie C. J. and Grima, Laura L. and Magill, Peter J. and Bogacz, Rafal and Brown, Peter and Walton, Mark E.},
  date = {2016-01},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {1},
  pages = {34--36},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4187},
  url = {https://www.nature.com/articles/nn.4187},
  urldate = {2022-05-03},
  abstract = {Mesolimbic dopamine has been implicated both in reward prediction and in promoting movement. This study demonstrates that the patterns of dopamine release in the nucleus accumbens core are shaped by the initiation of appropriate reward-guided actions and prospective response accuracy, and not just prediction errors.},
  issue = {1},
  langid = {english},
  keywords = {Motivation,Reward,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/K9CZEDMQ/Syed et al. - 2016 - Action initiation shapes mesolimbic dopamine encod.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/LW63PB92/nn.html}
}

@unpublished{tangExplorationDistributionalReinforcement2018,
  title = {Exploration by {{Distributional Reinforcement Learning}}},
  author = {Tang, Yunhao and Agrawal, Shipra},
  date = {2018-06-21},
  eprint = {1805.01907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.01907},
  urldate = {2021-12-07},
  abstract = {We propose a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that our proposed framework conceptually unifies multiple previous methods in exploration. We also derive a practical algorithm that achieves efficient exploration on challenging control tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/AJP3XWL2/Tang and Agrawal - 2018 - Exploration by Distributional Reinforcement Learni.pdf}
}

@article{tangMultipleTimescaleRepresentationsSpace2022,
  title = {Multiple-{{Timescale Representations}} of {{Space}}: {{Linking Memory}} to {{Navigation}}},
  shorttitle = {Multiple-{{Timescale Representations}} of {{Space}}},
  author = {Tang, Wenbo and Jadhav, Shantanu P.},
  date = {2022},
  journaltitle = {Annual Review of Neuroscience},
  volume = {45},
  number = {1},
  eprint = {34936810},
  eprinttype = {pmid},
  pages = {null},
  doi = {10.1146/annurev-neuro-111020-084824},
  url = {https://doi.org/10.1146/annurev-neuro-111020-084824},
  urldate = {2022-01-11},
  abstract = {When navigating through space, we must maintain a representation of our position in real time; when recalling a past episode, a memory can come back in a flash. Interestingly, the brain's spatial representation system, including the hippocampus, supports these two distinct timescale functions. How are neural representations of space used in the service of both real-world navigation and internal mnemonic processes? Recent progress has identified sequences of hippocampal place cells, evolving at multiple timescales in accordance with either navigational behaviors or internal oscillations, that underlie these functions. We review experimental findings on experience-dependent modulation of these sequential representations and consider how they link real-world navigation to time-compressed memories. We further discuss recent work suggesting the prevalence of these sequences beyond hippocampus and propose that these multiple-timescale mechanisms may represent a general algorithm for organizing cell assemblies, potentially unifying the dual roles of the spatial representation system in memory and navigation. Expected final online publication date for the Annual Review of Neuroscience, Volume 45 is July 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-neuro-111020-084824}
}

@article{tangMultipleTimescaleRepresentationsSpace2022a,
  title = {Multiple-{{Timescale Representations}} of {{Space}}: {{Linking Memory}} to {{Navigation}}},
  shorttitle = {Multiple-{{Timescale Representations}} of {{Space}}},
  author = {Tang, Wenbo and Jadhav, Shantanu P.},
  date = {2022-07-08},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {45},
  number = {1},
  pages = {1--21},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-111020-084824},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-111020-084824},
  urldate = {2022-09-28},
  abstract = {When navigating through space, we must maintain a representation of our position in real time; when recalling a past episode, a memory can come back in a flash. Interestingly, the brain’s spatial representation system, including the hippocampus, supports these two distinct timescale functions. How are neural representations of space used in the service of both real-world navigation and internal mnemonic processes? Recent progress has identified sequences of hippocampal place cells, evolving at multiple timescales in accordance with either navigational behaviors or internal oscillations, that underlie these functions. We review experimental findings on experiencedependent modulation of these sequential representations and consider how they link real-world navigation to time-compressed memories. We further discuss recent work suggesting the prevalence of these sequences beyond hippocampus and propose that these multiple-timescale mechanisms may represent a general algorithm for organizing cell assemblies, potentially unifying the dual roles of the spatial representation system in memory and navigation.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/APTW88S7/Tang and Jadhav - 2022 - Multiple-Timescale Representations of Space Linki.pdf}
}

@article{tangMultipleTimescalesDecisionmaking2021a,
  title = {Multiple Time-Scales of Decision-Making in the Hippocampus and Prefrontal Cortex},
  author = {Tang, Wenbo and Shin, Justin D and Jadhav, Shantanu P},
  editor = {Colgin, Laura L},
  date = {2021-03-08},
  journaltitle = {eLife},
  volume = {10},
  pages = {e66227},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.66227},
  url = {https://doi.org/10.7554/eLife.66227},
  urldate = {2022-01-11},
  abstract = {The prefrontal cortex and hippocampus are crucial for memory-guided decision-making. Neural activity in the hippocampus exhibits place-cell sequences at multiple timescales, including slow behavioral sequences (\textasciitilde seconds) and fast theta sequences (\textasciitilde 100–200 ms) within theta oscillation cycles. How prefrontal ensembles interact with hippocampal sequences to support decision-making is unclear. Here, we examined simultaneous hippocampal and prefrontal ensemble activity in rats during learning of a spatial working-memory decision task. We found clear theta sequences in prefrontal cortex, nested within its behavioral sequences. In both regions, behavioral sequences maintained representations of current choices during navigation. In contrast, hippocampal theta sequences encoded alternatives for deliberation and were coordinated with prefrontal theta sequences that predicted upcoming choices. During error trials, these representations were preserved to guide ongoing behavior, whereas replay sequences during inter-trial periods were impaired prior to navigation. These results establish cooperative interaction between hippocampal and prefrontal sequences at multiple timescales for memory-guided decision-making.},
  keywords = {decision making,hippocampus,prefrontal cortex,sharp-wave ripple,Theta oscillation,working memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/8SSKCM6C/Tang et al. - 2021 - Multiple time-scales of decision-making in the hip.pdf}
}

@article{tanoLocalTemporalDifference,
  title = {A {{Local Temporal Difference Code}} for {{Distributional Reinforcement Learning}}},
  author = {Tano, Pablo and Dayan, Peter and Pouget, Alexandre},
  pages = {12},
  abstract = {Recent theoretical and experimental results suggest that the dopamine system implements distributional temporal difference backups, allowing learning of the entire distributions of the long-run values of states rather than just their expected values. However, the distributional codes explored so far rely on a complex imputation step which crucially relies on spatial non-locality: in order to compute reward prediction errors, units must know not only their own state but also the states of the other units. It is far from clear how these steps could be implemented in realistic neural circuits. Here, we introduce the Laplace code: a local temporal difference code for distributional reinforcement learning that is representationally powerful and computationally straightforward. The code decomposes value distributions and prediction errors across three separated dimensions: reward magnitude (related to distributional quantiles), temporal discounting (related to the Laplace transform of future rewards) and time horizon (related to eligibility traces). Besides lending itself to a local learning rule, the decomposition recovers the temporal evolution of the immediate reward distribution, indicating all possible rewards at all future times. This increases representational capacity and allows for temporally-flexible computations that immediately adjust to changing horizons or discount factors.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/H7ABLGQP/Tano et al. - A Local Temporal Difference Code for Distributiona.pdf}
}

@article{tanoLocalTemporalDifferencea,
  title = {A {{Local Temporal Difference Code}} for {{Distributional Reinforcement Learning}}},
  author = {Tano, Pablo and Dayan, Peter and Pouget, Alexandre},
  pages = {12},
  abstract = {Recent theoretical and experimental results suggest that the dopamine system implements distributional temporal difference backups, allowing learning of the entire distributions of the long-run values of states rather than just their expected values. However, the distributional codes explored so far rely on a complex imputation step which crucially relies on spatial non-locality: in order to compute reward prediction errors, units must know not only their own state but also the states of the other units. It is far from clear how these steps could be implemented in realistic neural circuits. Here, we introduce the Laplace code: a local temporal difference code for distributional reinforcement learning that is representationally powerful and computationally straightforward. The code decomposes value distributions and prediction errors across three separated dimensions: reward magnitude (related to distributional quantiles), temporal discounting (related to the Laplace transform of future rewards) and time horizon (related to eligibility traces). Besides lending itself to a local learning rule, the decomposition recovers the temporal evolution of the immediate reward distribution, indicating all possible rewards at all future times. This increases representational capacity and allows for temporally-flexible computations that immediately adjust to changing horizons or discount factors.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NZ9YGI43/Tano et al. - A Local Temporal Difference Code for Distributiona.pdf}
}

@article{tavaresMapSocialNavigation2015,
  title = {A {{Map}} for {{Social Navigation}} in the {{Human Brain}}},
  author = {Tavares, Rita~Morais and Mendelsohn, Avi and Grossman, Yael and Williams, Christian~Hamilton and Shapiro, Matthew and Trope, Yaacov and Schiller, Daniela},
  date = {2015-07},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {87},
  number = {1},
  pages = {231--243},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.06.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315005243},
  urldate = {2021-12-07},
  abstract = {Deciphering the neural mechanisms of social behavior has propelled the growth of social neuroscience. The exact computations of the social brain, however, remain elusive. Here we investigated how the human brain tracks ongoing changes in social relationships using functional neuroimaging. Participants were lead characters in a role-playing game in which they were to find a new home and a job through interactions with virtual cartoon characters. We found that a twodimensional geometric model of social relationships, a ‘‘social space’’ framed by power and affiliation, predicted hippocampal activity. Moreover, participants who reported better social skills showed stronger covariance between hippocampal activity and ‘‘movement’’ through ‘‘social space.’’ The results suggest that the hippocampus is crucial for social cognition, and imply that beyond framing physical locations, the hippocampus computes a more general, inclusive, abstract, and multidimensional cognitive map consistent with its role in episodic memory.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/6HNRQMJJ/Tavares et al. - 2015 - A Map for Social Navigation in the Human Brain.pdf}
}

@article{teeWHEREMEMORYINFORMATION,
  title = {{{WHERE IS MEMORY INFORMATION STORED IN THE BRAIN}}?},
  author = {Tee, James and Taylor, Desmond P},
  pages = {26},
  abstract = {Within the scientific research community, memory information in the brain is commonly believed to be stored in the synapse – a hypothesis famously attributed to psychologist Donald Hebb. However, there is a growing minority who postulate that memory is stored inside the neuron at the molecular (RNA or DNA) level – an alternative postulation known as the cellintrinsic hypothesis, coined by psychologist Randy Gallistel. In this paper, we review a selection of key experimental evidence from both sides of the argument. We begin with Eric Kandel’s studies on sea slugs, which provided the first evidence in support of the synaptic hypothesis. Next, we touch on experiments in mice by John O’Keefe (declarative memory and the hippocampus) and Joseph LeDoux (procedural fear memory and the amygdala). Then, we introduce the synapse as the basic building block of today’s artificial intelligence neural networks. After that, we describe David Glanzman’s study on dissociating memory storage and synaptic change in sea slugs, and Susumu Tonegawa’s experiment on reactivating retrograde amnesia in mice using laser. From there, we highlight Germund Hesslow’s experiment on conditioned pauses in ferrets, and Beatrice Gelber’s experiment on conditioning in single-celled organisms without synapses (Paramecium aurelia). This is followed by a description of David Glanzman’s experiment on transplanting memory between sea slugs using RNA. Finally, we provide an overview of Brian Dias and Kerry Ressler’s experiment on DNA transfer of fear in mice from parents to offspring. We conclude with some potential implications for the wider field of psychology.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZUCD2X9P/Tee and Taylor - WHERE IS MEMORY INFORMATION STORED IN THE BRAIN.pdf}
}

@article{teradaAdaptiveStimulusSelection2022,
  title = {Adaptive Stimulus Selection for Consolidation in the Hippocampus},
  author = {Terada, Satoshi and Geiller, Tristan and Liao, Zhenrui and O’Hare, Justin and Vancura, Bert and Losonczy, Attila},
  date = {2022-01-13},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {601},
  number = {7892},
  pages = {240--244},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-04118-6},
  url = {https://www.nature.com/articles/s41586-021-04118-6},
  urldate = {2022-03-02},
  langid = {english},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KXEHUTNC/Terada et al. - 2022 - Adaptive stimulus selection for consolidation in t.pdf}
}

@book{terencetaoAnalysis,
  title = {Analysis {{I}}},
  author = {{Terence Tao}},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/7SGQT2YU/_.pdf}
}

@article{thompsonLikelihoodThatOne1933,
  title = {On the {{Likelihood}} That {{One Unknown Probability Exceeds Another}} in {{View}} of the {{Evidence}} of {{Two Samples}}},
  author = {Thompson, William R.},
  date = {1933-12},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {25},
  number = {3/4},
  eprint = {2332286},
  eprinttype = {jstor},
  pages = {285},
  issn = {00063444},
  doi = {10.2307/2332286},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3EZY5Y45/Thompson - 1933 - On the Likelihood that One Unknown Probability Exc.pdf}
}

@article{thrunCientExplorationReinforcement,
  title = {E Cient {{Exploration In Reinforcement Learning}}},
  author = {Thrun, Sebastian B},
  pages = {44},
  abstract = {Exploration plays a fundamental role in any active learning system. This study evaluates the role of exploration in active learning and describes several local techniques for exploration in nite, discrete domains, embedded in a reinforcement learning framework (delayed reinforcement).},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/4F9AJ2MG/Thrun - E cient Exploration In Reinforcement Learning.pdf}
}

@article{tingleyTransformationSpatialMap2018,
  title = {Transformation of a {{Spatial Map}} across the {{Hippocampal-Lateral Septal Circuit}}},
  author = {Tingley, David and Buzsáki, György},
  date = {2018-06},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {98},
  number = {6},
  pages = {1229-1242.e5},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.04.028},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318303362},
  urldate = {2021-12-08},
  abstract = {The hippocampus constructs a map of the environment. How this ‘‘cognitive map’’ is utilized by other brain regions to guide behavior remains unexplored. To examine how neuronal firing patterns in the hippocampus are transmitted and transformed, we recorded neurons in its principal subcortical target, the lateral septum (LS). We observed that LS neurons carry reliable spatial information in the phase of action potentials, relative to hippocampal theta oscillations, while the firing rates of LS neurons remained uninformative. Furthermore, this spatial phase code had an anatomical microstructure within the LS and was bound to the hippocampal spatial code by synchronous gamma frequency cell assemblies. Using a data-driven model, we show that rate-independent spatial tuning arises through the dynamic weighting of CA1 and CA3 cell assemblies. Our findings demonstrate that transformation of the hippocampal spatial map depends on higher-order theta-dependent neuronal sequences.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/T2E4UUVN/Tingley and Buzsáki - 2018 - Transformation of a Spatial Map across the Hippoca.pdf}
}

@article{todorovaHippocampalRipplesMode2020,
  title = {Hippocampal Ripples as a Mode of Communication with Cortical and Subcortical Areas},
  author = {Todorova, Ralitsa and Zugaro, Michaël},
  date = {2020},
  journaltitle = {Hippocampus},
  volume = {30},
  number = {1},
  pages = {39--49},
  issn = {1098-1063},
  doi = {10.1002/hipo.22997},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.22997},
  urldate = {2022-10-31},
  abstract = {Hippocampal sharp wave-ripple complexes are transient events of highly synchronous neuronal activity that typically occur during “offline” brain states. This endogenous surge of activity consists of behaviorally relevant spiking patterns, describing spatial trajectories. They have been shown to play a critical role in memory consolidation during sleep and in navigational planning during wakefulness. Beyond their local impact on the hippocampal formation, ripples also exert direct and indirect effects on target cortical and subcortical areas, which are thought to play a key role in information processing and semantic network reconfiguration. We review research into the function of hippocampal sharp waves-ripples, with a special focus on information flow between the hippocampus and its cortical and subcortical targets. First, we briefly review seminal work establishing a causal role of ripple-related activity in cognitive processes. We then review evidence for a functional interplay between hippocampal ripples and specific patterns of cortical and subcortical activity. Finally, we discuss the critical role of the functional coupling between ripples and other sleep rhythms, including the cortical slow oscillation and thalamocortical sleep spindles.},
  langid = {english},
  keywords = {learning and memory,memory consolidation,oscillations,replay,sleep},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hipo.22997},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LCHN9KR3/hipo.html}
}

@article{todorovaHippocampalRipplesMode2020a,
  title = {Hippocampal Ripples as a Mode of Communication with Cortical and Subcortical Areas},
  author = {Todorova, Ralitsa and Zugaro, Michaël},
  date = {2020-01},
  journaltitle = {Hippocampus},
  shortjournal = {Hippocampus},
  volume = {30},
  number = {1},
  pages = {39--49},
  issn = {1050-9631, 1098-1063},
  doi = {10.1002/hipo.22997},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/hipo.22997},
  urldate = {2022-10-31},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/A9IFS9RM/Todorova and Zugaro - 2020 - Hippocampal ripples as a mode of communication wit.pdf}
}

@article{todorovEfficientComputationOptimal2009,
  title = {Efficient Computation of Optimal Actions},
  author = {Todorov, Emanuel},
  date = {2009-07-14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {106},
  number = {28},
  eprint = {19574462},
  eprinttype = {pmid},
  pages = {11478--11483},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0710743106},
  url = {https://www.pnas.org/content/106/28/11478},
  urldate = {2022-02-25},
  abstract = {Optimal choice of actions is a fundamental problem relevant to fields as diverse as neuroscience, psychology, economics, computer science, and control engineering. Despite this broad relevance the abstract setting is similar: we have an agent choosing actions over time, an uncertain dynamical system whose state is affected by those actions, and a performance criterion that the agent seeks to optimize. Solving problems of this kind remains hard, in part, because of overly generic formulations. Here, we propose a more structured formulation that greatly simplifies the construction of optimal control laws in both discrete and continuous domains. An exhaustive search over actions is avoided and the problem becomes linear. This yields algorithms that outperform Dynamic Programming and Reinforcement Learning, and thereby solve traditional problems more efficiently. Our framework also enables computations that were not possible before: composing optimal control laws by mixing primitives, applying deterministic methods to stochastic systems, quantifying the benefits of error tolerance, and inferring goals from behavioral data via convex optimization. Development of a general class of easily solvable problems tends to accelerate progress—as linear systems theory has done, for example. Our framework may have similar impact in fields where optimal choice of actions is relevant.},
  isbn = {9780710743107},
  langid = {english},
  keywords = {action selection,cost function,linear Bellman equation,stochastic optimal control},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/G6LVVKMN/Todorov - 2009 - Efficient computation of optimal actions.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/X8EKQKFV/11478.html}
}

@article{tolmanCognitiveMapsRats1948,
  title = {Cognitive Maps in Rats and Men.},
  author = {Tolman, Edward C.},
  date = {1948},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {55},
  number = {4},
  pages = {189--208},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0061626},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0061626},
  urldate = {2022-05-11},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9XGBEVG8/Tolman - 1948 - Cognitive maps in rats and men..pdf}
}

@article{tolpinMCTSBasedSimple2012,
  title = {{{MCTS Based}} on {{Simple Regret}}},
  author = {Tolpin, David and Shimony, Solomon},
  date = {2012},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {26},
  number = {1},
  pages = {570--576},
  issn = {2374-3468},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/8126},
  urldate = {2022-03-02},
  abstract = {UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final ``arm pull'' (the actual move selection) that collects a reward, rather than all ``arm pulls''. Therefore, it makes more sense to minimize the simple regret, as opposed to the cumulative regret. We begin by introducing policies for multi-armed bandits with lower finite-time and asymptotic simple regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms UCT empirically. Optimizing the sampling process is itself a metareasoning problem, a solution of which can use value of information (VOI) techniques. Although the theory of VOI for search exists, applying it to MCTS is non-trivial, as typical myopic assumptions fail. Lacking a complete working VOI theory for MCTS, we nevertheless propose a sampling scheme that is ``aware'' of VOI, achieving an algorithm that in empirical evaluation outperforms both UCT and the other proposed algorithms.},
  issue = {1},
  langid = {english},
  keywords = {To read,VOI},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/HZ6TFB5A/Tolpin and Shimony - 2012 - MCTS Based on Simple Regret.pdf}
}

@article{topolnikRoleInhibitoryCircuits2022,
  title = {The Role of Inhibitory Circuits in Hippocampal Memory Processing},
  author = {Topolnik, Lisa and Tamboli, Suhel},
  date = {2022-05-30},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  pages = {1--17},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00599-0},
  url = {https://www.nature.com/articles/s41583-022-00599-0},
  urldate = {2022-06-30},
  abstract = {GABAergic inhibitory circuits play an essential role in coordinating various hippocampal functions. Several decades of work dedicated to a thorough characterization of hippocampal inhibitory populations have highlighted how specific types of interneuron can contribute to network activity. Recent studies have used genetically targeted recordings and peturbations of activity during memory-related behaviours to determine how interneurons that inhibit distinct subcellular domains of principal cells or specialize in principal cell disinhibition may sculpt hippocampal memory. These studies highlight unique contributions of distinct interneuron types to the temporal binding of hippocampal ensembles, synaptic plasticity and the acquisition of spatial and contextual information. Here, we review the current state of knowledge around hippocampal inhibition and memory by discussing the multifaceted roles of populations of inhibitory cells at different stages of hippocampal mnemonic processing.},
  langid = {english},
  keywords = {Hippocampus,Neural circuits},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/ZNXV23E5/Topolnik and Tamboli - 2022 - The role of inhibitory circuits in hippocampal mem.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/MSZ8PIKZ/s41583-022-00599-0.html}
}

@article{tsaoIntegratingTimeExperience2018,
  title = {Integrating Time from Experience in the Lateral Entorhinal Cortex},
  author = {Tsao, Albert and Sugar, Jørgen and Lu, Li and Wang, Cheng and Knierim, James J. and Moser, May-Britt and Moser, Edvard I.},
  date = {2018-09},
  journaltitle = {Nature},
  volume = {561},
  number = {7721},
  pages = {57--62},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0459-6},
  url = {https://www.nature.com/articles/s41586-018-0459-6.},
  urldate = {2022-04-13},
  abstract = {The encoding of time and its binding to events are crucial for episodic memory, but how these processes are carried out in hippocampal–entorhinal circuits is unclear. Here we show in freely foraging rats that temporal information is robustly encoded across time scales from seconds to hours within the overall population state of the lateral entorhinal cortex. Similarly pronounced encoding of time was not present in the medial entorhinal cortex or in hippocampal areas CA3–CA1. When animals’ experiences were constrained by behavioural tasks to become similar across repeated trials, the encoding of temporal flow across trials was reduced, whereas the encoding of time relative to the start of trials was improved. The findings suggest that populations of lateral entorhinal cortex neurons represent time inherently through the encoding of experience. This representation of episodic time may be integrated with spatial inputs from the medial entorhinal cortex in the hippocampus, allowing the hippocampus to store a unified representation of what, where and when.},
  issue = {7721},
  langid = {english},
  keywords = {Hippocampus,Neural encoding,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/RBX559EA/Tsao et al. - 2018 - Integrating time from experience in the lateral en.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/N7AXP349/s41586-018-0459-6..html}
}

@article{vandermeerInformationProcessingDecisionMaking2012,
  title = {Information {{Processing}} in {{Decision-Making Systems}}},
  author = {van der Meer, Matthijs and Kurth-Nelson, Zeb and Redish, A. David},
  options = {useprefix=true},
  date = {2012-08-01},
  journaltitle = {The Neuroscientist},
  shortjournal = {Neuroscientist},
  volume = {18},
  number = {4},
  pages = {342--359},
  publisher = {{SAGE Publications Inc STM}},
  issn = {1073-8584},
  doi = {10.1177/1073858411435128},
  url = {https://doi.org/10.1177/1073858411435128},
  urldate = {2022-04-05},
  abstract = {Decisions result from an interaction between multiple functional systems acting in parallel to process information in very different ways, each with strengths and weaknesses. In this review, the authors address three action-selection components of decision-making: The Pavlovian system releases an action from a limited repertoire of potential actions, such as approaching learned stimuli. Like the Pavlovian system, the habit system is computationally fast but, unlike the Pavlovian system permits arbitrary stimulus-action pairings. These associations are a “forward’’ mechanism; when a situation is recognized, the action is released. In contrast, the deliberative system is flexible but takes time to process. The deliberative system uses knowledge of the causal structure of the world to search into the future, planning actions to maximize expected rewards. Deliberation depends on the ability to imagine future possibilities, including novel situations, and it allows decisions to be taken without having previously experienced the options. Various anatomical structures have been identified that carry out the information processing of each of these systems: hippocampus constitutes a map of the world that can be used for searching/imagining the future; dorsal striatal neurons represent situation-action associations; and ventral striatum maintains value representations for all three systems. Each system presents vulnerabilities to pathologies that can manifest as psychiatric disorders. Understanding these systems and their relation to neuroanatomy opens up a deeper way to treat the structural problems underlying various disorders.},
  langid = {english},
  keywords = {decision making,dorsal striatum,hippocampus,rat,To read,ventral striatum},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BN4E3LEM/van der Meer et al. - 2012 - Information Processing in Decision-Making Systems.pdf}
}

@article{vandermeerProgressIssuesSecondorder2020,
  title = {Progress and Issues in Second-Order Analysis of Hippocampal Replay},
  author = {van der Meer, Matthijs A. A. and Kemere, Caleb and Diba, Kamran},
  options = {useprefix=true},
  date = {2020-05-25},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {375},
  number = {1799},
  pages = {20190238},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2019.0238},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rstb.2019.0238},
  urldate = {2022-08-12},
  abstract = {Patterns of neural activity that occur spontaneously during sharp-wave ripple (SWR) events in the hippocampus are thought to play an important role in memory formation, consolidation and retrieval. Typical studies examining the content of SWRs seek to determine whether the identity and/or temporal order of cell firing is different from chance. Such ‘first-order’ analyses are focused on a single time point and template (map), and have been used to show, for instance, the existence of preplay. The major methodological challenge in first-order analyses is the construction and interpretation of different chance distributions. By contrast, ‘second-order’ analyses involve a comparison of SWR content between different time points, and/or between different templates. Typical second-order questions include tests of experience-dependence (replay) that compare SWR content before and after experience, and comparisons or replay between different arms of a maze. Such questions entail additional methodological challenges that can lead to biases in results and associated interpretations. We provide an inventory of analysis challenges for second-order questions about SWR content, and suggest ways of preventing, identifying and addressing possible analysis biases. Given evolving interest in understanding SWR content in more complex experimental scenarios and across different time scales, we expect these issues to become increasingly pervasive. This article is part of the Theo Murphy meeting issue ‘Memory reactivation: replaying events past, present and future’.},
  keywords = {decoding,reactivation,replay,sequence analysis,sharp-wave ripple},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/QFHSMGRM/van der Meer et al. - 2020 - Progress and issues in second-order analysis of hi.pdf}
}

@inproceedings{vanhasseltWhenUseParametric2019,
  title = {When to Use Parametric Models in Reinforcement Learning?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {van Hasselt, Hado P and Hessel, Matteo and Aslanides, John},
  options = {useprefix=true},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/1b742ae215adf18b75449c6e272fd92d-Abstract.html},
  urldate = {2022-05-02},
  abstract = {We examine the question of when and how parametric models are most useful in reinforcement learning.  In particular, we look at commonalities and differences between parametric models and experience replay.  Replay-based learning algorithms share important traits with model-based approaches, including the ability to plan: to use more computation without additional data to improve predictions and behaviour. We discuss when to expect benefits from either approach, and interpret prior work in this context. We hypothesise that, under suitable conditions, replay-based algorithms should be competitive to or better than model-based algorithms if the model is used only to generate fictional transitions from observed states for an update rule that is otherwise model-free. We validated this hypothesis on Atari 2600 video games. The replay-based algorithm attained state-of-the-art data efficiency, improving over prior results with parametric models. Additionally, we discuss different ways to use models. We show that it can be better to plan backward than to plan forward when using models to perform credit assignment (e.g., to directly learn a value or policy), even though the latter seems more common.  Finally, we argue and demonstrate that it can be beneficial to plan forward for immediate behaviour, rather than for credit assignment.},
  keywords = {To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CDLE2EEA/van Hasselt et al. - 2019 - When to use parametric models in reinforcement lea.pdf}
}

@article{vikbladhHippocampalContributionsModelBased2019,
  title = {Hippocampal {{Contributions}} to {{Model-Based Planning}} and {{Spatial Memory}}},
  author = {Vikbladh, Oliver M. and Meager, Michael R. and King, John and Blackmon, Karen and Devinsky, Orrin and Shohamy, Daphna and Burgess, Neil and Daw, Nathaniel D.},
  date = {2019-05-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {102},
  number = {3},
  pages = {683-693.e4},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.02.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627319301230},
  urldate = {2022-09-21},
  abstract = {Little is known about the neural mechanisms that allow humans and animals to plan actions using knowledge of task contingencies. Emerging theories hypothesize that it involves the same hippocampal mechanisms that support self-localization and memory for locations. Yet limited direct evidence supports the link between planning and the hippocampal place map. We addressed this by investigating model-based planning and place memory in healthy controls and epilepsy patients treated using unilateral anterior temporal lobectomy with hippocampal resection. Both functions were impaired in the patient group. Specifically, the planning impairment was related to right hippocampal lesion size, controlling for overall lesion size. Furthermore, although planning and boundary-driven place memory covaried in the control group, this relationship was attenuated in patients, consistent with both functions relying on the same structure in the healthy brain. These findings clarify both the neural mechanism of model-based planning and the scope of hippocampal contributions to behavior.},
  langid = {english},
  keywords = {anterior temporal lobe,decision-making,hippocampus,human,lesion,memory,model-based,planning,reinforcement learning,spatial},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BG6LB5EL/Vikbladh et al. - 2019 - Hippocampal Contributions to Model-Based Planning .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/8HD8IQJQ/S0896627319301230.html}
}

@article{vonluxburgTutorialSpectralClustering2007,
  title = {A Tutorial on Spectral Clustering},
  author = {von Luxburg, Ulrike},
  options = {useprefix=true},
  date = {2007-12},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {17},
  number = {4},
  pages = {395--416},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-007-9033-z},
  url = {http://link.springer.com/10.1007/s11222-007-9033-z},
  urldate = {2021-12-11},
  abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3T557LYB/von Luxburg - 2007 - A tutorial on spectral clustering.pdf}
}

@article{wangAlternatingSequencesFuture2020,
  title = {Alternating Sequences of Future and Past Behavior Encoded within Hippocampal Theta Oscillations},
  author = {Wang, Mengni and Foster, David J. and Pfeiffer, Brad E.},
  date = {2020-10-09},
  journaltitle = {Science},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.abb4151},
  url = {https://www.science.org/doi/abs/10.1126/science.abb4151},
  urldate = {2022-01-11},
  abstract = {Hippocampal place cell firing sequences represent future and past experiences.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/52NCY2HR/Wang et al. - 2020 - Alternating sequences of future and past behavior .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/EPDY79MH/science.html}
}

@unpublished{wangExploringModelbasedPlanning2019,
  title = {Exploring {{Model-based Planning}} with {{Policy Networks}}},
  author = {Wang, Tingwu and Ba, Jimmy},
  date = {2019-06-20},
  eprint = {1906.08649},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.08649},
  urldate = {2021-12-07},
  abstract = {Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in terms of both sample efficiency and asymptotic performance. Despite their initial successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released in https://github.com/WilsonWangTHU/POPLIN.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/XHM64LZ2/Wang and Ba - 2019 - Exploring Model-based Planning with Policy Network.pdf}
}

@article{wanjiaAbruptHippocampalRemapping2021,
  title = {Abrupt Hippocampal Remapping Signals Resolution of Memory Interference},
  author = {Wanjia, Guo and Favila, Serra E. and Kim, Ghootae and Molitor, Robert J. and Kuhl, Brice A.},
  date = {2021-08-10},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {4816},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25126-0},
  url = {https://www.nature.com/articles/s41467-021-25126-0},
  urldate = {2022-09-20},
  abstract = {Remapping refers to a decorrelation of hippocampal representations of similar spatial environments. While it has been speculated that remapping may contribute to the resolution of episodic memory interference in humans, direct evidence is surprisingly limited. We tested this idea using high-resolution, pattern-based fMRI analyses. Here we show that activity patterns in human CA3/dentate gyrus exhibit an abrupt, temporally-specific decorrelation of highly similar memory representations that is precisely coupled with behavioral expressions of successful learning. The magnitude of this learning-related decorrelation was predicted by the amount of pattern overlap during initial stages of learning, with greater initial overlap leading to stronger decorrelation. Finally, we show that remapped activity patterns carry relatively more information about learned episodic associations compared to competing associations, further validating the learning-related significance of remapping. Collectively, these findings establish a critical link between hippocampal remapping and episodic memory interference and provide insight into why remapping occurs.},
  issue = {1},
  langid = {english},
  keywords = {Cognitive neuroscience,Hippocampus,Human behaviour,Long-term memory},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KIE4EQ23/Wanjia et al. - 2021 - Abrupt hippocampal remapping signals resolution of.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/IVDUJF43/s41467-021-25126-0.html}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  url = {https://doi.org/10.1007/BF00992698},
  urldate = {2022-02-14},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/9D48WN9G/Watkins and Dayan - 1992 - Q-learning.pdf}
}

@article{watkinsQlearning1992a,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  url = {https://doi.org/10.1007/BF00992698},
  urldate = {2022-10-05},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  keywords = {asynchronous dynamic programming,Q-learning,reinforcement learning,temporal differences},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/KNNCIXBB/Watkins and Dayan - 1992 - Q-learning.pdf}
}

@unpublished{whittingtonHowBuildCognitive2022a,
  title = {How to Build a Cognitive Map: Insights from Models of the Hippocampal Formation},
  shorttitle = {How to Build a Cognitive Map},
  author = {Whittington, James C. R. and McCaffary, David and Bakermans, Jacob J. W. and Behrens, Timothy E. J.},
  date = {2022-02-03},
  eprint = {2202.01682},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2202.01682},
  urldate = {2022-02-07},
  abstract = {Learning and interpreting the structure of the environment is an innate feature of biological systems, and is integral to guiding flexible behaviours for evolutionary viability. The concept of a cognitive map has emerged as one of the leading metaphors for these capacities, and unravelling the learning and neural representation of such a map has become a central focus of neuroscience. While experimentalists are providing a detailed picture of the neural substrate of cognitive maps in hippocampus and beyond, theorists have been busy building models to bridge the divide between neurons, computation, and behaviour. These models can account for a variety of known representations and neural phenomena, but often provide a differing understanding of not only the underlying principles of cognitive maps, but also the respective roles of hippocampus and cortex. In this Perspective, we bring many of these models into a common language, distil their underlying principles of constructing cognitive maps, provide novel (re)interpretations for neural phenomena, suggest how the principles can be extended to account for prefrontal cortex representations and, finally, speculate on the role of cognitive maps in higher cognitive capacities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FFZRUBSX/Whittington et al. - 2022 - How to build a cognitive map insights from models.pdf}
}

@article{whittingtonTolmanEichenbaumMachineUnifying2020,
  title = {The {{Tolman-Eichenbaum Machine}}: {{Unifying Space}} and {{Relational Memory}} through {{Generalization}} in the {{Hippocampal Formation}}},
  shorttitle = {The {{Tolman-Eichenbaum Machine}}},
  author = {Whittington, James C.R. and Muller, Timothy H. and Mark, Shirley and Chen, Guifen and Barry, Caswell and Burgess, Neil and Behrens, Timothy E.J.},
  date = {2020-11},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {183},
  number = {5},
  pages = {1249-1263.e23},
  issn = {00928674},
  doi = {10.1016/j.cell.2020.10.024},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S009286742031388X},
  urldate = {2021-12-07},
  abstract = {The hippocampal-entorhinal system is important for spatial and relational memory tasks. We formally link these domains, provide a mechanistic understanding of the hippocampal role in generalization, and offer unifying principles underlying many entorhinal and hippocampal cell types. We propose medial entorhinal cells form a basis describing structural knowledge, and hippocampal cells link this basis with sensory representations. Adopting these principles, we introduce the Tolman-Eichenbaum machine (TEM). After learning, TEM entorhinal cells display diverse properties resembling apparently bespoke spatial responses, such as grid, band, border, and object-vector cells. TEM hippocampal cells include place and landmark cells that remap between environments. Crucially, TEM also aligns with empirically recorded representations in complex nonspatial tasks. TEM also generates predictions that hippocampal remapping is not random as previously believed; rather, structural knowledge is preserved across environments. We confirm this structural transfer over remapping in simultaneously recorded place and grid cells.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EABB24T8/Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and .pdf}
}

@article{widloskiFlexibleReroutingHippocampal2022,
  title = {Flexible Rerouting of Hippocampal Replay Sequences around Changing Barriers in the Absence of Global Place Field Remapping},
  author = {Widloski, John and Foster, David J.},
  date = {2022-02-17},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {0},
  number = {0},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2022.02.002},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(22)00109-X},
  urldate = {2022-02-18},
  langid = {english},
  keywords = {adaptation,attractor dynamics,barriers,hippocampus,memory,place cells,replay,sequences,spatial navigation},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/H9R78IQ4/Widloski and Foster - 2022 - Flexible rerouting of hippocampal replay sequences.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/QPG6YNK6/S0896-6273(22)00109-X.html}
}

@article{wikenheiserRiverWoodsCognitive2016,
  title = {Over the River, through the Woods: Cognitive Maps in the Hippocampus and Orbitofrontal Cortex},
  shorttitle = {Over the River, through the Woods},
  author = {Wikenheiser, Andrew M. and Schoenbaum, Geoffrey},
  date = {2016-08},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {17},
  number = {8},
  pages = {513--523},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn.2016.56},
  url = {https://www.nature.com/articles/nrn.2016.56},
  urldate = {2022-06-30},
  abstract = {Recent studies have attributed surprisingly similar functional roles to the orbitofrontal cortex (OFC) and hippocampus. Evidence is presented that both the OFC and hippocampus contribute to 'cognitive mapping', and it is suggested that future work should focus on understanding the functional interactions between these structures.},
  issue = {8},
  langid = {english},
  keywords = {Cognitive control,Cortex,Hippocampus,Prefrontal cortex},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/NZE9QS9L/Wikenheiser and Schoenbaum - 2016 - Over the river, through the woods cognitive maps .pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/4G637M4N/nrn.2016.html}
}

@article{wilsonBalancingExplorationExploitation2021,
  title = {Balancing Exploration and Exploitation with Information and Randomization},
  author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent D and Ebitz, R Becket},
  date = {2021-04},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  volume = {38},
  pages = {49--56},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2020.10.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2352154620301467},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/DBF55ZZ9/Wilson et al. - 2021 - Balancing exploration and exploitation with inform.pdf}
}

@article{wilsonBalancingExplorationExploitation2021a,
  title = {Balancing Exploration and Exploitation with Information and Randomization},
  author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent D and Ebitz, R Becket},
  date = {2021-04-01},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  series = {Computational Cognitive Neuroscience},
  volume = {38},
  pages = {49--56},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2020.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2352154620301467},
  urldate = {2022-09-06},
  abstract = {Explore-exploit decisions require us to trade off the benefits of exploring unknown options to learn more about them, with exploiting known options, for immediate reward. Such decisions are ubiquitous in nature, but from a computational perspective, they are notoriously hard. There is therefore much interest in how humans and animals make these decisions and recently there has been an explosion of research in this area. Here we provide a biased and incomplete snapshot of this field focusing on the major finding that many organisms use two distinct strategies to solve the explore-exploit dilemma: a bias for information (‘directed exploration’) and the randomization of choice (‘random exploration’). We review evidence for the existence of these strategies, their computational properties, their neural implementations, as well as how directed and random exploration vary over the lifespan. We conclude by highlighting open questions in this field that are ripe to both explore and exploit.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/FMJSBS87/Wilson et al. - 2021 - Balancing exploration and exploitation with inform.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/KF9QGYIS/S2352154620301467.html}
}

@article{wilsonDynamicsHippocampalEnsemble1993,
  title = {Dynamics of the {{Hippocampal Ensemble Code}} for {{Space}}},
  author = {Wilson, Matthew A. and McNaughton, Bruce L.},
  date = {1993-08-20},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {261},
  number = {5124},
  pages = {1055--1058},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.8351520},
  url = {https://www.science.org/doi/10.1126/science.8351520},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/3RSIM83R/Wilson and McNaughton - 1993 - Dynamics of the Hippocampal Ensemble Code for Spac.pdf}
}

@article{wilsonReactivationHippocampalEnsemble1994,
  title = {Reactivation of {{Hippocampal Ensemble Memories During Sleep}}},
  author = {Wilson, Matthew A. and McNaughton, Bruce L.},
  date = {1994-07-29},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {265},
  number = {5172},
  pages = {676--679},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.8036517},
  url = {https://www.science.org/doi/10.1126/science.8036517},
  urldate = {2021-12-07},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/LRLCTDVB/Wilson and McNaughton - 1994 - Reactivation of Hippocampal Ensemble Memories Duri.pdf}
}

@report{wimmerDistinctReplaySignatures2021,
  type = {preprint},
  title = {Distinct Replay Signatures for Planning and Memory Maintenance},
  author = {Wimmer, G. Elliott and Liu, Yunzhe and McNamee, Daniel and Dolan, Raymond J.},
  date = {2021-11-10},
  institution = {{Neuroscience}},
  doi = {10.1101/2021.11.08.467745},
  url = {http://biorxiv.org/lookup/doi/10.1101/2021.11.08.467745},
  urldate = {2021-12-07},
  abstract = {Abstract           Theories of neural replay propose that it supports a range of different functions, most prominently planning and memory maintenance. Here, we test the hypothesis that distinct replay signatures relate to planning and memory maintenance. Our reward learning task required human participants to utilize structure knowledge for ‘model-based’ evaluation, while maintaining knowledge for two independent and randomly alternating task environments. Using magnetoencephalography (MEG) and multivariate analysis, we found neural evidence for compressed forward replay during planning and backward replay following reward feedback. Prospective replay strength was enhanced for the current environment when the benefits of a model-based planning strategy were higher. Following reward receipt, backward replay for the alternative, distal environment was enhanced as a function of decreasing recency of experience for that environment. Consistent with a memory maintenance role, stronger maintenance-related replay was associated with a modulation of subsequent choices. These findings identify distinct replay signatures consistent with key theoretical proposals on planning and memory maintenance functions, with their relative strength modulated by on-going computational and task demands.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/64LUQCFJ/Wimmer et al. - 2021 - Distinct replay signatures for planning and memory.pdf}
}

@article{wittkuhnReplayMindsMachines2021,
  title = {Replay in Minds and Machines},
  author = {Wittkuhn, Lennart and Chien, Samson and Hall-McMaster, Sam and Schuck, Nicolas W.},
  date = {2021-10-01},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {129},
  pages = {367--388},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2021.08.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0149763421003444},
  urldate = {2022-04-19},
  abstract = {Experience-related brain activity patterns reactivate during sleep, wakeful rest, and brief pauses from active behavior. In parallel, machine learning research has found that experience replay can lead to substantial performance improvements in artificial agents. Together, these lines of research suggest that replay has a variety of computational benefits for decision-making and learning. Here, we provide an overview of putative computational functions of replay as suggested by machine learning and neuroscientific research. We show that replay can lead to faster learning, less forgetting, reorganization or augmentation of experiences, and support planning and generalization. In addition, we highlight the benefits of reactivating abstracted internal representations rather than veridical memories, and discuss how replay could provide a mechanism to build internal representations that improve learning and decision-making.},
  langid = {english},
  keywords = {Decision-making,Machine learning,Reinforcement learning,Replay,Representation learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/TTYIWRHV/Wittkuhn et al. - 2021 - Replay in minds and machines.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/D8HUDZF9/S0149763421003444.html}
}

@article{wong-linMultiscaleModelingFramework2017,
  title = {Toward a Multiscale Modeling Framework for Understanding Serotonergic Function},
  author = {Wong-Lin, KongFatt and Wang, Da-Hui and Moustafa, Ahmed A and Cohen, Jeremiah Y and Nakamura, Kae},
  date = {2017-09-01},
  journaltitle = {Journal of Psychopharmacology},
  shortjournal = {J Psychopharmacol},
  volume = {31},
  number = {9},
  pages = {1121--1136},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0269-8811},
  doi = {10.1177/0269881117699612},
  url = {https://doi.org/10.1177/0269881117699612},
  urldate = {2022-05-06},
  abstract = {Despite its importance in regulating emotion and mental wellbeing, the complex structure and function of the serotonergic system present formidable challenges toward understanding its mechanisms. In this paper, we review studies investigating the interactions between serotonergic and related brain systems and their behavior at multiple scales, with a focus on biologically-based computational modeling. We first discuss serotonergic intracellular signaling and neuronal excitability, followed by neuronal circuit and systems levels. At each level of organization, we will discuss the experimental work accompanied by related computational modeling work. We then suggest that a multiscale modeling approach that integrates the various levels of neurobiological organization could potentially transform the way we understand the complex functions associated with serotonin.},
  langid = {english},
  keywords = {midbrain raphe nucleus,multiscale computational model,neural circuit,Serotonin 5-HT,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/N4KPPR6P/Wong-Lin et al. - 2017 - Toward a multiscale modeling framework for underst.pdf}
}

@article{wuHippocampalAwakeReplay2017,
  title = {Hippocampal Awake Replay in Fear Memory Retrieval},
  author = {Wu, Chun-Ting and Haggerty, Daniel and Kemere, Caleb and Ji, Daoyun},
  date = {2017-04},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {20},
  number = {4},
  pages = {571--580},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4507},
  url = {https://www.nature.com/articles/nn.4507},
  urldate = {2022-10-05},
  abstract = {How hippocampal place cells participate in fear memory retrieval is unknown. Wu et al. show that, when rats retrieve prior shock experience prompting them to avoid a shock zone, precise place cell activity patterns encoding paths from animals’ current locations to the shock zone are replayed in association with high-frequency ripple oscillations.},
  issue = {4},
  langid = {english},
  keywords = {Hippocampus,Neural circuits},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/BNMPNSNU/Wu et al. - 2017 - Hippocampal awake replay in fear memory retrieval.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/W4TFLJNZ/nn.html}
}

@article{xuNoveltyNotSurprise2021,
  title = {Novelty Is Not Surprise: {{Human}} Exploratory and Adaptive Behavior in Sequential Decision-Making},
  shorttitle = {Novelty Is Not Surprise},
  author = {Xu, He A. and Modirshanechi, Alireza and Lehmann, Marco P. and Gerstner, Wulfram and Herzog, Michael H.},
  date = {2021-06-03},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {17},
  number = {6},
  pages = {e1009070},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009070},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009070},
  urldate = {2022-03-20},
  abstract = {Classic reinforcement learning (RL) theories cannot explain human behavior in the absence of external reward or when the environment changes. Here, we employ a deep sequential decision-making paradigm with sparse reward and abrupt environmental changes. To explain the behavior of human participants in these environments, we show that RL theories need to include surprise and novelty, each with a distinct role. While novelty drives exploration before the first encounter of a reward, surprise increases the rate of learning of a world-model as well as of model-free action-values. Even though the world-model is available for model-based RL, we find that human decisions are dominated by model-free action choices. The world-model is only marginally used for planning, but it is important to detect surprising events. Our theory predicts human action choices with high probability and allows us to dissociate surprise, novelty, and reward in EEG signals.},
  langid = {english},
  keywords = {Algorithms,Decision making,Electroencephalography,Event-related potentials,Learning,Linear regression analysis,Regression analysis,Statistical models},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/5HHGDLQR/Xu et al. - 2021 - Novelty is not surprise Human exploratory and ada.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/U8BFZX8F/article.html}
}

@misc{yangExplorationDeepReinforcement2022,
  title = {Exploration in {{Deep Reinforcement Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Exploration in {{Deep Reinforcement Learning}}},
  author = {Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Hao, Jianye and Meng, Zhaopeng and Liu, Peng and Wang, Zhen},
  date = {2022-07-12},
  number = {arXiv:2109.06668},
  eprint = {2109.06668},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.06668},
  url = {http://arxiv.org/abs/2109.06668},
  urldate = {2022-09-26},
  abstract = {Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning (MARL) have achieved significant successes across a wide range of domains, including game AI, autonomous vehicles, robotics, and so on. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning towards the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and non-stationary co-learners. In this paper, we conduct a comprehensive survey on existing exploration methods for both single-agent and multi-agent RL. We start the survey by identifying several key challenges to efficient exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/CGD86PCY/Yang et al. - 2022 - Exploration in Deep Reinforcement Learning A Comp.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/MKM2X9CV/2109.html}
}

@article{yoonGridCellResponses2016,
  title = {Grid {{Cell Responses}} in {{1D Environments Assessed}} as {{Slices}} through a {{2D Lattice}}},
  author = {Yoon, KiJung and Lewallen, Sam and Kinkhabwala, Amina~A. and Tank, David~W. and Fiete, Ila~R.},
  date = {2016-03},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {89},
  number = {5},
  pages = {1086--1099},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.01.039},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627316000647},
  urldate = {2021-12-07},
  abstract = {Grid cells, defined by their striking periodic spatial responses in open 2D arenas, appear to respond differently on 1D tracks: the multiple response fields are not periodically arranged, peak amplitudes vary across fields, and the mean spacing between fields is larger than in 2D environments. We ask whether such 1D responses are consistent with the system’s 2D dynamics. Combining analytical and numerical methods, we show that the 1D responses of grid cells with stable 1D fields are consistent with a linear slice through a 2D triangular lattice. Further, the 1D responses of comodular cells are well described by parallel slices, and the offsets in the starting points of the 1D slices can predict the measured 2D relative spatial phase between the cells. From these results, we conclude that the 2D dynamics of these cells is preserved in 1D, suggesting a common computation during both types of navigation behavior.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/65PLPQ7J/Yoon et al. - 2016 - Grid Cell Responses in 1D Environments Assessed as.pdf}
}

@unpublished{yuanImprovingExperienceReplay2021,
  title = {Improving {{Experience Replay}} with {{Successor Representation}}},
  author = {Yuan, Yizhi and Mattar, Marcelo},
  date = {2021-11-29},
  eprint = {2111.14331},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.14331},
  urldate = {2021-12-07},
  abstract = {Prioritized experience replay is a reinforcement learning technique whereby agents speed up learning by replaying useful past experiences. This usefulness is quantified as the expected gain from replaying the experience, a quantity often approximated as the prediction error (TD-error) observed during the corresponding experience. However, prediction error, and expected gain more generally, is only one possible metric for prioritization of replay. Recent work in neuroscience suggests that, in biological organisms, replay is prioritized not only by gain, but also by “need” – a quantity measuring the expected relevance of each experience with respect to the current situation. Importantly, this term is not currently considered in algorithms such as prioritized experience replay. In this paper we present a new approach for prioritizing experiences for replay that considers both gain and need. We test our approach by considering the need term, quantified as the Successor Representation, into the sampling process of different experience replay algorithms. Our proposed algorithms show an increase in performance in benchmarks including the Dyna-Q maze and a selection of Atari games.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/P8HPLSGN/Yuan and Mattar - 2021 - Improving Experience Replay with Successor Represe.pdf}
}

@article{yuGridCodesAfford,
  title = {Do Grid Codes Afford Generalization and Flexible Decision-Making?},
  author = {Yu, Linda Q and Park, Seongmin A and Sweigart, Sarah C and Boorman, Erie D},
  pages = {30},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EBAMDVLZ/Yu et al. - Do grid codes afford generalization and flexible d.pdf}
}

@article{yuGridCodesAffordb,
  title = {Do Grid Codes Afford Generalization and Flexible Decision-Making?},
  author = {Yu, Linda Q and Park, Seongmin A and Sweigart, Sarah C and Boorman, Erie D},
  pages = {30},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/E2YXSZA5/Yu et al. - Do grid codes afford generalization and flexible d.pdf}
}

@article{zajkowskiCausalRoleRight2017,
  title = {A Causal Role for Right Frontopolar Cortex in Directed, but Not Random, Exploration},
  author = {Zajkowski, Wojciech K and Kossut, Malgorzata and Wilson, Robert C},
  editor = {Frank, Michael J},
  date = {2017-09-15},
  journaltitle = {eLife},
  volume = {6},
  pages = {e27430},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.27430},
  url = {https://doi.org/10.7554/eLife.27430},
  urldate = {2022-09-09},
  abstract = {The explore-exploit dilemma occurs anytime we must choose between exploring unknown options for information and exploiting known resources for reward. Previous work suggests that people use two different strategies to solve the explore-exploit dilemma: directed exploration, driven by information seeking, and random exploration, driven by decision noise. Here, we show that these two strategies rely on different neural systems. Using transcranial magnetic stimulation to inhibit the right frontopolar cortex, we were able to selectively inhibit directed exploration while leaving random exploration intact. This suggests a causal role for right frontopolar cortex in directed, but not random, exploration and that directed and random exploration rely on (at least partially) dissociable neural systems.},
  keywords = {decision making,directed exploration,explore-exploit,frontal pole,random exploration,reinforcement learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/EITX3LLY/Zajkowski et al. - 2017 - A causal role for right frontopolar cortex in dire.pdf}
}

@article{zengTheoryGeometryRepresentations2022,
  title = {A Theory of Geometry Representations for Spatial Navigation},
  author = {Zeng, Taiping and Si, Bailu and Feng, Jianfeng},
  date = {2022-04-01},
  journaltitle = {Progress in Neurobiology},
  shortjournal = {Progress in Neurobiology},
  volume = {211},
  pages = {102228},
  issn = {0301-0082},
  doi = {10.1016/j.pneurobio.2022.102228},
  url = {https://www.sciencedirect.com/science/article/pii/S0301008222000144},
  urldate = {2022-04-01},
  abstract = {The geometric information of space, such as environment boundaries, is represented heterogeneously across brain regions. The computational mechanisms of encoding the spatial layout of environments remain to be determined. Here, we postulate a conjunctive encoding theory to illustrate the construct of cognitive maps from geometric perception. The theory naturally describes a spectrum of cell types including experimentally observed boundary vector cells, border cells, “annulus” and “bulls-eye” cells as special examples. In a similar way, inspired by the integration of egocentric and allocentric information as found in the postrhinal cortex, the theory also predicts a new cell type, named geometry cell. Geometry cells encode the geometric layout of the local space relative to the environment center, independent of the animal's positions and headings within the local space. The predicted geometry cell provides pure allocentric high-level representations of local scenes to support the quick formation of cognitive map representations capturing the spatial layout of complex environments. The theory sheds new light on the neural mechanisms of spatial cognition and brain-inspired autonomous intelligent systems.},
  langid = {english},
  keywords = {Boundary cells,Cognitive map,Conjunctive encoding,Geometry representations,Postrhinal cortex,Spatial navigation,To read},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/IZEYHQ8B/Zeng et al. - 2022 - A theory of geometry representations for spatial n.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/B8T5CJ8V/S0301008222000144.html}
}

@misc{zhangDeeperLookExperience2018,
  title = {A {{Deeper Look}} at {{Experience Replay}}},
  author = {Zhang, Shangtong and Sutton, Richard S.},
  date = {2018-04-30},
  number = {arXiv:1712.01275},
  eprint = {1712.01275},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.01275},
  url = {http://arxiv.org/abs/1712.01275},
  urldate = {2022-10-31},
  abstract = {Recently experience replay is widely used in various deep reinforcement learning (RL) algorithms, in this paper we rethink the utility of experience replay. It introduces a new hyper-parameter, the memory buffer size, which needs carefully tuning. However unfortunately the importance of this new hyper-parameter has been underestimated in the community for a long time. In this paper we did a systematic empirical study of experience replay under various function representations. We showcase that a large replay buffer can significantly hurt the performance. Moreover, we propose a simple O(1) method to remedy the negative influence of a large replay buffer. We showcase its utility in both simple grid world and challenging domains like Atari games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/2GK4K7YS/Zhang and Sutton - 2018 - A Deeper Look at Experience Replay.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/U3GUEI26/1712.html}
}

@article{zhouCoreFunctionOrbitofrontal2021,
  title = {Is the Core Function of Orbitofrontal Cortex to Signal Values or Make Predictions?},
  author = {Zhou, Jingfeng and Gardner, Matthew PH and Schoenbaum, Geoffrey},
  date = {2021-10-01},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  series = {Value Based Decision-Making},
  volume = {41},
  pages = {1--9},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2021.02.011},
  url = {https://www.sciencedirect.com/science/article/pii/S2352154621000334},
  urldate = {2022-08-07},
  abstract = {One dominant hypothesis about the function of the orbitofrontal cortex (OFC) is that the OFC signals the subjective values of possible outcomes to other brain areas for learning and decision making. This popular view generally neglects the fact that OFC is not necessary for simple value-based behavior (i.e. when values have been directly experienced). An alternative, emerging view suggests that OFC plays a more general role in representing structural information about the task or environment, derived from prior experience, and relevant to predicting behavioral outcomes, such as value. From this perspective, value signaling is simply one derivative of the core underlying function of OFC. New data in favor of both views have been accumulating rapidly. Here we review these new data in discussing the relative merits of these two ideas.},
  langid = {english},
  file = {/home/georgy/snap/zotero-snap/common/Zotero/storage/JQVMXHCJ/Zhou et al. - 2021 - Is the core function of orbitofrontal cortex to si.pdf;/home/georgy/snap/zotero-snap/common/Zotero/storage/C6SAN7A8/S2352154621000334.html}
}

@online{ZoteroYourPersonal,
  title = {Zotero | {{Your}} Personal Research Assistant},
  url = {https://www.zotero.org/start},
  urldate = {2021-12-10}
}

