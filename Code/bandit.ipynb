{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a3ed96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T13:30:42.691103Z",
     "start_time": "2021-11-12T13:30:39.907259Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "from scipy.special import kl_div\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbacf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(q_values, temp=None, type='softmax'):\n",
    "\n",
    "    '''\n",
    "    ----\n",
    "    Agent's policy\n",
    "    q_values -- q values at the current state\n",
    "    temp     -- inverse temperature\n",
    "    type     -- softmax / greeedy\n",
    "    ----\n",
    "    '''\n",
    "\n",
    "    if np.all(q_values == 0):\n",
    "        return np.array([0.5, 0.5])\n",
    "\n",
    "    if temp:\n",
    "        t = temp\n",
    "    else:\n",
    "        t = 1\n",
    "        \n",
    "    if type == 'softmax':\n",
    "        return np.exp(q_values*t)/np.sum(np.exp(q_values*t))\n",
    "    else:\n",
    "        return np.array(q_values >= q_values.max()).astype(int)\n",
    "\n",
    "def belief_update(M_curr, arm, rew):\n",
    "\n",
    "    '''\n",
    "    ----\n",
    "    Bayesian belief updates for beta prior\n",
    "    M_curr -- matrix with the current beliefs\n",
    "    arm    -- chosen arm \n",
    "    rew    -- received reward\n",
    "    ----\n",
    "    '''\n",
    "\n",
    "    M_next = M_curr.copy()\n",
    "    if rew == 1:\n",
    "        M_next[arm, 0] += 1\n",
    "    else:\n",
    "        M_next[arm, 1] += 1\n",
    "    return M_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9210f86",
   "metadata": {},
   "source": [
    "### Construct belief tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c3ef5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T13:30:48.351790Z",
     "start_time": "2021-11-12T13:30:48.336832Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_belief_tree(h, b):\n",
    "\n",
    "    '''\n",
    "    Generate planning belief tree\n",
    "    ----\n",
    "    h -- horizon\n",
    "    b -- initial belief at the root\n",
    "    Q -- MF Q values at the root \n",
    "    ----\n",
    "    '''\n",
    "\n",
    "    # initialise the hyperstate tree\n",
    "    tree = {hi:{} for hi in range(h)}\n",
    "    \n",
    "    tree[0][(0, 0, 0)] = b\n",
    "\n",
    "    for hi in range(1, h):\n",
    "        c = 0\n",
    "        if hi == 1:\n",
    "            for a in range(2):\n",
    "                for r in [1, 0]:\n",
    "                    b1 = belief_update(b, a, r)\n",
    "                    tree[hi][(a, 0, c)] = b1\n",
    "                    c += 1\n",
    "        else:\n",
    "            for k, v in tree[hi-1].items():\n",
    "                prev_c = k[-1]\n",
    "                for a in range(2):\n",
    "                    for r in [1, 0]:\n",
    "                        b1 = belief_update(v, a, r)\n",
    "                        tree[hi][(a, prev_c, c)] = b1\n",
    "                        c += 1\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c1f4a",
   "metadata": {},
   "source": [
    "### Replay in belief tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0ec7d",
   "metadata": {},
   "source": [
    "We want to prioritise updates in this belief tree. The prioritisation scheme is the following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    v(ba^*)-v(b) =& \\mathbb{E}_{b'\\sim p(b'\\mid b, a^*)}\\Big[\\sum_a \\big(\\pi(a\\mid b')-\\pi(a\\mid b)\\big)q(b',a) \\\\\n",
    "    +& \\mathbb{E}_{a\\sim \\pi(a\\mid b)}\\big[r(b',a) - r(b,a)\\big]  \\\\ \n",
    "    +& \\mathbb{E}_{a\\sim \\pi(a\\mid b)}\\big[\\gamma \\sum_{b''}p(b''\\mid b', a)v(b'') - \\gamma \\sum_{g'}p(g'\\mid b, a)v(g') \\big] \\Big] \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $ba^*$ is a belief that results from choosing action $a^*$ from belief $b$, the policy $\\pi(a\\mid b)$ is determined by the Gittins index associated with belief $b$, $q(b', a)$ is the Gittins index, and $v(b')=\\mathbb{E}_{\\pi(a\\mid b')}\\big[q(b',a)\\big]$\n",
    "\n",
    "For the moment, we will only consider single-step updates from children to their parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f2547e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(tree, Q, xi=0.1):\n",
    "    '''\n",
    "    Perform replay updates in the belief tree\n",
    "    ----\n",
    "    tree -- belief tree\n",
    "    Q    -- MF Q values at the root\n",
    "    xi   -- EVB threshold\n",
    "    ----\n",
    "    '''\n",
    "\n",
    "    h         = len(tree.keys())           # planning horizon\n",
    "    evb_tree  = {hi:{} for hi in range(h)} # tree with evb values for each node\n",
    "    qval_tree = {hi:{} for hi in range(h)} # tree with value estimate for each node\n",
    "    need_tree = {hi:{} for hi in range(h)}\n",
    "    backups   = [None]\n",
    "\n",
    "    qval_history = []\n",
    "    need_history = []\n",
    "\n",
    "    for hi in range(h):\n",
    "        for k, b in tree[hi].items():\n",
    "            # Q values at the root\n",
    "            if (hi == 0):\n",
    "                q_values = Q\n",
    "            # Q values at the leaves\n",
    "            elif (hi == h-1):\n",
    "                q_values = np.array([b[0, 0]/np.sum(b[0, :]), b[1, 0]/np.sum(b[1, :])]) # initial q-values are just the immediate rewards\n",
    "            # intermediate Q values\n",
    "            else:\n",
    "                q_values = np.zeros(2)\n",
    "                \n",
    "            qval_tree[hi][k] = copy(q_values) # change temperature?\n",
    "\n",
    "            prev_c = k[-2]\n",
    "            c      = k[-1]\n",
    "            a      = k[0]\n",
    "            proba  = 1\n",
    "            for hin in reversed(range(hi)):\n",
    "                for kn, bn in tree[hin].items():\n",
    "                    if kn[-1] == prev_c:\n",
    "                        policy_proba = policy(qval_tree[hin][kn])\n",
    "                        proba *= policy_proba[a]*(bn[a, c%2]/np.sum(bn[a, :]))\n",
    "\n",
    "                        c      = kn[-1]\n",
    "                        prev_c = kn[-2]\n",
    "                        a      = kn[0]\n",
    "                        break\n",
    "\n",
    "            need_tree[hi][k] = proba\n",
    "\n",
    "    qval_history += [deepcopy(qval_tree)]\n",
    "    need_history += [deepcopy(need_tree)]\n",
    "\n",
    "    # compute evb for every backup\n",
    "    num = 0\n",
    "    while True: # for N steps\n",
    "        max_evb = 0\n",
    "\n",
    "        nqval_tree = {hi:{} for hi in range(h)} # tree with new updated value estimates for each node\n",
    "        for hi in reversed(range(h-1)):\n",
    "            for k, b in tree[hi].items():\n",
    "                \n",
    "                q        = copy(qval_tree[hi][k])  # Q values of this belief state\n",
    "                v        = np.dot(policy(q), q)     # value of this belief state\n",
    "\n",
    "                # -- probability of reaching this belief state -- #\n",
    "                prev_c = k[-2]\n",
    "                c      = k[-1]\n",
    "                a      = k[0]\n",
    "                proba  = 1\n",
    "                for hin in reversed(range(hi)):\n",
    "                    for kn, bn in tree[hin].items():\n",
    "                        if kn[-1] == prev_c:\n",
    "                            policy_proba = policy(qval_tree[hin][kn])\n",
    "                            proba *= policy_proba[a]*(bn[a, c%2]/np.sum(bn[a, :]))\n",
    "\n",
    "                            c      = kn[-1]\n",
    "                            prev_c = kn[-2]\n",
    "                            a      = kn[0]\n",
    "                            break\n",
    "\n",
    "                need_tree[hi][k] = proba\n",
    "                        \n",
    "                v_primes = []\n",
    "\n",
    "                c = k[-1]\n",
    "                for a in range(2):\n",
    "                    v_primes = []\n",
    "                    for k1, q1 in qval_tree[hi+1].items():\n",
    "                        prev_c = k1[-2]\n",
    "                        if prev_c == c and k1[0] == a:\n",
    "                            v_primes += [np.dot(policy(q1), q1)] # values of next belief states\n",
    "\n",
    "                    # new (updated) Q value for action a\n",
    "                    q_upd = (b[a, 0]/np.sum(b[a, :]))*(1 + gamma*v_primes[0]) + (b[a, 1]/np.sum(b[a, :]))*(0 + gamma*v_primes[1])\n",
    "\n",
    "                    if a == 0:\n",
    "                        q_new = np.array([q_upd, q[1]])\n",
    "                    else:\n",
    "                        q_new = np.array([q[0], q_upd])\n",
    "                    \n",
    "                    v_new   = np.dot(policy(q_new), q_new) \n",
    "\n",
    "                    new_key = tuple(list(k) + [a])\n",
    "                    \n",
    "                    # can i get this proba here? it should be \n",
    "                    # different after each new replay update \n",
    "                    # (for the updated state, of course)\n",
    "\n",
    "                    # proba = 0.5\n",
    "                    evb   = proba*(v_new - v)\n",
    "                    # evb = v_new - v\n",
    "                        \n",
    "                    if evb > max_evb:\n",
    "                        max_evb = evb\n",
    "\n",
    "                    evb_tree[hi][new_key]   = evb\n",
    "                    nqval_tree[hi][new_key] = q_upd\n",
    "\n",
    "        if max_evb < xi:\n",
    "            break\n",
    "\n",
    "        max_val = 0\n",
    "        for hi in reversed(range(h-1)):\n",
    "            for k, v in evb_tree[hi].items():\n",
    "                if v > max_val:\n",
    "                    backup  = [hi, k]\n",
    "                    max_val = v\n",
    "        if max_val <= 0:\n",
    "            return qval_history, need_history, backups\n",
    "\n",
    "        hi = backup[0]\n",
    "        k  = backup[1][:-1]\n",
    "        a  = backup[1][-1]\n",
    "\n",
    "        qvals    = qval_tree[hi][k]\n",
    "        new_qval = nqval_tree[hi][backup[1]]\n",
    "        qvals[a] = new_qval\n",
    "        qval_tree[hi][k] = qvals\n",
    "\n",
    "        qval_history += [deepcopy(qval_tree)]\n",
    "        need_history += [deepcopy(need_tree)]\n",
    "        backups      += [[tree[hi][k], backup[0], backup[1]]]\n",
    "\n",
    "        num += 1\n",
    "\n",
    "    return  qval_history, need_history, backups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66ed72",
   "metadata": {},
   "source": [
    "### Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a5f6a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = np.array([\n",
    "#     [100, 1],\n",
    "#     [1, 100]\n",
    "# ])\n",
    "\n",
    "# gamma = 0.9\n",
    "\n",
    "# Q = np.array([3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6379b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tex_tree import generate_tex_tree\n",
    "\n",
    "# h    = 3\n",
    "# tree = get_belief_tree(h, M)\n",
    "# val_tree, replays = replay(tree, Q, xi=0.1)\n",
    "# save_path = os.path.join('/home/georgy/Documents/Dayan_lab/PhD/bandits/Data/Tree/tex_tree_small.tex')\n",
    "# generate_tex_tree(M, replays, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "99f56304",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([\n",
    "    [5, 2],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "Q = np.array([2, 1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7143eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] 7.5e-01\n",
      "[0.75 0.  ]\n",
      "\n",
      "\n",
      "[0. 0.] 1.3704040662007253e+00\n",
      "[1.37040407 0.        ]\n",
      "\n",
      "\n",
      "[0.75 0.  ] 1.4876551591911622e+00\n",
      "[1.48765516 0.        ]\n",
      "\n",
      "\n",
      "[0. 0.] 1.1983961219685322e+00\n",
      "[1.19839612 0.        ]\n",
      "\n",
      "\n",
      "[1.48765516 0.        ] 1.6948025443989323e+00\n",
      "[1.69480254 0.        ]\n",
      "\n",
      "\n",
      "[0. 0.] 6.25e-01\n",
      "[0.625 0.   ]\n",
      "\n",
      "\n",
      "[0. 0.] 1.1983961219685322e+00\n",
      "[1.19839612 0.        ]\n",
      "\n",
      "\n",
      "[0.625 0.   ] 1.1428684630194252e+00\n",
      "[1.14286846 0.        ]\n",
      "\n",
      "\n",
      "[0. 0.] 7.142857142857143e-01\n",
      "[0.71428571 0.        ]\n",
      "\n",
      "\n",
      "[0. 0.] 1.3895811926290884e+00\n",
      "[1.38958119 0.        ]\n",
      "\n",
      "\n",
      "[0.71428571 0.        ] 1.4293967887098964e+00\n",
      "[1.42939679 0.        ]\n",
      "\n",
      "\n",
      "[0. 0.] 7.142857142857143e-01\n",
      "[0.71428571 0.        ]\n",
      "\n",
      "\n",
      "[0. 0.] 1.276488028037742e+00\n",
      "[1.27648803 0.        ]\n",
      "\n",
      "\n",
      "[0.71428571 0.        ] 1.3558725142830275e+00\n",
      "[1.35587251 0.        ]\n",
      "\n",
      "\n",
      "[2. 1.] 1.5040786183926742e+00\n",
      "[2.        1.5040786]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tex_tree import generate_big_tex_tree\n",
    "\n",
    "h    = 4\n",
    "tree = get_belief_tree(h, M)\n",
    "qval_history, need_history, replays = replay(tree, Q, xi=0.05)\n",
    "\n",
    "save_folder = '/home/georgy/Documents/Dayan_lab/PhD/bandits/Reports/2/data/Tree/seq/'\n",
    "if os.path.exists(save_folder):\n",
    "    shutil.rmtree(save_folder)\n",
    "    os.mkdir(save_folder)\n",
    "else:\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "for idx, rep in enumerate(replays):\n",
    "    these_replays = replays[:idx+1]\n",
    "    save_path = os.path.join(save_folder, 'tex_tree_%u.tex'%idx)\n",
    "    generate_big_tex_tree(h, these_replays, qval_history[idx], need_history[idx], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c62fc",
   "metadata": {},
   "source": [
    "### Distribution updates asymmetry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2702e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.001, 1, 100)\n",
    "\n",
    "a1b, b1b = np.array([2, 1])\n",
    "rv1b = beta(a1b, b1b)  \n",
    "\n",
    "a1a, b1a = np.array([3, 1])\n",
    "rv1a = beta(a1a, b1a)\n",
    "\n",
    "a1aa, b1aa = np.array([4, 1])\n",
    "rv1aa = beta(a1aa, b1aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(131)\n",
    "plt.plot(x, rv1b.pdf(x))\n",
    "plt.title(r'$\\alpha=2, \\beta=1$')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(x, rv1a.pdf(x))\n",
    "plt.title(r'$\\alpha=3, \\beta=1$')\n",
    "\n",
    "print(np.sum(kl_div(rv1b.pdf(x), rv1a.pdf(x))))\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(x, rv1aa.pdf(x))\n",
    "plt.title(r'$\\alpha=4, \\beta=1$')\n",
    "\n",
    "print(np.sum(kl_div(rv1a.pdf(x), rv1aa.pdf(x))))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/home/georgy/Documents/Dayan_lab/PhD/bandits/Data/betas.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9791ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "560.85px",
    "left": "1066px",
    "right": "20px",
    "top": "120px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
