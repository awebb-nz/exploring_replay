\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage[font=footnotesize]{caption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=blue,
    citecolor=red,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\usepackage{xcolor}
\def\labelitemi{--}

\newcommand{\old}{\text{old}}
\newcommand{\new}{\text{new}}

\usepackage{lineno}
\linenumbers

\setlength{\parindent}{0pt}

\begin{document}

\subsection*{EVB for information states}

\begin{table}
\begin{tabular*}{4cm}{ll}
    $s_{\text{agent}}$ & current physical state of the agent\\
    $b_{\text{agent}}$ & current belief state of the agent\\
    $s_\rho$ & physical root state of a planning tree\\
    $N_{s_\rho}$ & number of steps it takes the agent to reach $s_\rho$ from $s_{\text{agent}}$\\
    $s_k$ & physical state at which an update is considered\\
    $a_k$ & action for which an update is considered\\
    $b_k$ & belief state at which an update is considered
\end{tabular*}
\caption{Notation}
\end{table}

\bigbreak

We are ultimately interested in how the value of the current information state $z_{\text{agent}} = \langle s_{\text{agent}}, b_{\text{agent}} \rangle$ changes as a result of 
a policy update at some future information state $z_k \in \mathcal{Z}$:
\begin{equation*}
    \label{eqn:evb}
    v_{\pi_{\text{new}}}(z_{\text{agent}}) - v_{\pi_{\text{old}}}(z_{\text{agent}}) = \sum_{z \in \mathcal{Z}} \underbrace{\sum_{i=0}^{\infty} \gamma^i P(z_{\text{agent}} \rightarrow z, i, \pi_{\old})}_{\text{Need}} \times \underbrace{\sum_a \big[ \pi_{\new}(z, a) - \pi_{\old}(z, a) \big] q_{\pi_{\new}}(z, a)}_{\text{Gain}}
\end{equation*}

\bigbreak

Need is currently estimated in the following way:
\begin{itemize}
    \item[1.] Simulate $K$ forward trajectories from the agent's current state $s_{\text{agent}}$ and belief $b_{\text{agent}}$, using the old policy $\pi_{\text{old}}$
    \item[2.] Terminate each trajectory if $\gamma^{d} < \epsilon$ where $d$ is the trajectory length 
    \item[3.] For all states, get the miminal number of steps (across those $K$ trajectories) it takes the agent to get to those states. Denote this by $N_{s_\rho}$ for each $s$
\end{itemize}

\bigbreak 

Then:

\begin{align*}
\widehat{\text{Need}}(\langle s_k, b_k \rangle) = &\gamma^{N_{s_\rho}} P(\langle s_{\text{agent}}, b_{\text{agent}} \rangle \rightarrow \langle s_\rho, b_{\text{agent}} \rangle, N_{s_\rho}, \pi_{\text{old}}) \times \gamma^h P(\langle s_\rho, b_{\text{agent}} \rangle \rightarrow  \langle s_k, b_k \rangle, h, \pi_{\text{old}})\\
+ \sum_{i=N_{s_\rho}+H+1}^{\infty} &\gamma^i P(\langle s_{\text{agent}}, b_k \rangle \rightarrow \langle s_k, b_k \rangle, i, \pi_{\text{old}})
\end{align*}

\clearpage
Do we need $s_\rho$? POCMP builds a partial tree (in the sense that not all histories are evaluated, but 
only the ones chosen by the tree policy)

\bigbreak

One way of estimating Need is therefore to:
\begin{itemize}
    \item[1.] Simulate $K$ forward trajectories from the agent's current information state $z_\text{agent}= \langle s_\text{agent}, b_\text{agent}\rangle$ using the old policy $\pi_\text{old}$. Update the belief along the way.
    \item[2.] Terminate each trajectory if $\gamma^{d} < \epsilon$ where $d$ is the trajectory length
    \item[3.] Estimate the probability of reaching $z_k= \langle s_k, b_k\rangle$ and the average number of steps to reach it -- denote this by $N(z_k)$
    \item[4.] Estimate Need as:
    \begin{align*}
        \widehat{\text{Need}}(z_k) &= \gamma^{N(z_k)}P(z_\text{agent} \rightarrow z_k, N(z_k), \pi_\text{old})\\
        &+ \sum_{i=N(z_k)+1}^{\infty} \gamma^i P(\langle s_\text{agent}, b_k\rangle \rightarrow \langle s_k, b_k \rangle, i, \pi_\text{old})
    \end{align*}
\end{itemize}

Problems: i) this is not going to scale very well to larger problems; and ii) simulations are still done on-policy, for 
estimating Need under $\pi_\text{old}$.

\end{document}