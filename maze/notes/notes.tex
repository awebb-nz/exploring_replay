\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage[font=footnotesize]{caption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=blue,
    citecolor=red,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\usepackage{xcolor}
\def\labelitemi{--}

\newcommand{\old}{\text{old}}
\newcommand{\new}{\text{new}}

\usepackage{lineno}
\linenumbers

\setlength{\parindent}{0pt}

\begin{document}

\subsection*{EVB for information states}

We are ultimately interested in how the value of our current information state $z_{\text{agent}} = \langle s_{\text{agent}}, b_{\text{agent}} \rangle$ changes as a result of 
a policy update at some future information state:
\begin{equation}
    \label{eqn:evb}
    \text{EVB}(z_k, a_k) = \sum_{z \in \mathcal{Z}} \underbrace{\sum_{i=0}^{\infty} \gamma^i P(z_{\text{agent}} \rightarrow z, i, \pi_{\old})}_{\text{Need}} \times \underbrace{\sum_a \big[ \pi_{\new}(z, a) - \pi_{\old}(z, a) \big] q_{\pi_{\new}}(z, a)}_{\text{Gain}}
\end{equation}
Note that M\&D get rid of the first summation (in their case, it was over $\mathcal{S}$), since they assume that 
a single update changes the policy only at that update location (and hence Gain is $0$ everywhere except that location). 
In our case, this is not quite true, because there are a number of information states that can potentially benefit from a 
policy update elsewhere -- and thus ideally we want to keep that sum.

\bigbreak

Our current model has a number of assumptions. For the updates of distal information states, we have:
\begin{equation}
    \label{eqn:distal}
    \text{EVB}_{\text{dist}}(z_k=\langle s_k, b_k \rangle, a_k) = \gamma^i P(\langle s_\rho, b_{\text{agent}} \rangle \rightarrow \langle s_k, b_k \rangle, i, \pi_{\old}) \times \text{Gain}(\langle s_k, b_k \rangle, a_k)
\end{equation}
where $\langle s_\rho, b_{\text{agent}} \rangle$ is the information state at the root of the state-action tree which contains information state $z_k=\langle s_k, b_k \rangle$. The assumption is therefore 
that the agent can 'teleport' from its current physical state $s_\text{agent}$ to the physical root state of that tree $s_\rho$ without changing its belief $b_{\text{agent}}$.

\bigbreak

As for the root updates, we have the following:
\begin{equation}
    \label{eqn:root}
    \text{EVB}_{\text{root}}(z_\rho=\langle s_\rho, b_\rho \rangle, a_k) = \underbrace{\sum_{i=0}^{\infty} \gamma^i P(\langle s_{\text{agent}}, b_{\text{agent}} \rangle \rightarrow \langle s_\rho, b_{\text{agent}} \rangle, i, \pi_{\old})}_{\text{certainty-equivalent Need}} \times \underbrace{\text{Gain}_{\text{sweep}}(\langle s_\rho, b_{\text{agent}} \rangle, a_k) \vphantom{\sum_{i=0}^{\infty}}}_{\text{sweeped Gain}}
\end{equation}
Where we partially account for the (fixed horizon) learnt information by our sweeping procedure when estimating Gain (by using equation \ref{eqn:distal}). 
Partially because this isn't reflected in the Need term, since we do not account for the changes in the transition model when calculating Need.

\bigbreak

Another limitation is the lack of generalisaton across information states because we exclude the summation over $\mathcal{Z}$ in equation \ref{eqn:evb}. 
Mattar \& Daw do it as well (in their case the sum is over $\mathcal{S}$) since they assume greedy updates which means that Gain is non-zero only for 
the updated experience $(s_k, a_k)$.

\bigbreak

The certainty-equivalence assumption right from the agent's curent physical location $s_\text{agent}$ seems a bit odd, since we learn something about 
the transition model and then simply ignore it. My suggestion is to change how we compute Need to account for at least a fixed-horizon learning, and 
then do the certainty-equivalence approximation. This would be more consistent with how we calculate Gain, and in fact then we can have a single equation 
for both distal and root updates.

\bigbreak

The idea is essentially to have two forms of certainty-equivalent Need. The first one would be a non-cumulative Need of reaching some root state $s_\rho$ 
from the agent's current physical location $s_\text{agent}$. Then, from that root state we plan up to some fixed horizon, and finish off Need with certainty-equivalent 
Need based on the belief reached during that planning. Formally, this would look like this:
\begin{linenomath}
\begin{align}
    \text{Need}(z_k=\langle s_k, b_k \rangle) = &\sum_{i=0}^{N} \gamma^i P_\text{old}(\langle s_\text{agent}, b_\text{agent} \rangle \rightarrow \langle s_k, b_\text{agent} \rangle, i, \pi_\old) & \text{get to the root, fixed belief} \nonumber \\
    + &\sum_{j=N}^{N+H} \gamma^i P_\text{upd}(\langle s_\text{agent}, b_\text{agent} \rangle \rightarrow \langle s_k, b_k \rangle, j, \pi_\old) & \text{consequitive belief updates} \nonumber \\
    + &\sum_{k=N+H+1}^{\infty} \gamma^i P_\text{new}(\langle s_\text{agent}, b_k \rangle \rightarrow \langle s_k, b_k \rangle, k, \pi_\old) & \text{fixed, updated belief} \nonumber
\end{align}
\end{linenomath}
where $N$ is the expected number of steps needed to reach the root state $s_\rho$ from the agent's current location $s_\text{agent}$ and 
$H$ is the planning horizon.

\bigbreak

\begin{itemize}
    \item Might not be worth it depending on the discount $\gamma$ and/or the distance, since the contributions of future learning fade away 
    \item Makes me think -- maybe we can derive the approximation error due to a limited horizon analytically? E.g., some notion of 'information regret'
\end{itemize}

\end{document}