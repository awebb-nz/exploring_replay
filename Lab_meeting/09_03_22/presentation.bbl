% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{drieuHippocampalSequencesExploration2019}{article}{}
    \name{author}{2}{}{%
      {{hash=DC}{%
         family={Drieu},
         familyi={D\bibinitperiod},
         given={CÃ©line},
         giveni={C\bibinitperiod},
      }}%
      {{hash=ZM}{%
         family={Zugaro},
         familyi={Z\bibinitperiod},
         given={MichaÃ«l},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{DC+1}
    \strng{fullhash}{DCZM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2019}
    \field{labeldatesource}{}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    Although the hippocampus plays a critical role in spatial and episodic
  memories, the mechanisms underlying memory formation, stabilization, and
  recall for adaptive behavior remain relatively unknown. During exploration,
  within single cycles of the ongoing theta rhythm that dominates hippocampal
  local field potentials, place cells form precisely ordered sequences of
  activity. These neural sequences result from the integration of both external
  inputs conveying sensory-motor information, and intrinsic network dynamics
  possibly related to memory processes. Their endogenous replay during
  subsequent sleep is critical for memory consolidation. The present review
  discusses possible mechanisms and functions of hippocampal theta sequences
  during exploration. We present several lines of evidence suggesting that
  these neural sequences play a key role in information processing and support
  the formation of initial memory traces, and discuss potential functional
  distinctions between neural sequences emerging during theta vs. awake
  sharp-wave ripples.%
    }
    \field{issn}{1662-5102}
    \field{shorttitle}{Hippocampal {{Sequences During Exploration}}}
    \field{title}{Hippocampal {{Sequences During Exploration}}: {{Mechanisms}}
  and {{Functions}}}
    \verb{url}
    \verb https://www.frontiersin.org/article/10.3389/fncel.2019.00232
    \endverb
    \field{volume}{13}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/48VQ49YH/Drieu an
    \verb d Zugaro - 2019 - Hippocampal Sequences During Exploration Mechanis.p
    \verb df
    \endverb
    \field{journaltitle}{Frontiers in Cellular Neuroscience}
    \field{year}{2019}
    \field{urlday}{07}
    \field{urlmonth}{03}
    \field{urlyear}{2022}
  \endentry

  \entry{duffQLearningBanditProblems1995}{incollection}{}
    \name{author}{1}{}{%
      {{hash=DMO}{%
         family={Duff},
         familyi={D\bibinitperiod},
         given={Michael\bibnamedelima O.},
         giveni={M\bibinitperiod\bibinitdelim O\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=PA}{%
         family={Prieditis},
         familyi={P\bibinitperiod},
         given={Armand},
         giveni={A\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Russell},
         familyi={R\bibinitperiod},
         given={Stuart},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Morgan Kaufmann}}%
    }
    \strng{namehash}{DMO1}
    \strng{fullhash}{DMO1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1995}
    \field{labeldatesource}{}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    Multi-armed bandits may be viewed as decompositionally-structured Markov
  decision processes (MDP's) with potentially very-large state sets. A
  particularly elegant methodology for computing optimal policies was developed
  over twenty ago by Gittins [Gittins \& Jones, 1974]. Gittins' approach
  reduces the problem of finding optimal policies for the original MDP to a
  sequence of low-dimensional stopping problems whose solutions determine the
  optimal policy through the so-called â€œGittins indices.â€ Katehakis and
  Veinott [Katehakis \& Veinott, 1987] have shown that the Gittins index for a
  process in state i may be interpreted as a particular component of the
  maximum-value function associated with the â€œrestart-in-iâ€ process, a
  simple MDP to which standard solution methods for computing optimal policies,
  such as successive approximation, apply. This paper explores the problem of
  learning the Gittins indices on-line without the aid of a process model; it
  suggests utilizing process-state- specific Q-learning agents to solve their
  respective restart-in-state-i subproblems, and includes an example in which
  the online reinforcement learning approach is applied to a problem of
  stochastic scheduling-one instance drawn from a wide class of problems that
  may be formulated as bandit problems.%
    }
    \field{booktitle}{Machine {{Learning Proceedings}} 1995}
    \verb{doi}
    \verb 10.1016/B978-1-55860-377-6.50034-7
    \endverb
    \field{isbn}{978-1-55860-377-6}
    \field{pages}{209\bibrangedash 217}
    \field{title}{Q-{{Learning}} for {{Bandit Problems}}}
    \verb{url}
    \verb https://www.sciencedirect.com/science/article/pii/B978155860377650034
    \verb 7
    \endverb
    \field{langid}{english}
    \list{location}{1}{%
      {{San Francisco (CA)}}%
    }
    \verb{file}
    \verb /home/georgy/Documents/Papers/RL/Duff2015.pdf;/home/georgy/snap/zoter
    \verb o-snap/common/Zotero/storage/43EQNUK8/B9781558603776500347.html
    \endverb
    \field{day}{01}
    \field{month}{01}
    \field{year}{1995}
    \field{urlday}{12}
    \field{urlmonth}{12}
    \field{urlyear}{2021}
  \endentry

  \entry{gittinsBanditProcessesDynamic1979}{article}{}
    \name{author}{1}{}{%
      {{hash=GJC}{%
         family={Gittins},
         familyi={G\bibinitperiod},
         given={J.\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \strng{namehash}{GJC1}
    \strng{fullhash}{GJC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1979}
    \field{labeldatesource}{}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    The paper aims to give a unified account of the central concepts in recent
  work on bandit processes and dynamic allocation indices; to show how these
  reduce some previously intractable problems to the problem of calculating
  such indices; and to describe how these calculations may be carried out.
  Applications to stochastic scheduling, sequential clinical trials and a class
  of search problems are discussed.%
    }
    \verb{doi}
    \verb 10.1111/j.2517-6161.1979.tb01068.x
    \endverb
    \field{issn}{00359246}
    \field{number}{2}
    \field{pages}{148\bibrangedash 164}
    \field{shortjournal}{Journal of the Royal Statistical Society: Series B
  (Methodological)}
    \field{title}{Bandit {{Processes}} and {{Dynamic Allocation Indices}}}
    \verb{url}
    \verb https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1979.tb01068.
    \verb x
    \endverb
    \field{volume}{41}
    \field{langid}{english}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/XP5VEX69/Gittins
    \verb - 1979 - Bandit Processes and Dynamic Allocation Indices.pdf
    \endverb
    \field{journaltitle}{Journal of the Royal Statistical Society: Series B
  (Methodological)}
    \field{month}{01}
    \field{year}{1979}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2021}
  \endentry

  \entry{guezSampleBasedSearchMethods2015}{thesis}{}
    \name{author}{1}{}{%
      {{hash=GA}{%
         family={Guez},
         familyi={G\bibinitperiod},
         given={Arthur},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{GA1}
    \strng{fullhash}{GA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2015}
    \field{labeldatesource}{}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{title}{Sample-{{Based Search Methods}} for {{Bayes-Adaptive
  Planning}}}
    \field{langid}{english}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/K3Z23WNB/Guez - S
    \verb ample-Based Search Methods for Bayes-Adaptive Pla.pdf
    \endverb
    \field{year}{2015}
  \endentry

  \entry{guptaHippocampalReplayNot2010}{article}{useprefix}
    \name{author}{4}{}{%
      {{hash=GAS}{%
         family={Gupta},
         familyi={G\bibinitperiod},
         given={Anoopum\bibnamedelima S.},
         giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=vdMMA}{%
         prefix={van\bibnamedelima der},
         prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod},
         family={Meer},
         familyi={M\bibinitperiod},
         given={Matthijs\bibnamedelima A.A.},
         giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=TDS}{%
         family={Touretzky},
         familyi={T\bibinitperiod},
         given={David\bibnamedelima S.},
         giveni={D\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=RAD}{%
         family={Redish},
         familyi={R\bibinitperiod},
         given={A.\bibnamedelima David},
         giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
    }
    \strng{namehash}{GAS+1}
    \strng{fullhash}{GASvdMMATDSRAD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2010}
    \field{labeldatesource}{}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Replay of behavioral sequences in the hippocampus during sharp wave ripple
  complexes (SWRs) provides a potential mechanism for memory consolidation and
  the learning of knowledge structures. Current hypotheses imply that replay
  should straightforwardly reflect recent experience. However, we find these
  hypotheses to be incompatible with the content of replay on a task with two
  distinct behavioral sequences (A and B). We observed forward and backward
  replay of B even when rats had been performing A for {$>$}10 min.
  Furthermore, replay of nonlocal sequence B occurred more often when B was
  infrequently experienced. Neither forward nor backward sequences
  preferentially represented highly experienced trajectories within a session.
  Additionally, we observed the construction of never-experienced novel-path
  sequences. These observations challenge the idea that sequence activation
  during SWRs is a simple replay of recent experience. Instead, replay
  reflected all physically available trajectories within the environment,
  suggesting a potential role in active learning and maintenance of the
  cognitive map.%
    }
    \verb{doi}
    \verb 10.1016/j.neuron.2010.01.034
    \endverb
    \field{issn}{08966273}
    \field{number}{5}
    \field{pages}{695\bibrangedash 705}
    \field{shortjournal}{Neuron}
    \field{title}{Hippocampal {{Replay Is Not}} a {{Simple Function}} of
  {{Experience}}}
    \verb{url}
    \verb https://linkinghub.elsevier.com/retrieve/pii/S0896627310000607
    \endverb
    \field{volume}{65}
    \field{langid}{english}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/ISXGV3H4/Gupta et
    \verb  al. - 2010 - Hippocampal Replay Is Not a Simple Function of Exp.pdf
    \endverb
    \field{journaltitle}{Neuron}
    \field{month}{03}
    \field{year}{2010}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2021}
  \endentry

  \entry{moorePrioritizedSweepingReinforcement1993}{article}{}
    \name{author}{2}{}{%
      {{hash=MAW}{%
         family={Moore},
         familyi={M\bibinitperiod},
         given={Andrew\bibnamedelima W.},
         giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
      {{hash=ACG}{%
         family={Atkeson},
         familyi={A\bibinitperiod},
         given={Christopher\bibnamedelima G.},
         giveni={C\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \strng{namehash}{MAW+1}
    \strng{fullhash}{MAWACG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{1993}
    \field{labeldatesource}{}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    We present a new algorithm,prioritized sweeping, for efficientprediction
  and control of stochastic Markovsystems.Incrementallearningmethodssuchas
  temporaldifferencingand Q-learninghavereal-timeperformance.
  Classicalmethodsare slower,but moreaccurate,becausetheymakefull use of the
  observations.Prioritized sweepingaims for the best of both worlds. It uses
  all previous experiencesboth to prioritize important
  dynamicprogrammingsweepsand to guidethe explorationof state-space.
  Wecompareprioritizedsweepingwith other reinforcementlearningschemesfor a
  numberofdifferentstochasticoptimalcontrolproblems.It successfully solves
  large state-space real-time problems with which other methods have
  difficulty.%
    }
    \verb{doi}
    \verb 10.1007/BF00993104
    \endverb
    \field{issn}{0885-6125, 1573-0565}
    \field{number}{1}
    \field{pages}{103\bibrangedash 130}
    \field{shortjournal}{Mach Learn}
    \field{shorttitle}{Prioritized Sweeping}
    \field{title}{Prioritized Sweeping: {{Reinforcement}} Learning with Less
  Data and Less Time}
    \verb{url}
    \verb http://link.springer.com/10.1007/BF00993104
    \endverb
    \field{volume}{13}
    \field{langid}{english}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/3XSA2SVI/Moore an
    \verb d Atkeson - 1993 - Prioritized sweeping Reinforcement learning with .
    \verb pdf
    \endverb
    \field{journaltitle}{Machine Learning}
    \field{month}{10}
    \field{year}{1993}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2021}
  \endentry

  \entry{suttonIntegratedArchitecturesLearning1990}{incollection}{}
    \name{author}{1}{}{%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Elsevier}}%
    }
    \strng{namehash}{SRS1}
    \strng{fullhash}{SRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1990}
    \field{labeldatesource}{}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    This paper extends previous work with Dyna, a class of architectures for
  intelligent systems based on approximating dynamic programming methods. Dyna
  architectures integrate trial-and-error (reinforcement) learning and
  execution-time planning into a single process operating alternately on the
  world and on a learned model of the world. In this paper, I present and show
  results for two Dyna architectures. The Dyna-PI architecture is based on
  dynamic programming's policy iteration method and can be related to existing
  AI ideas such as evaluation functions and universal plans (reactive systems).
  Using a navigation task, results are shown for a simple Dyna-PI system that
  simultaneously learns by trial and error, learns a world model, and plans
  optimal routes using the evolving world model. The Dyna-Q architecture is
  based on Watkins?s Q-learning, a new kind of reinforcement learning. Dyna-Q
  uses a less familiar set of data structures than does Dyna-PI, but is
  arguably simpler to implement and use. We show that Dyna-Q architectures are
  easy to adapt for use in changing environments.%
    }
    \field{booktitle}{Machine {{Learning Proceedings}} 1990}
    \verb{doi}
    \verb 10.1016/B978-1-55860-141-3.50030-4
    \endverb
    \field{isbn}{978-1-55860-141-3}
    \field{pages}{216\bibrangedash 224}
    \field{title}{Integrated {{Architectures}} for {{Learning}}, {{Planning}},
  and {{Reacting Based}} on {{Approximating Dynamic Programming}}}
    \verb{url}
    \verb https://linkinghub.elsevier.com/retrieve/pii/B9781558601413500304
    \endverb
    \field{langid}{english}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/LDM5E86P/Sutton -
    \verb  1990 - Integrated Architectures for Learning, Planning, a.pdf
    \endverb
    \field{year}{1990}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2021}
  \endentry

  \entry{suttonDynaIntegratedArchitecture1991}{article}{}
    \name{author}{1}{}{%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \strng{namehash}{SRS1}
    \strng{fullhash}{SRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1991}
    \field{labeldatesource}{}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Dyna is an AI architecture that integrates learning, planning, and reactive
  execution. Learning methods are used in Dyna both for compiling planning
  results and for updating a model of the effects of the agent's actions on the
  world. Planning is incremental and can use the probabilistic and ofttimes
  incorrect world models generated by learning processes. Execution is fully
  reactive in the sense that no planning intervenes between perception and
  action. Dyna relies on machine learning methods for learning from
  examples--these are among the basic building blocks making up the
  architecture--yet is not tied to any particular method. This paper briefly
  introduces Dyna and discusses its strengths and weaknesses with respect to
  other architectures.%
    }
    \verb{doi}
    \verb 10.1145/122344.122377
    \endverb
    \field{issn}{0163-5719}
    \field{number}{4}
    \field{pages}{160\bibrangedash 163}
    \field{shortjournal}{SIGART Bull.}
    \field{title}{Dyna, an Integrated Architecture for Learning, Planning, and
  Reacting}
    \verb{url}
    \verb https://dl.acm.org/doi/10.1145/122344.122377
    \endverb
    \field{volume}{2}
    \field{langid}{english}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/9IDGAIW3/Sutton -
    \verb  1991 - Dyna, an integrated architecture for learning, pla.pdf
    \endverb
    \field{journaltitle}{ACM SIGART Bulletin}
    \field{month}{07}
    \field{year}{1991}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2021}
  \endentry

  \entry{olafsdottirHippocampalPlaceCells2015}{article}{}
    \name{author}{5}{}{%
      {{hash=ÃHF}{%
         family={Ã“lafsdÃ³ttir},
         familyi={Ã\bibinitperiod},
         given={H\bibnamedelima Freyja},
         giveni={H\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={Barry},
         familyi={B\bibinitperiod},
         given={Caswell},
         giveni={C\bibinitperiod},
      }}%
      {{hash=SAB}{%
         family={Saleem},
         familyi={S\bibinitperiod},
         given={Aman\bibnamedelima B},
         giveni={A\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=HD}{%
         family={Hassabis},
         familyi={H\bibinitperiod},
         given={Demis},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SHJ}{%
         family={Spiers},
         familyi={S\bibinitperiod},
         given={Hugo\bibnamedelima J},
         giveni={H\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \strng{namehash}{ÃHF+1}
    \strng{fullhash}{ÃHFBCSABHDSHJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2015}
    \field{labeldatesource}{}
    \field{sortinit}{Ã}
    \field{sortinithash}{Ã}
    \field{abstract}{%
    Dominant theories of hippocampal function propose that place cell
  representations are formed during an animal's first encounter with a novel
  environment and are subsequently replayed during off-line states to support
  consolidation and future behaviour. Here we report that viewing the delivery
  of food to an unvisited portion of an environment leads to off-line
  pre-activation of place cells sequences corresponding to that space. Such
  â€˜preplayâ€™ was not observed for an unrewarded but otherwise similar
  portion of the environment. These results suggest that a hippocampal
  representation of a visible, yet unexplored environment can be formed if the
  environment is of motivational relevance to the animal. We hypothesise such
  goal-biased preplay may support preparation for future experiences in novel
  environments. , As an animal explores an area, part of the brain called the
  hippocampus creates a mental map of the space. When the animal is in one
  location, a few neurons called â€˜place cellsâ€™ will fire. If the animal
  moves to a new spot, other place cells fire instead. Each time the animal
  returns to that spot, the same place cells will fire. Thus, as the animal
  moves, a place-specific pattern of firing emerges that scientists can view by
  recording the cells' activity and which can be used to reconstruct the
  animal's position. After exploring a space, the hippocampus may replay the
  new place-specific pattern of activity during sleep. By doing so, the brain
  consolidates the memory of the space for return visits. Recent evidence now
  suggests that these mental rehearsalsâ€”or internal simulations of the
  spaceâ€”may begin even before a new space has been explored. Now,
  Ã“lafsdÃ³ttir, Barry et al. report that whether an animal's brain simulates a
  first visit to a new space depends on whether the animal anticipates a
  reward. In the experiments, rats were allowed to run up to the junction in a
  T-shaped track. The animals could see into each of the arms, but not enter
  them. Food was then placed in one of the inaccessible arms. Ã“lafsdÃ³ttir,
  Barry et al. recorded the firing of place cells in the brain of the animals
  when they were on the track and during a rest period afterwards. The rats
  were then allowed onto the inaccessible arms, and again their brain activity
  was recorded. In the rest period after the rats first viewed the inaccessible
  arms, the place cell pattern that would later form the mental map of a
  journey to and from the food-containing arm was pre-activated. However, the
  place cell pattern that would become the mental map of the other inaccessible
  arm was not activated before the rat explored that area. Therefore,
  Ã“lafsdÃ³ttir, Barry et al. suggest that the perception of reward influences
  which place cell pattern is simulated during rest. An implication of these
  findings is that the brain preferentially simulates past or future
  experiences that are deemed to be functionally significant, such as those
  associated with reward. A future challenge will be to determine whether this
  goal-related simulation of unvisited spaces predicts and is needed for
  behaviour such as successful navigation to a goal.%
    }
    \verb{doi}
    \verb 10.7554/eLife.06063
    \endverb
    \field{issn}{2050-084X}
    \field{pages}{e06063}
    \field{title}{Hippocampal Place Cells Construct Reward Related Sequences
  through Unexplored Space}
    \verb{url}
    \verb https://elifesciences.org/articles/06063
    \endverb
    \field{volume}{4}
    \field{langid}{english}
    \verb{file}
    \verb /home/georgy/snap/zotero-snap/common/Zotero/storage/KW39L6NQ/Ã“lafsdÃ
    \verb ³ttir et al. - 2015 - Hippocampal place cells construct reward relate
    \verb d s.pdf
    \endverb
    \field{journaltitle}{eLife}
    \field{day}{26}
    \field{month}{06}
    \field{year}{2015}
    \field{urlday}{07}
    \field{urlmonth}{12}
    \field{urlyear}{2021}
  \endentry
\enddatalist
\endinput
